"""
UI ì»´í¬ë„ŒíŠ¸ ëª¨ë“ˆ
Streamlit UI íƒ­ ë Œë”ë§ í•¨ìˆ˜ë“¤
"""
import streamlit as st
import pandas as pd
import io
import re
import yaml
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
from datetime import datetime
from .data_loader import UnifiedDataLoader
from .data_utils import (
    dict_to_yaml_string, yaml_string_to_dict,
    extract_date_range_from_df, save_to_parquet_with_metadata,
    save_to_hdf5_with_metadata, load_from_hdf5_with_metadata,
    safe_display_df, prepare_df_for_parquet, prepare_df_for_display
)
from .visualization_plots import (
    render_timeseries_plot, render_scatter_plot, render_histogram,
    render_boxplot, render_correlation_heatmap
)


def render_config_tab():
    st.header("ğŸ“‹ YAML ì„¤ì •")
    
    col1, col2, col3 = st.columns([1, 1, 1])
    
    with col1:
        if st.button("ğŸ”„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.config = {
                'file_info': {'description': '', 'file_type': 'excel'},
                'csv_options': {'encoding': 'utf-8', 'delimiter': ',', 'quotechar': '"', 
                               'skip_blank_lines': True, 'comment': '#'},
                'sheets': {'mode': 'single', 'names': [], 'indices': [], 'exclude': []},
                'header': {'skip_rows': 0, 'header_rows': {}, 'data_start_row': 1},
                'timestamp': {
                    'combine_time_columns': False, 
                    'keywords': ['timestamp', 'datetime', 'date'],
                    'use_first_column': False, 
                    'target_name': 'timestamp', 
                    'drop_time_columns': True, 
                    'strict': False,
                    'exclude_from_output': False  # ğŸ†•
                },
                'sampling': {  # ğŸ†•
                    'enabled': False,
                    'method': 'every_n',
                    'interval': 5
                },                
                'column_names': {'replace_spaces': '_', 'keep_special_chars': True, 'lowercase': False},
                'data_types': {'auto_infer': True, 'sample_rows': 100, 'value_mapping': {}, 'null_values': []},
                'post_processing': {'remove_empty_rows': True, 'remove_high_null_columns': None, 
                                   'remove_duplicates': False},
                'output': {'format': 'parquet', 'compression': 'snappy', 'save_metadata': True},
                'error_handling': {'on_parse_error': 'skip_row', 'save_log': True, 
                                  'log_path': 'logs/parser.log', 'verbose': False}
            }
            st.session_state['yaml_loaded'] = False
            st.rerun()
    
    with col2:
        uploaded_yaml = st.file_uploader("ğŸ“¥ YAML íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°", type=['yaml', 'yml'], key='yaml_upload')
        if uploaded_yaml:
            # YAMLì´ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if st.session_state.get('yaml_loaded', False):
                st.success("âœ… YAML ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
            
            col2a, col2b = st.columns(2)
            with col2a:
                if st.button("YAML ì ìš©", key='apply_yaml', type='primary'):
                    try:
                        # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
                        uploaded_yaml.seek(0)
                        config_data = yaml.safe_load(uploaded_yaml)
                        
                        # ìˆ«ìí˜• ê°’ë“¤ì´ ì œëŒ€ë¡œ ë¡œë”©ë˜ë„ë¡ ë³´ì¥
                        if 'header' in config_data:
                            if 'skip_rows' in config_data['header']:
                                config_data['header']['skip_rows'] = int(config_data['header']['skip_rows'])
                            if 'data_start_row' in config_data['header']:
                                config_data['header']['data_start_row'] = int(config_data['header']['data_start_row'])
                            if 'header_rows' in config_data['header'] and config_data['header']['header_rows']:
                                header_rows = {}
                                for key, val in config_data['header']['header_rows'].items():
                                    if val is not None and val != '':
                                        try:
                                            header_rows[key] = int(val)
                                        except:
                                            header_rows[key] = 0
                                config_data['header']['header_rows'] = header_rows
                        
                        # session_stateì— ì €ì¥
                        st.session_state.config = config_data
                        st.session_state['yaml_loaded'] = True
                        
                        # ì ìš© ì™„ë£Œ ë©”ì‹œì§€ì™€ í•¨ê»˜ ìƒˆë¡œê³ ì¹¨
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"âŒ YAML íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
            
            with col2b:
                # í˜„ì¬ í—¤ë” ì„¤ì • ë¯¸ë¦¬ë³´ê¸°
                if st.button("ğŸ” ë¯¸ë¦¬ë³´ê¸°", key='preview_yaml'):
                    try:
                        uploaded_yaml.seek(0)
                        config_data = yaml.safe_load(uploaded_yaml)
                        if 'header' in config_data:
                            st.info(f"""
**YAML íŒŒì¼ ë‚´ìš©:**
- skip_rows: {config_data['header'].get('skip_rows', 0)}
- data_start_row: {config_data['header'].get('data_start_row', 1)}
- header_rows: {config_data['header'].get('header_rows', {})}
                            """)
                    except Exception as e:
                        st.error(f"ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {str(e)}")
    
    with col3:
        yaml_str = dict_to_yaml_string(st.session_state.config)
        st.download_button(
            label="ğŸ’¾ YAML íŒŒì¼ ì €ì¥",
            data=yaml_str,
            file_name=f"config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml",
            mime="text/yaml",
            use_container_width=True
        )
    
    st.divider()
    
    # ì„¤ì • ì„¹ì…˜ë“¤
    with st.expander("ğŸ“ íŒŒì¼ ì •ë³´", expanded=True):
        st.session_state.config['file_info']['description'] = st.text_input(
            "ì„¤ëª…", 
            value=st.session_state.config['file_info'].get('description', '')
        )
        st.session_state.config['file_info']['file_type'] = st.selectbox(
            "íŒŒì¼ íƒ€ì…",
            options=['excel', 'csv'],
            index=0 if st.session_state.config['file_info'].get('file_type', 'excel') == 'excel' else 1
        )
    
    # CSV ì˜µì…˜ (file_typeì´ csvì¼ ë•Œë§Œ í‘œì‹œ)
    if st.session_state.config['file_info']['file_type'] == 'csv':
        with st.expander("ğŸ“„ CSV ì˜µì…˜"):
            csv_opt = st.session_state.config.get('csv_options', {})
            
            col1, col2 = st.columns(2)
            with col1:
                csv_opt['encoding'] = st.selectbox(
                    "ì¸ì½”ë”©",
                    options=['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin1'],
                    index=['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin1'].index(
                        csv_opt.get('encoding', 'utf-8')
                    )
                )
                csv_opt['delimiter'] = st.text_input("êµ¬ë¶„ì", value=csv_opt.get('delimiter', ','))
            
            with col2:
                csv_opt['quotechar'] = st.text_input("ë”°ì˜´í‘œ ë¬¸ì", value=csv_opt.get('quotechar', '"'))
                csv_opt['comment'] = st.text_input("ì£¼ì„ ë¬¸ì (ì„ íƒ)", value=csv_opt.get('comment', '#'))
            
            csv_opt['skip_blank_lines'] = st.checkbox("ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°", value=csv_opt.get('skip_blank_lines', True))
            
            st.session_state.config['csv_options'] = csv_opt
    
    # ì‹œíŠ¸ ì„¤ì • (Excel ì „ìš©)
    if st.session_state.config['file_info']['file_type'] == 'excel':
        with st.expander("ğŸ“Š ì‹œíŠ¸ ì²˜ë¦¬ ì„¤ì •"):
            sheets = st.session_state.config.get('sheets', {})
            
            sheets['mode'] = st.radio(
                "ì²˜ë¦¬ ëª¨ë“œ",
                options=['single', 'all', 'specific'],
                index=['single', 'all', 'specific'].index(sheets.get('mode', 'single')),
                horizontal=True
            )
            
            if sheets['mode'] == 'specific':
                names_str = st.text_input("ì‹œíŠ¸ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)", 
                                         value=','.join(sheets.get('names', [])))
                sheets['names'] = [n.strip() for n in names_str.split(',') if n.strip()]
                
                indices_str = st.text_input("ì‹œíŠ¸ ì¸ë±ìŠ¤ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)", 
                                           value=','.join(map(str, sheets.get('indices', []))))
                try:
                    sheets['indices'] = [int(i.strip()) for i in indices_str.split(',') if i.strip()]
                except:
                    sheets['indices'] = []
            
            if sheets['mode'] == 'all':
                exclude_str = st.text_input("ì œì™¸í•  ì‹œíŠ¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)", 
                                          value=','.join(sheets.get('exclude', [])))
                sheets['exclude'] = [e.strip() for e in exclude_str.split(',') if e.strip()]
            
            st.session_state.config['sheets'] = sheets
    
    # í—¤ë” êµ¬ì¡°
    with st.expander("ğŸ“‘ í—¤ë” êµ¬ì¡°", expanded=True):
        header = st.session_state.config.get('header', {})
        
        # skip_rows ì²˜ë¦¬
        current_skip = 0
        if 'skip_rows' in header and header['skip_rows'] is not None:
            try:
                current_skip = int(header['skip_rows'])
            except:
                current_skip = 0
        
        header['skip_rows'] = st.number_input(
            "ìƒë‹¨ì—ì„œ ê±´ë„ˆë›¸ í–‰ ìˆ˜ (1-based)",
            min_value=0, value=current_skip, step=1
        )
        
        st.markdown("**í—¤ë” í–‰ ë²ˆí˜¸ (skip_rows ì ìš© í›„ ê¸°ì¤€, 1-based)**")
        st.caption("0ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ í—¤ë”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        col1, col2, col3 = st.columns(3)
        
        # header_rows ë”•ì…”ë„ˆë¦¬ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬)
        if 'header_rows' not in header:
            header['header_rows'] = {}
        header_rows = header['header_rows']
        
        with col1:
            # ê¸°ë³¸ê°’ ê³„ì‚°
            current_desc = 0
            if 'description' in header_rows and header_rows['description'] is not None:
                try:
                    current_desc = int(header_rows['description'])
                except:
                    current_desc = 0
            
            desc_row = st.number_input("ì„¤ëª…(Description) í–‰", min_value=0, 
                                       value=current_desc, 
                                       step=1)
            if desc_row > 0:
                header_rows['description'] = int(desc_row)
            elif 'description' in header_rows:
                del header_rows['description']
        
        with col2:
            # ê¸°ë³¸ê°’ ê³„ì‚°
            current_unit = 0
            if 'unit' in header_rows and header_rows['unit'] is not None:
                try:
                    current_unit = int(header_rows['unit'])
                except:
                    current_unit = 0
            
            unit_row = st.number_input("ë‹¨ìœ„(Unit) í–‰", min_value=0, 
                                       value=current_unit,
                                       step=1)
            if unit_row > 0:
                header_rows['unit'] = int(unit_row)
            elif 'unit' in header_rows:
                del header_rows['unit']
        
        with col3:
            # ê¸°ë³¸ê°’ ê³„ì‚°
            current_tag = 0
            if 'tag_name' in header_rows and header_rows['tag_name'] is not None:
                try:
                    current_tag = int(header_rows['tag_name'])
                except:
                    current_tag = 0
            
            tag_row = st.number_input("íƒœê·¸ëª…(Tag) í–‰", min_value=0, 
                                      value=current_tag,
                                      step=1)
            if tag_row > 0:
                header_rows['tag_name'] = int(tag_row)
            elif 'tag_name' in header_rows:
                del header_rows['tag_name']
        
        header['header_rows'] = header_rows
        
        # data_start_row ì²˜ë¦¬
        current_data_start = 1
        if 'data_start_row' in header and header['data_start_row'] is not None:
            try:
                current_data_start = int(header['data_start_row'])
            except:
                current_data_start = 1
        
        header['data_start_row'] = st.number_input(
            "ë°ì´í„° ì‹œì‘ í–‰ (skip_rows ì ìš© í›„ ê¸°ì¤€, 1-based)",
            min_value=1, value=current_data_start, step=1
        )
        
        st.session_state.config['header'] = header
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
    with st.expander("ğŸ• íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬"):
        ts = st.session_state.config.get('timestamp', {})
        
        ts['combine_time_columns'] = st.checkbox(
            "ë¶„ë¦¬ëœ ì‹œê°„ ì»¬ëŸ¼ í•©ì¹˜ê¸° (year, month, day ë“±)",
            value=ts.get('combine_time_columns', False)
        )
        
        if ts['combine_time_columns']:
            st.markdown("**ì‹œê°„ ì»¬ëŸ¼ ì„¤ì •**")
            time_cols_str = st.text_input(
                "ì°¾ì„ ì‹œê°„ ì»¬ëŸ¼ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
                value=','.join(ts.get('time_columns', ['year', 'month', 'day', 'hour', 'minute', 'second']))
            )
            ts['time_columns'] = [c.strip() for c in time_cols_str.split(',') if c.strip()]
            
            col1, col2, col3 = st.columns(3)
            defaults = ts.get('defaults', {})
            with col1:
                defaults['year'] = st.number_input("ê¸°ë³¸ ì—°ë„", value=defaults.get('year', 2025))
                defaults['month'] = st.number_input("ê¸°ë³¸ ì›”", min_value=1, max_value=12, 
                                                    value=defaults.get('month', 1))
            with col2:
                defaults['day'] = st.number_input("ê¸°ë³¸ ì¼", min_value=1, max_value=31, 
                                                  value=defaults.get('day', 1))
                defaults['hour'] = st.number_input("ê¸°ë³¸ ì‹œ", min_value=0, max_value=23, 
                                                   value=defaults.get('hour', 0))
            with col3:
                defaults['minute'] = st.number_input("ê¸°ë³¸ ë¶„", min_value=0, max_value=59, 
                                                     value=defaults.get('minute', 0))
                defaults['second'] = st.number_input("ê¸°ë³¸ ì´ˆ", min_value=0, max_value=59, 
                                                     value=defaults.get('second', 0))
            ts['defaults'] = defaults
            
            ts['base_year'] = st.number_input("2ìë¦¬ ì—°ë„ ë³€í™˜ ê¸°ì¤€ë…„ë„", 
                                             value=ts.get('base_year', 2000))
        else:
            keywords_str = st.text_input(
                "íƒ€ì„ìŠ¤íƒ¬í”„ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
                value=','.join(ts.get('keywords', ['timestamp', 'datetime', 'date']))
            )
            ts['keywords'] = [k.strip() for k in keywords_str.split(',') if k.strip()]
            
            ts['use_first_column'] = st.checkbox(
                "ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì‚¬ìš©",
                value=ts.get('use_first_column', False)
            )
        
        ts['target_name'] = st.text_input("ìƒì„±í•  íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ëª…", 
                                         value=ts.get('target_name', 'timestamp'))
        ts['drop_time_columns'] = st.checkbox("ì›ë³¸ ì‹œê°„ ì»¬ëŸ¼ ì œê±°", 
                                               value=ts.get('drop_time_columns', True))
        ts['strict'] = st.checkbox("ì—„ê²© ëª¨ë“œ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìœ¼ë©´ ì—ëŸ¬)", 
                                   value=ts.get('strict', False))
        
        
        # ğŸ†• ìƒˆë¡œìš´ ì˜µì…˜: íƒ€ì„ìŠ¤íƒ¬í”„ ì œì™¸
        st.divider()
        st.markdown("#### ğŸ†• ì¶œë ¥ ì˜µì…˜")
        st.caption("ì €ì¥í•  íŒŒì¼ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ì„ ì œì™¸í• ì§€ ì„ íƒí•©ë‹ˆë‹¤.")
        
        ts['exclude_from_output'] = st.checkbox(
            "âš ï¸ ì €ì¥ ì‹œ íƒ€ì„ìŠ¤íƒ¬í”„ ì œì™¸ (íŠ¹ì§• ì»¬ëŸ¼ë§Œ ì €ì¥)",
            value=ts.get('exclude_from_output', False),
            help="ì²´í¬í•˜ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ì´ íŠ¹ì§• ë°ì´í„°ë§Œ ì €ì¥ë©ë‹ˆë‹¤. ì‹œê°„ ì •ë³´ê°€ ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ì—°ì†ì ì´ì§€ ì•Šì€ ê²½ìš° ì‚¬ìš©í•˜ì„¸ìš”."
        )
        
        if ts['exclude_from_output']:
            st.warning("âš ï¸ ì €ì¥ ì‹œ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì œì™¸ë©ë‹ˆë‹¤. ì‹œê°í™”ëŠ” ê°€ëŠ¥í•˜ì§€ë§Œ ì‹œê³„ì—´ ë¶„ì„ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        st.session_state.config['timestamp'] = ts
    
    # ğŸ†• ìƒ˜í”Œë§ ì„¤ì • (ìƒˆë¡œìš´ ì„¹ì…˜)
    with st.expander("ğŸ¯ ìƒ˜í”Œë§ ì„¤ì • (ì‹ ê·œ)"):
        sampling = st.session_state.config.get('sampling', {})
        
        st.markdown("#### ë°ì´í„° ìƒ˜í”Œë§")
        st.caption("ë°ì´í„°ê°€ ë„ˆë¬´ ì´˜ì´˜í•œ ê²½ìš° ê°„ê²©ì„ ë‘ê³  ìƒ˜í”Œë§í•˜ê±°ë‚˜ ì§‘ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        sampling['enabled'] = st.checkbox(
            "ìƒ˜í”Œë§ í™œì„±í™”",
            value=sampling.get('enabled', False),
            help="ë°ì´í„° ê°„ê²©ì„ ì¤„ì—¬ì„œ ì €ì¥í•©ë‹ˆë‹¤."
        )
        
        if sampling['enabled']:
            col1, col2 = st.columns([1, 2])
            
            with col1:
                sampling['interval'] = st.number_input(
                    "ìƒ˜í”Œë§ ê°„ê²©",
                    min_value=2,
                    max_value=1000,
                    value=sampling.get('interval', 5),
                    step=1,
                    help="Nê°œ ë°ì´í„°ë§ˆë‹¤ 1ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì˜ˆ: 5 â†’ 5ê°œë‹¹ 1ê°œ"
                )
            
            with col2:
                sampling['method'] = st.selectbox(
                    "ìƒ˜í”Œë§ ë°©ë²•",
                    options=['every_n', 'mean', 'median', 'first', 'last'],
                    index=['every_n', 'mean', 'median', 'first', 'last'].index(
                        sampling.get('method', 'every_n')
                    ),
                    help="""
â€¢ every_n: Nê°œë§ˆë‹¤ 1ê°œ ì„ íƒ (ë‹¨ìˆœ ìƒ˜í”Œë§)
â€¢ mean: Nê°œì”© ê·¸ë£¹í™”í•˜ì—¬ í‰ê· ê°’ ì €ì¥
â€¢ median: Nê°œì”© ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ì•™ê°’ ì €ì¥
â€¢ first: Nê°œì”© ê·¸ë£¹í™”í•˜ì—¬ ì²« ë²ˆì§¸ ê°’ ì €ì¥
â€¢ last: Nê°œì”© ê·¸ë£¹í™”í•˜ì—¬ ë§ˆì§€ë§‰ ê°’ ì €ì¥
                    """
                )
            
            # ì˜ˆì‹œ í‘œì‹œ
            method_examples = {
                'every_n': "ì˜ˆ) 1,2,3,4,5,6,7,8,9,10 â†’ 1,6 (5ê°œë§ˆë‹¤ 1ê°œ)",
                'mean': "ì˜ˆ) [1,2,3,4,5],[6,7,8,9,10] â†’ 3.0, 8.0 (5ê°œì”© í‰ê· )",
                'median': "ì˜ˆ) [1,2,3,4,5],[6,7,8,9,10] â†’ 3, 8 (5ê°œì”© ì¤‘ì•™ê°’)",
                'first': "ì˜ˆ) [1,2,3,4,5],[6,7,8,9,10] â†’ 1, 6 (5ê°œì”© ì²«ê°’)",
                'last': "ì˜ˆ) [1,2,3,4,5],[6,7,8,9,10] â†’ 5, 10 (5ê°œì”© ëê°’)"
            }
            
            st.info(f"**{sampling['method']}** - {method_examples[sampling['method']]}")
            
            # ì˜ˆìƒ ì¶•ì†Œìœ¨ ê³„ì‚°
            estimated_reduction = (1 - 1/sampling['interval']) * 100
            st.success(f"âœ… ì˜ˆìƒ ë°ì´í„° ì¶•ì†Œìœ¨: ì•½ {estimated_reduction:.1f}%")
        
        st.session_state.config['sampling'] = sampling
    
    # ì»¬ëŸ¼ëª… ì •ê·œí™”
    with st.expander("ğŸ”¤ ì»¬ëŸ¼ëª… ì •ê·œí™”"):
        col_names = st.session_state.config.get('column_names', {})
        
        col1, col2 = st.columns(2)
        with col1:
            replace_space = st.text_input("ê³µë°± ì¹˜í™˜ ë¬¸ì", 
                                         value=col_names.get('replace_spaces', '_'))
            col_names['replace_spaces'] = replace_space if replace_space else None
        
        with col2:
            col_names['keep_special_chars'] = st.checkbox("íŠ¹ìˆ˜ë¬¸ì ìœ ì§€", 
                                                          value=col_names.get('keep_special_chars', True))
            col_names['lowercase'] = st.checkbox("ì†Œë¬¸ì ë³€í™˜", 
                                                 value=col_names.get('lowercase', False))
        
        st.session_state.config['column_names'] = col_names
    
    # ë°ì´í„° íƒ€ì…
    with st.expander("ğŸ”¢ ë°ì´í„° íƒ€ì… ë³€í™˜"):
        dtypes = st.session_state.config.get('data_types', {})
        
        col1, col2 = st.columns(2)
        with col1:
            dtypes['auto_infer'] = st.checkbox("ìë™ íƒ€ì… ì¶”ë¡ ", 
                                               value=dtypes.get('auto_infer', True))
            if dtypes['auto_infer']:
                dtypes['sample_rows'] = st.number_input("ìƒ˜í”Œ í–‰ ìˆ˜", min_value=10, 
                                                       value=dtypes.get('sample_rows', 100))
        
        with col2:
            st.markdown("**ê°’ ë§¤í•‘ (ë¬¸ìì—´ â†’ ë¶ˆë¦°/ìˆ«ì)**")
            mapping_str = st.text_area(
                "í˜•ì‹: KEY=VALUE (í•œ ì¤„ì— í•˜ë‚˜ì”©)",
                value='\n'.join([f"{k}={v}" for k, v in dtypes.get('value_mapping', {}).items()]),
                height=100
            )
            value_mapping = {}
            for line in mapping_str.split('\n'):
                if '=' in line:
                    k, v = line.split('=', 1)
                    k, v = k.strip(), v.strip()
                    if v.lower() == 'true':
                        value_mapping[k] = True
                    elif v.lower() == 'false':
                        value_mapping[k] = False
                    else:
                        try:
                            value_mapping[k] = float(v) if '.' in v else int(v)
                        except:
                            value_mapping[k] = v
            dtypes['value_mapping'] = value_mapping
        
        null_str = st.text_input(
            "NULLë¡œ ê°„ì£¼í•  ê°’ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            value=','.join(dtypes.get('null_values', []))
        )
        dtypes['null_values'] = [n.strip() for n in null_str.split(',') if n.strip()]
        
        st.session_state.config['data_types'] = dtypes
    
    # í›„ì²˜ë¦¬
    with st.expander("ğŸ”§ í›„ì²˜ë¦¬"):
        post = st.session_state.config.get('post_processing', {})
        
        col1, col2, col3 = st.columns(3)
        with col1:
            post['remove_empty_rows'] = st.checkbox("ë¹ˆ í–‰ ì œê±°", 
                                                    value=post.get('remove_empty_rows', True))
        with col2:
            enable_null_threshold = st.checkbox("NULL ë¹„ìœ¨ ë†’ì€ ì»¬ëŸ¼ ì œê±°", 
                                               value=post.get('remove_high_null_columns') is not None)
            if enable_null_threshold:
                post['remove_high_null_columns'] = st.slider("NULL ë¹„ìœ¨ ì„ê³„ê°’ (%)", 0, 100, 
                                                             value=post.get('remove_high_null_columns', 90))
            else:
                post['remove_high_null_columns'] = None
        with col3:
            post['remove_duplicates'] = st.checkbox("ì¤‘ë³µ í–‰ ì œê±°", 
                                                    value=post.get('remove_duplicates', False))
        
        st.session_state.config['post_processing'] = post
    
    # ì¶œë ¥ ì„¤ì •
    with st.expander("ğŸ’¾ ì¶œë ¥ ì„¤ì •"):
        output = st.session_state.config.get('output', {})
        
        col1, col2, col3 = st.columns(3)
        with col1:
            output['format'] = st.selectbox(
                "ì¶œë ¥ í˜•ì‹",
                options=['parquet', 'csv', 'excel'],
                index=['parquet', 'csv', 'excel'].index(output.get('format', 'parquet'))
            )
        with col2:
            if output['format'] == 'parquet':
                output['compression'] = st.selectbox(
                    "ì••ì¶• ë°©ì‹",
                    options=['snappy', 'gzip', 'brotli'],
                    index=['snappy', 'gzip', 'brotli'].index(output.get('compression', 'snappy'))
                )
        with col3:
            output['save_metadata'] = st.checkbox("ë©”íƒ€ë°ì´í„° ì €ì¥", 
                                                 value=output.get('save_metadata', True))
        
        st.session_state.config['output'] = output
    
    # ì—ëŸ¬ ì²˜ë¦¬
    with st.expander("âš ï¸ ì—ëŸ¬ ì²˜ë¦¬"):
        error = st.session_state.config.get('error_handling', {})
        
        col1, col2 = st.columns(2)
        with col1:
            error['on_parse_error'] = st.selectbox(
                "íŒŒì‹± ì—ëŸ¬ ë°œìƒ ì‹œ",
                options=['skip_row', 'raise', 'ignore'],
                index=['skip_row', 'raise', 'ignore'].index(error.get('on_parse_error', 'skip_row'))
            )
            error['save_log'] = st.checkbox("ë¡œê·¸ ì €ì¥", value=error.get('save_log', True))
        
        with col2:
            error['log_path'] = st.text_input("ë¡œê·¸ ê²½ë¡œ", value=error.get('log_path', 'logs/parser.log'))
            error['verbose'] = st.checkbox("ìƒì„¸ ë¡œê·¸ (DEBUG)", value=error.get('verbose', False))
        
        st.session_state.config['error_handling'] = error
    
    # í˜„ì¬ ì„¤ì • ë¯¸ë¦¬ë³´ê¸°
    with st.expander("ğŸ‘ï¸ í˜„ì¬ ì„¤ì • ë¯¸ë¦¬ë³´ê¸°"):
        st.code(dict_to_yaml_string(st.session_state.config), language='yaml')




# ============================================
# Tab 2: ë°ì´í„° ë¡œë”©
# ============================================
def render_loading_tab():
    st.header("ğŸ“‚ ë°ì´í„° ë¡œë”©")
    
    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì—…ë¡œë” í‘œì‹œ
    file_type = st.session_state.config['file_info']['file_type']
    
    if file_type == 'csv':
        # CSVëŠ” ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ê°€ëŠ¥
        uploaded_files = st.file_uploader(
            "CSV íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)",
            type=['csv'],
            accept_multiple_files=True,
            key='csv_upload'
        )
    else:
        # Excelì€ ë‹¨ì¼ íŒŒì¼
        uploaded_files = st.file_uploader(
            "Excel íŒŒì¼ ì—…ë¡œë“œ",
            type=['xlsx', 'xls'],
            key='excel_upload'
        )
        # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í†µì¼ëœ ì²˜ë¦¬ë¥¼ ìœ„í•´)
        if uploaded_files:
            uploaded_files = [uploaded_files]
    
    if uploaded_files:
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        if len(uploaded_files) > 1:
            st.info(f"ğŸ“ {len(uploaded_files)}ê°œ íŒŒì¼ ì„ íƒë¨")
            for i, f in enumerate(uploaded_files, 1):
                st.caption(f"{i}. {f.name}")
        
        # íŒŒì¼ í™•ì¥ì ì²´í¬
        file_type = st.session_state.config['file_info']['file_type']
        
        if file_type == 'excel' and uploaded_files[0].name.split('.')[-1].lower() not in ['xlsx', 'xls']:
            st.warning("âš ï¸ ì—…ë¡œë“œëœ íŒŒì¼ì€ Excelì´ ì•„ë‹ˆì§€ë§Œ ì„¤ì •ì€ Excelì…ë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        elif file_type == 'csv' and any(f.name.split('.')[-1].lower() != 'csv' for f in uploaded_files):
            st.warning("âš ï¸ ì¼ë¶€ íŒŒì¼ì´ CSVê°€ ì•„ë‹™ë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            if st.button("ğŸš€ ë°ì´í„° ë¡œë“œ ì‹œì‘", type="primary", use_container_width=True):
                progress_text = st.empty()
                progress_bar = st.empty()
                
                with st.spinner("ë°ì´í„°ë¥¼ ë¡œë”©í•˜ëŠ” ì¤‘..."):
                    try:
                        # ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
                        def update_progress(sheet_name, current, total):
                            progress_text.text(f"ì²˜ë¦¬ ì¤‘: {current}/{total} - {sheet_name}")
                            progress_bar.progress(current / total)
                        
                        # ë¡œë” ìƒì„± (ì½œë°± ì „ë‹¬)
                        loader = UnifiedDataLoader(st.session_state.config, progress_callback=update_progress)
                        
                        # logger ì„ ì–¸
                        import logging
                        logger = logging.getLogger(__name__)
                        
                        # íƒ€ì… ì •ë¦¬ í•¨ìˆ˜ ì •ì˜ (ë£¨í”„ ë°–ì—ì„œ)
                        def fix_timestamps_immediately(df):
                            """ì¦‰ì‹œ ëª¨ë“  íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë¦¬ - ì™„ì „íˆ ìƒˆë¡œìš´ DataFrame ìƒì„±"""
                            logger.info(f"ğŸ”§ fix_timestamps_immediately ì‹œì‘: {df.shape}")
                            
                            # ì™„ì „íˆ ìƒˆë¡œìš´ ë”•ì…”ë„ˆë¦¬ë¡œ ì»¬ëŸ¼ ìƒì„±
                            new_data = {}
                            
                            for col in df.columns:
                                try:
                                    # datetime íƒ€ì…ì¸ ê²½ìš°
                                    if pd.api.types.is_datetime64_any_dtype(df[col]):
                                        logger.info(f"  -> '{col}': datetime íƒ€ì… ì •ë¦¬ ì¤‘")
                                        # ìƒˆë¡œìš´ Series ìƒì„±
                                        new_data[col] = pd.to_datetime(
                                            df[col].dt.strftime('%Y-%m-%d %H:%M:%S'),
                                            format='%Y-%m-%d %H:%M:%S',
                                            errors='coerce'
                                        )
                                    
                                    # object íƒ€ì…ì— Timestamp ìˆëŠ” ê²½ìš°
                                    elif df[col].dtype == 'object':
                                        non_null = df[col].dropna()
                                        if len(non_null) > 0:
                                            first_val = non_null.iloc[0]
                                            if isinstance(first_val, (pd.Timestamp, type(pd.NaT))):
                                                logger.warning(f"  -> '{col}': object íƒ€ì… Timestamp ë°œê²¬! ë³€í™˜ ì¤‘...")
                                                temp_col = pd.to_datetime(df[col], errors='coerce')
                                                new_data[col] = pd.to_datetime(
                                                    temp_col.dt.strftime('%Y-%m-%d %H:%M:%S'),
                                                    format='%Y-%m-%d %H:%M:%S',
                                                    errors='coerce'
                                                )
                                                logger.info(f"  -> '{col}': ë³€í™˜ ì™„ë£Œ dtype={new_data[col].dtype}")
                                            else:
                                                # ì¼ë°˜ object - ë³µì‚¬
                                                new_data[col] = df[col].copy()
                                        else:
                                            # ë¹ˆ ì»¬ëŸ¼ - ë³µì‚¬
                                            new_data[col] = df[col].copy()
                                    else:
                                        # ë‹¤ë¥¸ íƒ€ì… - ë³µì‚¬
                                        new_data[col] = df[col].copy()
                                        
                                except Exception as e:
                                    logger.error(f"  -> '{col}': ë³€í™˜ ì‹¤íŒ¨ - {e}")
                                    # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë³µì‚¬
                                    new_data[col] = df[col].copy()
                            
                            # ì™„ì „íˆ ìƒˆë¡œìš´ DataFrame ìƒì„±
                            df_new = pd.DataFrame(new_data, index=df.index)
                            
                            # attrs ë³µì‚¬ (ë©”íƒ€ë°ì´í„° ìœ ì§€)
                            df_new.attrs = df.attrs.copy()
                            
                            logger.info(f"ğŸ”§ fix_timestamps_immediately ì™„ë£Œ")
                            return df_new
                        
                        # ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬
                        all_data = {}
                        
                        for idx, uploaded_file in enumerate(uploaded_files, 1):
                            if len(uploaded_files) > 1:
                                progress_text.text(f"íŒŒì¼ {idx}/{len(uploaded_files)} ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
                                progress_bar.progress(idx / len(uploaded_files))
                            
                            # ë°ì´í„° ë¡œë“œ
                            data = loader.load(uploaded_file, file_type)
                            
                            # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
                            file_base_name = uploaded_file.name.rsplit('.', 1)[0]
                            
                            # ê²°ê³¼ ì €ì¥ (ì¦‰ì‹œ ì •ë¦¬)
                            if isinstance(data, pd.DataFrame):
                                # fix_timestamps_immediatelyê°€ ìƒˆ DataFrame ìƒì„±
                                df_clean = fix_timestamps_immediately(data)
                                all_data[file_base_name] = df_clean
                            else:
                                # ë‹¤ì¤‘ ì‹œíŠ¸ì¸ ê²½ìš°
                                for sheet_name, sheet_df in data.items():
                                    combined_name = f"{file_base_name}_{sheet_name}"
                                    # fix_timestamps_immediatelyê°€ ìƒˆ DataFrame ìƒì„±
                                    df_clean = fix_timestamps_immediately(sheet_df)
                                    all_data[combined_name] = df_clean
                        
                        # ì§„í–‰ ìƒí™© í‘œì‹œ ì •ë¦¬
                        progress_text.empty()
                        progress_bar.empty()
                        
                        logger.info(f"ğŸ’¾ session_state ì €ì¥ ì¤€ë¹„: {len(all_data)}ê°œ íŒŒì¼")
                        
                        # ì™„ì „íˆ ìƒˆë¡œìš´ DataFrameìœ¼ë¡œ ì €ì¥ (ë” ì´ìƒ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ)
                        if len(all_data) == 1:
                            final_df = list(all_data.values())[0]
                            
                            logger.info(f"   ë‹¨ì¼ DataFrame ì €ì¥: shape={final_df.shape}")
                            logger.info(f"   timestamp dtype: {final_df['timestamp'].dtype if 'timestamp' in final_df.columns else 'N/A'}")
                            
                            # timestamp ì»¬ëŸ¼ì˜ ìƒ˜í”Œ ê°’ íƒ€ì… ì²´í¬
                            if 'timestamp' in final_df.columns:
                                sample_vals = final_df['timestamp'].dropna().head(3)
                                for idx, val in enumerate(sample_vals):
                                    logger.info(f"   timestamp[{idx}] íƒ€ì…={type(val).__name__}, ê°’={val}")
                            
                            # âš ï¸ CRITICAL: session_stateì— ì €ì¥í•˜ê¸° ì „ì— ì™„ì „íˆ ì •ë¦¬
                            # prepare_df_for_displayë¡œ í•œ ë²ˆ ë” ì •ë¦¬ (Timestamp ê°ì²´ ì œê±°)
                            final_df_safe = prepare_df_for_display(final_df)
                            
                            logger.info(f"   prepare_df_for_display í›„ timestamp dtype: {final_df_safe['timestamp'].dtype if 'timestamp' in final_df_safe.columns else 'N/A'}")
                            
                            if 'timestamp' in final_df_safe.columns:
                                sample_vals = final_df_safe['timestamp'].dropna().head(3)
                                for idx, val in enumerate(sample_vals):
                                    logger.info(f"   ì •ë¦¬ í›„ timestamp[{idx}] íƒ€ì…={type(val).__name__}, ê°’={val}")
                            
                            st.session_state.loaded_data = final_df_safe
                            
                            logger.info(f"   âœ… st.session_state.loaded_data ì €ì¥ ì™„ë£Œ")
                            
                            # ë©”íƒ€ë°ì´í„°ëŠ” ì •ë¦¬ëœ DataFrameì—ì„œ ìƒì„±
                            st.session_state.metadata = {
                                'source_name': final_df_safe.attrs.get('source_name', 'unknown'),
                                'header_metadata': final_df_safe.attrs.get('header_metadata', {}),
                                'removed_columns': final_df_safe.attrs.get('removed_columns', []),  # ì œê±°ëœ ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
                                'shape': final_df_safe.shape,
                                'columns': final_df_safe.columns.tolist(),
                                'dtypes': {str(k): str(v) for k, v in final_df_safe.dtypes.items()}
                            }
                        else:
                            logger.info(f"   ë‹¤ì¤‘ DataFrame ì €ì¥: {len(all_data)}ê°œ")
                            
                            # ê° DataFrameì„ ì •ë¦¬
                            cleaned_data = {}
                            for name, df in all_data.items():
                                logger.info(f"   - {name}: shape={df.shape}")
                                cleaned_data[name] = prepare_df_for_display(df)
                            
                            st.session_state.loaded_data = cleaned_data
                            
                            # ê° ì‹œíŠ¸ì˜ ë©”íƒ€ë°ì´í„° ìƒì„±
                            st.session_state.metadata = {
                                sheet: {
                                    'source_name': df.attrs.get('source_name', sheet),
                                    'header_metadata': df.attrs.get('header_metadata', {}),
                                    'removed_columns': df.attrs.get('removed_columns', []),  # ì œê±°ëœ ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
                                    'shape': df.shape,
                                    'columns': df.columns.tolist(),
                                    'dtypes': {str(k): str(v) for k, v in df.dtypes.items()}
                                }
                                for sheet, df in cleaned_data.items()
                            }
                        
                        # ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²´í¬ ë° ê²½ê³ 
                        def check_duplicates(df):
                            cols = df.columns.tolist()
                            # _ìˆ«ìë¡œ ëë‚˜ëŠ” ì¤‘ë³µ íŒ¨í„´ ì°¾ê¸°
                            import re
                            dup_pattern = {}
                            for col in cols:
                                match = re.match(r'(.+)_(\d+)$', col)
                                if match:
                                    base = match.group(1)
                                    if base not in dup_pattern:
                                        dup_pattern[base] = []
                                    dup_pattern[base].append(col)
                            return dup_pattern
                        
                        duplicate_info = None
                        if len(all_data) == 1:
                            duplicate_info = check_duplicates(list(all_data.values())[0])
                        
                        # ë©”íƒ€ë°ì´í„° ì²´í¬
                        metadata_info = []
                        if 'metadata' in st.session_state and st.session_state.metadata:
                            header_meta = st.session_state.metadata.get('header_metadata', {})
                            if header_meta:
                                if 'description' in header_meta:
                                    metadata_info.append(f"âœ… Description: {len(header_meta['description'])}ê°œ")
                                if 'unit' in header_meta:
                                    metadata_info.append(f"âœ… Unit: {len(header_meta['unit'])}ê°œ")
                                if 'tag_name' in header_meta:
                                    metadata_info.append(f"âœ… Tag_name: {len(header_meta['tag_name'])}ê°œ")
                                if 'id' in header_meta:
                                    metadata_info.append(f"âœ… ID: {len(header_meta['id'])}ê°œ")
                        
                        st.success("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
                        
                        # ë©”íƒ€ë°ì´í„° ì •ë³´ í‘œì‹œ
                        if metadata_info:
                            st.info("ğŸ“‹ **ë©”íƒ€ë°ì´í„° ë°œê²¬:**\n" + "\n".join(metadata_info))
                        else:
                            st.warning("âš ï¸ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. YAML ì„¤ì •ì—ì„œ header_rowsë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                        
                        # ì¤‘ë³µ ì»¬ëŸ¼ëª…ì´ ìˆì—ˆìœ¼ë©´ ê²½ê³  í‘œì‹œ
                        if duplicate_info and any(len(v) > 0 for v in duplicate_info.values()):
                            with st.warning("âš ï¸ ì¤‘ë³µëœ ì»¬ëŸ¼ëª…ì´ ë°œê²¬ë˜ì–´ ìë™ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤."):
                                st.caption("ì¤‘ë³µëœ ì»¬ëŸ¼ëª…ì— '_ìˆ«ì' ì ‘ë¯¸ì‚¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                dup_count = sum(len(v) for v in duplicate_info.values())
                                st.caption(f"ì´ {dup_count}ê°œì˜ ì¤‘ë³µ ë°œê²¬")
                        
                        st.rerun()
                    
                    except Exception as e:
                        st.error(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
        
        with col2:
            if st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
                # ëª¨ë“  ì„¸ì…˜ ìƒíƒœ ì™„ì „ ì œê±°
                keys_to_remove = ['loaded_data', 'metadata']
                for key in keys_to_remove:
                    if key in st.session_state:
                        del st.session_state[key]
                
                # ê°•ì œ ì¬ì‹œì‘
                st.rerun()
    
    # ë¡œë“œëœ ë°ì´í„° í‘œì‹œ
    if st.session_state.loaded_data is not None:
        st.divider()
        st.subheader("ğŸ“Š ë¡œë“œëœ ë°ì´í„°")
        
        import logging
        logger = logging.getLogger(__name__)
        
        # session_stateì—ëŠ” ì´ë¯¸ ì •ë¦¬ëœ ë°ì´í„°ê°€ ì €ì¥ë¨
        data = st.session_state.loaded_data
        
        # ë‹¨ì¼ DataFrame
        if isinstance(data, pd.DataFrame):
            st.markdown(f"**Shape:** {data.shape[0]:,} rows Ã— {data.shape[1]:,} columns")
            
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            with st.expander("ğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                logger.info(f"ğŸ“Š [ë¯¸ë¦¬ë³´ê¸°] ì‹œì‘: data.shape={data.shape}")
                logger.info(f"   dataì˜ timestamp dtype: {data['timestamp'].dtype if 'timestamp' in data.columns else 'N/A'}")
                
                n_rows = st.slider("í‘œì‹œí•  í–‰ ìˆ˜", 5, 100, 10, key='single_preview_rows')
                
                # safe_display_dfê°€ ëª¨ë“  ë³€í™˜ ì²˜ë¦¬
                preview_head = data.head(n_rows)
                
                logger.info(f"   .head() í›„ - shape={preview_head.shape}")
                logger.info(f"   safe_display_df í˜¸ì¶œ ì „")
                
                st.dataframe(safe_display_df(preview_head), use_container_width=True)
            
            # ë©”íƒ€ë°ì´í„°
            if st.session_state.metadata:
                with st.expander("â„¹ï¸ ë©”íƒ€ë°ì´í„°"):
                    meta = st.session_state.metadata
                    if 'header_metadata' in meta and meta['header_metadata']:
                        st.json(meta['header_metadata'])

            # ì œê±°ëœ ì»¬ëŸ¼ ì •ë³´
            if st.session_state.metadata and 'removed_columns' in st.session_state.metadata:
                removed_cols = st.session_state.metadata['removed_columns']
                if removed_cols and len(removed_cols) > 0:
                    with st.expander(f"ğŸ—‘ï¸ ì œê±°ëœ ì¤‘ë³µ ì»¬ëŸ¼ ({len(removed_cols)}ê°œ)", expanded=True):
                        st.warning(f"âš ï¸ ì´ {len(removed_cols)}ê°œì˜ ì¤‘ë³µ ì»¬ëŸ¼ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")

                        # í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œ
                        removed_df = pd.DataFrame(removed_cols)

                        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
                        cols_order = ['tag_name', 'description', 'unit', 'reason']
                        cols_order = [c for c in cols_order if c in removed_df.columns]
                        removed_df = removed_df[cols_order]

                        # ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½
                        removed_df.columns = ['íƒœê·¸ëª…', 'ì„¤ëª…', 'ë‹¨ìœ„', 'ì œê±° ì´ìœ '][:len(removed_df.columns)]

                        st.dataframe(removed_df, use_container_width=True, hide_index=True)

                        # ì œê±° ì´ìœ ë³„ í†µê³„
                        st.caption("**ì œê±° ì´ìœ ë³„ í†µê³„:**")
                        reason_counts = pd.DataFrame(removed_cols)['reason'].value_counts()
                        for reason, count in reason_counts.items():
                            st.caption(f"  - {reason}: {count}ê°œ")

            # ë°ì´í„° í†µê³„ (ì›ë³¸ display_df ì‚¬ìš© - datetime ìœ ì§€)
            with st.expander("ğŸ“ˆ ê¸°ë³¸ í†µê³„"):
                logger.info(f"ğŸ“Š [ê¸°ë³¸í†µê³„] ì‹œì‘")
                
                # dataëŠ” ì´ë¯¸ prepare_df_for_displayë¡œ ì •ë¦¬ë¨
                stats_df = data.describe(include='all').reset_index()
                
                logger.info(f"   describe() í›„ stats_df.shape={stats_df.shape}")
                # logger.info(f"   stats_df dtypes: {dict(stats_df.dtypes)}")
                logger.info(f"   safe_display_df í˜¸ì¶œ ì „")
                
                st.dataframe(safe_display_df(stats_df), use_container_width=True)
            
            # ì €ì¥ ì˜µì…˜
            st.divider()
            st.subheader("ğŸ’¾ ë°ì´í„° ì €ì¥")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                save_format = st.selectbox("ì €ì¥ í˜•ì‹", ['hdf5', 'parquet', 'csv', 'excel'], key='single_save_format')
            
            with col2:
                file_name = st.text_input("íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)", value="output_data", key='single_file_name')
                include_date = st.checkbox("íŒŒì¼ëª…ì— ë‚ ì§œ ì¶”ê°€", value=False,
                                         help="ë°ì´í„°ì˜ timestampì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª…ì— ì¶”ê°€í•©ë‹ˆë‹¤ (ì˜ˆ: output_data_20250101_20250131.parquet)",
                                         key='single_include_date')
            
            with col3:
                st.write("")  # ê°„ê²©
                st.write("")  # ê°„ê²©
                if st.button("ğŸ’¾ ì €ì¥", type="primary", use_container_width=True, key='single_save_btn'):
                    try:
                        # timestampì—ì„œ ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
                        date_str = extract_date_range_from_df(data) if include_date else ''
                        
                        if save_format == 'hdf5':
                            # HDF5ëŠ” íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•˜ë¯€ë¡œ ì„ì‹œ íŒŒì¼ ì‚¬ìš©
                            import tempfile
                            import os
                            
                            # HDF5 ì €ì¥ ì „ ë°ì´í„° íƒ€ì… ì •ë¦¬
                            data_copy = prepare_df_for_parquet(data)
                            
                            with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp:
                                tmp_path = tmp.name
                            
                            try:
                                # HDF5ë¡œ ì €ì¥
                                save_to_hdf5_with_metadata(data_copy, tmp_path, key='data', compression='gzip')
                                
                                # íŒŒì¼ ì½ê¸°
                                with open(tmp_path, 'rb') as f:
                                    buffer = io.BytesIO(f.read())
                                
                                mime = 'application/x-hdf5'
                                ext = '.h5'
                            finally:
                                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                                if os.path.exists(tmp_path):
                                    os.unlink(tmp_path)
                        
                        elif save_format == 'parquet':
                            buffer = io.BytesIO()
                            # Parquet ì €ì¥ ì „ ë°ì´í„° íƒ€ì… ì •ë¦¬
                            data_copy = prepare_df_for_parquet(data)
                            
                            # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
                            save_to_parquet_with_metadata(data_copy, buffer, compression='snappy')
                            
                            mime = 'application/octet-stream'
                            ext = '.parquet'
                        elif save_format == 'csv':
                            buffer = io.BytesIO()
                            data.to_csv(buffer, index=False, encoding='utf-8-sig')
                            mime = 'text/csv'
                            ext = '.csv'
                        else:  # excel
                            buffer = io.BytesIO()
                            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                                data.to_excel(writer, index=False, sheet_name='Data')
                            mime = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                            ext = '.xlsx'
                        
                        buffer.seek(0)
                        
                        # íŒŒì¼ëª… ìƒì„±
                        if date_str:
                            download_name = f"{file_name}_{date_str}{ext}"
                        else:
                            download_name = f"{file_name}{ext}"
                        
                        st.download_button(
                            label=f"â¬‡ï¸ {download_name} ë‹¤ìš´ë¡œë“œ",
                            data=buffer,
                            file_name=download_name,
                            mime=mime,
                            use_container_width=True
                        )
                        
                        # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€
                        st.success(f"âœ… {save_format.upper()} íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ!")
                        
                        # ë©”íƒ€ë°ì´í„° ì €ì¥ í™•ì¸ (HDF5ì¸ ê²½ìš°ë§Œ)
                        if save_format == 'hdf5' and hasattr(data, 'attrs') and 'header_metadata' in data.attrs:
                            header_meta = data.attrs['header_metadata']
                            meta_saved = []
                            if 'description' in header_meta:
                                meta_saved.append(f"Description ({len(header_meta['description'])}ê°œ)")
                            if 'unit' in header_meta:
                                meta_saved.append(f"Unit ({len(header_meta['unit'])}ê°œ)")
                            if 'tag_name' in header_meta:
                                meta_saved.append(f"Tag_name ({len(header_meta['tag_name'])}ê°œ)")
                            if 'id' in header_meta:
                                meta_saved.append(f"ID ({len(header_meta['id'])}ê°œ)")
                            
                            if meta_saved:
                                st.info(f"ğŸ’¾ **ë©”íƒ€ë°ì´í„° ì €ì¥ë¨:** {', '.join(meta_saved)}")
                    
                    except Exception as e:
                        st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        # ë‹¤ì¤‘ DataFrame (ì‹œíŠ¸ë³„)
        else:
            st.markdown(f"**ì´ {len(data)}ê°œ ì‹œíŠ¸ ë¡œë“œë¨**")
            
            selected_sheet = st.selectbox("ì‹œíŠ¸ ì„ íƒ", options=list(data.keys()), key='loading_sheet_select')
            df = data[selected_sheet]
            
            st.markdown(f"**Shape:** {df.shape[0]:,} rows Ã— {df.shape[1]:,} columns")
            
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            with st.expander("ğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                n_rows = st.slider("í‘œì‹œí•  í–‰ ìˆ˜", 5, 100, 10, key='multi_sheet_preview_rows')
                
                # safe_display_dfê°€ ëª¨ë“  ë³€í™˜ ì²˜ë¦¬
                st.dataframe(safe_display_df(df.head(n_rows)), use_container_width=True)
            
            # ë©”íƒ€ë°ì´í„°
            if st.session_state.metadata and selected_sheet in st.session_state.metadata:
                with st.expander("â„¹ï¸ ë©”íƒ€ë°ì´í„°"):
                    meta = st.session_state.metadata[selected_sheet]
                    if 'header_metadata' in meta and meta['header_metadata']:
                        st.json(meta['header_metadata'])

            # ì œê±°ëœ ì»¬ëŸ¼ ì •ë³´ (ë‹¤ì¤‘ ì‹œíŠ¸)
            if st.session_state.metadata and selected_sheet in st.session_state.metadata:
                meta = st.session_state.metadata[selected_sheet]
                if 'removed_columns' in meta:
                    removed_cols = meta['removed_columns']
                    if removed_cols and len(removed_cols) > 0:
                        with st.expander(f"ğŸ—‘ï¸ ì œê±°ëœ ì¤‘ë³µ ì»¬ëŸ¼ ({len(removed_cols)}ê°œ)", expanded=True):
                            st.warning(f"âš ï¸ ì´ {len(removed_cols)}ê°œì˜ ì¤‘ë³µ ì»¬ëŸ¼ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")

                            # í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œ
                            removed_df = pd.DataFrame(removed_cols)

                            # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
                            cols_order = ['tag_name', 'description', 'unit', 'reason']
                            cols_order = [c for c in cols_order if c in removed_df.columns]
                            removed_df = removed_df[cols_order]

                            # ì»¬ëŸ¼ëª… í•œê¸€ë¡œ ë³€ê²½
                            removed_df.columns = ['íƒœê·¸ëª…', 'ì„¤ëª…', 'ë‹¨ìœ„', 'ì œê±° ì´ìœ '][:len(removed_df.columns)]

                            st.dataframe(removed_df, use_container_width=True, hide_index=True)

                            # ì œê±° ì´ìœ ë³„ í†µê³„
                            st.caption("**ì œê±° ì´ìœ ë³„ í†µê³„:**")
                            reason_counts = pd.DataFrame(removed_cols)['reason'].value_counts()
                            for reason, count in reason_counts.items():
                                st.caption(f"  - {reason}: {count}ê°œ")

            # ë°ì´í„° í†µê³„ (ì›ë³¸ ì‚¬ìš© - datetime ìœ ì§€)
            with st.expander("ğŸ“ˆ ê¸°ë³¸ í†µê³„"):
                stats_df = df.describe(include='all')
                st.dataframe(safe_display_df(stats_df), use_container_width=True)
            
            # ì €ì¥ ì˜µì…˜
            st.divider()
            st.subheader("ğŸ’¾ ë°ì´í„° ì €ì¥")
            
            # ì‹œíŠ¸ ì„ íƒ ì˜µì…˜
            st.markdown("**ğŸ“‹ ì €ì¥í•  ë°ì´í„° ì„ íƒ**")
            
            all_sheet_names = list(data.keys())
            
            col_select1, col_select2 = st.columns([3, 1])
            
            with col_select1:
                save_mode = st.radio(
                    "ì €ì¥ ëª¨ë“œ",
                    ["ê°œë³„ ì €ì¥", "ì„ íƒí•œ ì‹œíŠ¸ ë³‘í•©", "ëª¨ë“  ì‹œíŠ¸ ë³‘í•©"],
                    help="ê°œë³„ ì €ì¥: ê° ì‹œíŠ¸ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥\në³‘í•©: ì—¬ëŸ¬ ì‹œíŠ¸ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ê²°í•©",
                    horizontal=True,
                    key='multi_save_mode'
                )
            
            with col_select2:
                if save_mode != "ê°œë³„ ì €ì¥":
                    sort_by_time = st.checkbox(
                        "ì‹œê°„ìˆœ ì •ë ¬",
                        value=True,
                        help="timestampê°€ ìˆìœ¼ë©´ ì‹œê°„ìˆœìœ¼ë¡œ, ì—†ìœ¼ë©´ ì…ë ¥ ìˆœì„œëŒ€ë¡œ ë³‘í•©",
                        key='sort_by_time'
                    )
                else:
                    sort_by_time = False
            
            # ì„ íƒí•œ ì‹œíŠ¸ ë³‘í•© ëª¨ë“œì¼ ë•Œ ì„ íƒ UI
            selected_sheets = []
            if save_mode == "ì„ íƒí•œ ì‹œíŠ¸ ë³‘í•©":
                st.markdown("**ë³‘í•©í•  ì‹œíŠ¸ ì„ íƒ** (ìˆœì„œëŒ€ë¡œ ë³‘í•©ë©ë‹ˆë‹¤)")
                selected_sheets = st.multiselect(
                    "ì‹œíŠ¸ ì„ íƒ",
                    options=all_sheet_names,
                    default=all_sheet_names[:min(3, len(all_sheet_names))],
                    key='selected_sheets_to_merge'
                )
                if not selected_sheets:
                    st.warning("âš ï¸ ë³‘í•©í•  ì‹œíŠ¸ë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
            elif save_mode == "ëª¨ë“  ì‹œíŠ¸ ë³‘í•©":
                selected_sheets = all_sheet_names
                st.info(f"ğŸ“Š ì´ {len(selected_sheets)}ê°œ ì‹œíŠ¸ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.")
            
            st.divider()
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                save_format = st.selectbox("ì €ì¥ í˜•ì‹", ['hdf5', 'parquet', 'csv', 'excel'], key='multi_save_format')
                
                # ë³‘í•© ëª¨ë“œì—ì„œëŠ” ê°œë³„ íŒŒì¼ ì €ì¥ ì˜µì…˜ ìˆ¨ê¹€
                if save_mode == "ê°œë³„ ì €ì¥":
                    save_all = st.checkbox("ëª¨ë“  ì‹œíŠ¸ ì €ì¥", value=False, key='multi_save_all')
                else:
                    save_all = False
            
            with col2:
                file_name = st.text_input("íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)", value="output_data", key='multi_file_name')
                if save_mode == "ê°œë³„ ì €ì¥" and save_all and save_format in ['parquet', 'csv']:
                    include_date = st.checkbox("íŒŒì¼ëª…ì— ë‚ ì§œ ì¶”ê°€", value=True, 
                                             help="ê° ì‹œíŠ¸ì˜ timestampì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª…ì— ì¶”ê°€í•©ë‹ˆë‹¤",
                                             key='multi_include_date')
                elif save_mode != "ê°œë³„ ì €ì¥":
                    include_date = st.checkbox("íŒŒì¼ëª…ì— ë‚ ì§œ ì¶”ê°€", value=True,
                                             help="ë³‘í•©ëœ ë°ì´í„°ì˜ timestampì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª…ì— ì¶”ê°€í•©ë‹ˆë‹¤",
                                             key='multi_include_date_merged')
                else:
                    include_date = False
            
            with col3:
                st.write("")
                st.write("")
                if st.button("ğŸ’¾ ì €ì¥", type="primary", use_container_width=True, key='multi_save_btn'):
                    try:
                        # ë³‘í•© ëª¨ë“œ
                        if save_mode in ["ì„ íƒí•œ ì‹œíŠ¸ ë³‘í•©", "ëª¨ë“  ì‹œíŠ¸ ë³‘í•©"]:
                            if not selected_sheets:
                                st.error("âŒ ë³‘í•©í•  ì‹œíŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                            else:
                                # ì„ íƒëœ ì‹œíŠ¸ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©
                                dfs_to_merge = [data[sheet] for sheet in selected_sheets]
                                
                                # timestamp ì»¬ëŸ¼ ì°¾ê¸°
                                has_timestamp = False
                                ts_col = None
                                for df in dfs_to_merge:
                                    for col in df.columns:
                                        if 'timestamp' in str(col).lower() or 'datetime' in str(col).lower():
                                            if pd.api.types.is_datetime64_any_dtype(df[col]):
                                                has_timestamp = True
                                                ts_col = col
                                                break
                                    if has_timestamp:
                                        break
                                
                                # ë³‘í•©
                                merged_df = pd.concat(dfs_to_merge, ignore_index=True)
                                
                                # ì •ë ¬
                                if sort_by_time and has_timestamp and ts_col:
                                    merged_df = merged_df.sort_values(by=ts_col).reset_index(drop=True)
                                    st.success(f"âœ… {len(selected_sheets)}ê°œ ì‹œíŠ¸ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ë³‘í•©í–ˆìŠµë‹ˆë‹¤. (ì´ {len(merged_df)}í–‰)")
                                else:
                                    st.success(f"âœ… {len(selected_sheets)}ê°œ ì‹œíŠ¸ë¥¼ ì…ë ¥ ìˆœì„œëŒ€ë¡œ ë³‘í•©í–ˆìŠµë‹ˆë‹¤. (ì´ {len(merged_df)}í–‰)")
                                
                                # ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
                                date_str = extract_date_range_from_df(merged_df) if include_date else ''
                                
                                # íŒŒì¼ ìƒì„±
                                if save_format == 'hdf5':
                                    # HDF5ëŠ” ì„ì‹œ íŒŒì¼ í•„ìš”
                                    import tempfile
                                    import os
                                    
                                    # ë°ì´í„° íƒ€ì… ì •ë¦¬
                                    merged_df_copy = prepare_df_for_parquet(merged_df)
                                    
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp:
                                        tmp_path = tmp.name
                                    
                                    try:
                                        save_to_hdf5_with_metadata(merged_df_copy, tmp_path, key='data', compression='gzip')
                                        
                                        with open(tmp_path, 'rb') as f:
                                            buffer = io.BytesIO(f.read())
                                        
                                        mime = 'application/x-hdf5'
                                        ext = '.h5'
                                    finally:
                                        if os.path.exists(tmp_path):
                                            os.unlink(tmp_path)
                                
                                elif save_format == 'parquet':
                                    buffer = io.BytesIO()
                                    merged_df_copy = prepare_df_for_parquet(merged_df)
                                    save_to_parquet_with_metadata(merged_df_copy, buffer, compression='snappy')
                                    mime = 'application/octet-stream'
                                    ext = '.parquet'
                                elif save_format == 'csv':
                                    buffer = io.BytesIO()
                                    merged_df.to_csv(buffer, index=False, encoding='utf-8-sig')
                                    mime = 'text/csv'
                                    ext = '.csv'
                                else:  # excel
                                    buffer = io.BytesIO()
                                    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                                        merged_df.to_excel(writer, index=False, sheet_name='MergedData')
                                    mime = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                                    ext = '.xlsx'
                                
                                buffer.seek(0)
                                
                                # íŒŒì¼ëª… ìƒì„±
                                if date_str:
                                    download_name = f"{file_name}_{date_str}_merged{ext}"
                                else:
                                    download_name = f"{file_name}_merged{ext}"
                                
                                st.download_button(
                                    label=f"â¬‡ï¸ {download_name} ë‹¤ìš´ë¡œë“œ",
                                    data=buffer,
                                    file_name=download_name,
                                    mime=mime,
                                    use_container_width=True
                                )
                        
                        # ê°œë³„ ì €ì¥ ëª¨ë“œ
                        elif save_mode == "ê°œë³„ ì €ì¥":
                            if save_all:
                                # ëª¨ë“  ì‹œíŠ¸ë¥¼ í•˜ë‚˜ì˜ Excel íŒŒì¼ë¡œ
                                if save_format == 'excel':
                                    buffer = io.BytesIO()
                                    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                                        for sheet_name, sheet_df in data.items():
                                            safe_name = re.sub(r'[<>:"/\\|?*]', '_', sheet_name)[:31]
                                            sheet_df.to_excel(writer, index=False, sheet_name=safe_name)
                                    buffer.seek(0)
                                    
                                    st.download_button(
                                        label=f"â¬‡ï¸ {file_name}.xlsx ë‹¤ìš´ë¡œë“œ (ëª¨ë“  ì‹œíŠ¸)",
                                        data=buffer,
                                        file_name=f"{file_name}.xlsx",
                                        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                                        use_container_width=True
                                    )
                                    st.success("âœ… Excel íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ!")
                                
                                # ëª¨ë“  ì‹œíŠ¸ë¥¼ ê°œë³„ HDF5 íŒŒì¼ë¡œ
                                elif save_format == 'hdf5':
                                    st.info(f"ì´ {len(data)}ê°œì˜ HDF5 íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                                    
                                    import zipfile
                                    import tempfile
                                    import os
                                    zip_buffer = io.BytesIO()
                                    
                                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                                        for sheet_name, sheet_df in data.items():
                                            safe_name = re.sub(r'[<>:"/\\|?*]', '_', sheet_name)
                                            
                                            date_str = extract_date_range_from_df(sheet_df) if include_date else ''
                                            
                                            # ë°ì´í„° íƒ€ì… ì •ë¦¬
                                            sheet_df_copy = prepare_df_for_parquet(sheet_df)
                                            
                                            # ì„ì‹œ HDF5 íŒŒì¼ ìƒì„±
                                            with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp:
                                                tmp_path = tmp.name
                                            
                                            try:
                                                save_to_hdf5_with_metadata(sheet_df_copy, tmp_path, key='data', compression='gzip')
                                                
                                                with open(tmp_path, 'rb') as f:
                                                    h5_data = f.read()
                                                
                                                # íŒŒì¼ëª… ìƒì„±
                                                if date_str:
                                                    file_name_in_zip = f"{file_name}_{date_str}_{safe_name}.h5"
                                                else:
                                                    file_name_in_zip = f"{file_name}_{safe_name}.h5"
                                                
                                                zip_file.writestr(file_name_in_zip, h5_data)
                                            
                                            finally:
                                                if os.path.exists(tmp_path):
                                                    os.unlink(tmp_path)
                                    
                                    zip_buffer.seek(0)
                                    
                                    if include_date:
                                        first_sheet = list(data.values())[0]
                                        date_str = extract_date_range_from_df(first_sheet)
                                        if date_str:
                                            zip_name = f"{file_name}_{date_str}_all_sheets.zip"
                                        else:
                                            zip_name = f"{file_name}_all_sheets.zip"
                                    else:
                                        zip_name = f"{file_name}_all_sheets.zip"
                                    
                                    st.download_button(
                                        label=f"â¬‡ï¸ {zip_name} (ì „ì²´ ë‹¤ìš´ë¡œë“œ)",
                                        data=zip_buffer,
                                        file_name=zip_name,
                                        mime='application/zip',
                                        use_container_width=True
                                    )
                                    st.success(f"âœ… {len(data)}ê°œì˜ HDF5 íŒŒì¼ì´ ZIPìœ¼ë¡œ ì••ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                
                                # ëª¨ë“  ì‹œíŠ¸ë¥¼ ê°œë³„ Parquet íŒŒì¼ë¡œ
                                elif save_format == 'parquet':
                                    st.info(f"ì´ {len(data)}ê°œì˜ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                                    
                                    # ZIP íŒŒì¼ ìƒì„±
                                    import zipfile
                                    zip_buffer = io.BytesIO()
                                    
                                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                                        for sheet_name, sheet_df in data.items():
                                            safe_name = re.sub(r'[<>:"/\\|?*]', '_', sheet_name)
                                            
                                            # ê° ì‹œíŠ¸ì˜ ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
                                            date_str = extract_date_range_from_df(sheet_df) if include_date else ''
                                            
                                            # Parquet ì €ì¥ ì „ ë°ì´í„° íƒ€ì… ì •ë¦¬
                                            sheet_df_copy = prepare_df_for_parquet(sheet_df)
                                            
                                            # Parquet íŒŒì¼ ìƒì„±
                                            parquet_buffer = io.BytesIO()
                                            save_to_parquet_with_metadata(sheet_df_copy, parquet_buffer, compression='snappy')
                                            parquet_buffer.seek(0)
                                            
                                            # íŒŒì¼ëª… ìƒì„±
                                            if date_str:
                                                file_name_in_zip = f"{file_name}_{date_str}_{safe_name}.parquet"
                                            else:
                                                file_name_in_zip = f"{file_name}_{safe_name}.parquet"
                                            
                                            # ZIPì— ì¶”ê°€
                                            zip_file.writestr(file_name_in_zip, parquet_buffer.getvalue())
                                    
                                    zip_buffer.seek(0)
                                    
                                    # ZIP íŒŒì¼ëª… ìƒì„±
                                    if include_date:
                                        # ì²« ë²ˆì§¸ ì‹œíŠ¸ì˜ ë‚ ì§œ ì •ë³´ ì‚¬ìš©
                                        first_sheet = list(data.values())[0]
                                        date_str = extract_date_range_from_df(first_sheet)
                                        if date_str:
                                            zip_name = f"{file_name}_{date_str}_all_sheets.zip"
                                        else:
                                            zip_name = f"{file_name}_all_sheets.zip"
                                    else:
                                        zip_name = f"{file_name}_all_sheets.zip"
                                    
                                    st.download_button(
                                        label=f"â¬‡ï¸ {zip_name} (ì „ì²´ ë‹¤ìš´ë¡œë“œ)",
                                        data=zip_buffer,
                                        file_name=zip_name,
                                        mime='application/zip',
                                        use_container_width=True
                                    )
                                    
                                    st.success(f"âœ… {len(data)}ê°œ Parquet íŒŒì¼ì´ í¬í•¨ëœ ZIP íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ!")
                                    
                                    # ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜µì…˜
                                    with st.expander("ğŸ“„ ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)"):
                                        for sheet_name, sheet_df in data.items():
                                            safe_name = re.sub(r'[<>:"/\\|?*]', '_', sheet_name)
                                            
                                            # ê° ì‹œíŠ¸ì˜ ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
                                            date_str = extract_date_range_from_df(sheet_df) if include_date else ''
                                            
                                            # Parquet ì €ì¥ ì „ ë°ì´í„° íƒ€ì… ì •ë¦¬
                                            sheet_df_copy = prepare_df_for_parquet(sheet_df)
                                            
                                            buffer = io.BytesIO()
                                            save_to_parquet_with_metadata(sheet_df_copy, buffer, compression='snappy')
                                            buffer.seek(0)
                                            
                                            # íŒŒì¼ëª… ìƒì„±
                                            if date_str:
                                                download_name = f"{file_name}_{date_str}_{safe_name}.parquet"
                                            else:
                                                download_name = f"{file_name}_{safe_name}.parquet"
                                            
                                            st.download_button(
                                                label=f"â¬‡ï¸ {download_name}",
                                                data=buffer,
                                                file_name=download_name,
                                                mime='application/octet-stream',
                                                key=f'download_{sheet_name}',
                                                use_container_width=True
                                            )
                                
                                # ëª¨ë“  ì‹œíŠ¸ë¥¼ ê°œë³„ CSV íŒŒì¼ë¡œ
                                elif save_format == 'csv':
                                    st.info(f"ì´ {len(data)}ê°œì˜ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                                    
                                    # ZIP íŒŒì¼ ìƒì„±
                                    import zipfile
                                    zip_buffer = io.BytesIO()
                                    
                                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                                        for sheet_name, sheet_df in data.items():
                                            safe_name = re.sub(r'[<>:"/\\|?*]', '_', sheet_name)
                                            
                                            # ê° ì‹œíŠ¸ì˜ ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
                                            date_str = extract_date_range_from_df(sheet_df) if include_date else ''
                                            
                                            # CSV íŒŒì¼ ìƒì„±
                                            csv_buffer = io.BytesIO()
                                            sheet_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                                            csv_buffer.seek(0)
                                            
                                            # íŒŒì¼ëª… ìƒì„±
                                            if date_str:
                                                file_name_in_zip = f"{file_name}_{date_str}_{safe_name}.csv"
                                            else:
                                                file_name_in_zip = f"{file_name}_{safe_name}.csv"
                                            
                                            # ZIPì— ì¶”ê°€
                                            zip_file.writestr(file_name_in_zip, csv_buffer.getvalue())
                                    
                                    zip_buffer.seek(0)
                                    
                                    # ZIP íŒŒì¼ëª… ìƒì„±
                                    if include_date:
                                        # ì²« ë²ˆì§¸ ì‹œíŠ¸ì˜ ë‚ ì§œ ì •ë³´ ì‚¬ìš©
                                        first_sheet = list(data.values())[0]
                                        date_str = extract_date_range_from_df(first_sheet)
                                        if date_str:
                                            zip_name = f"{file_name}_{date_str}_all_sheets.zip"
                                        else:
                                            zip_name = f"{file_name}_all_sheets.zip"
                                    else:
                                        zip_name = f"{file_name}_all_sheets.zip"
                                    
                                    st.download_button(
                                        label=f"â¬‡ï¸ {zip_name} (ì „ì²´ ë‹¤ìš´ë¡œë“œ)",
                                        data=zip_buffer,
                                        file_name=zip_name,
                                        mime='application/zip',
                                        use_container_width=True
                                    )
                                    
                                    st.success(f"âœ… {len(data)}ê°œ CSV íŒŒì¼ì´ í¬í•¨ëœ ZIP íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ!")
                                    
                                    # ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜µì…˜
                                    with st.expander("ğŸ“„ ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)"):
                                        for sheet_name, sheet_df in data.items():
                                            safe_name = re.sub(r'[<>:"/\\|?*]', '_', sheet_name)
                                            
                                            # ê° ì‹œíŠ¸ì˜ ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
                                            date_str = extract_date_range_from_df(sheet_df) if include_date else ''
                                            
                                            buffer = io.BytesIO()
                                            sheet_df.to_csv(buffer, index=False, encoding='utf-8-sig')
                                            buffer.seek(0)
                                            
                                            # íŒŒì¼ëª… ìƒì„±
                                            if date_str:
                                                download_name = f"{file_name}_{date_str}_{safe_name}.csv"
                                            else:
                                                download_name = f"{file_name}_{safe_name}.csv"
                                            
                                            st.download_button(
                                                label=f"â¬‡ï¸ {download_name}",
                                                data=buffer,
                                                file_name=download_name,
                                                mime='text/csv',
                                                key=f'download_csv_{sheet_name}',
                                                use_container_width=True
                                            )
                                else:
                                    st.warning("âš ï¸ ë‹¤ì¤‘ ì‹œíŠ¸ëŠ” Excel ë˜ëŠ” ê°œë³„ íŒŒì¼ í˜•ì‹ìœ¼ë¡œë§Œ ì €ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                            else:
                                # ì„ íƒëœ ì‹œíŠ¸ë§Œ
                                # ë‚ ì§œ ë²”ìœ„ ì¶”ì¶œ
                                date_str = extract_date_range_from_df(df) if include_date else ''
                                
                                if save_format == 'hdf5':
                                    # HDF5ëŠ” ì„ì‹œ íŒŒì¼ í•„ìš”
                                    import tempfile
                                    import os
                                    
                                    # ë°ì´í„° íƒ€ì… ì •ë¦¬
                                    df_copy = prepare_df_for_parquet(df)
                                    
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp:
                                        tmp_path = tmp.name
                                    
                                    try:
                                        save_to_hdf5_with_metadata(df_copy, tmp_path, key='data', compression='gzip')
                                        
                                        with open(tmp_path, 'rb') as f:
                                            buffer = io.BytesIO(f.read())
                                        
                                        mime = 'application/x-hdf5'
                                        ext = '.h5'
                                    finally:
                                        if os.path.exists(tmp_path):
                                            os.unlink(tmp_path)
                                
                                elif save_format == 'parquet':
                                    buffer = io.BytesIO()
                                    # Parquet ì €ì¥ ì „ ë°ì´í„° íƒ€ì… ì •ë¦¬
                                    df_copy = prepare_df_for_parquet(df)
                                    save_to_parquet_with_metadata(df_copy, buffer, compression='snappy')
                                    mime = 'application/octet-stream'
                                    ext = '.parquet'
                                elif save_format == 'csv':
                                    buffer = io.BytesIO()
                                    df.to_csv(buffer, index=False, encoding='utf-8-sig')
                                    mime = 'text/csv'
                                    ext = '.csv'
                                else:  # excel
                                    buffer = io.BytesIO()
                                    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                                        df.to_excel(writer, index=False, sheet_name='Data')
                                    mime = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                                    ext = '.xlsx'
                                
                                buffer.seek(0)
                                
                                # íŒŒì¼ëª… ìƒì„±
                                if date_str:
                                    download_name = f"{file_name}_{date_str}_{selected_sheet}{ext}"
                                else:
                                    download_name = f"{file_name}_{selected_sheet}{ext}"
                                
                                st.download_button(
                                    label=f"â¬‡ï¸ {download_name} ë‹¤ìš´ë¡œë“œ",
                                    data=buffer,
                                    file_name=download_name,
                                    mime=mime,
                                    use_container_width=True
                            )
                            st.success(f"âœ… {save_format.upper()} íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ!")
                    
                    except Exception as e:
                        st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")


# ============================================
# Tab 3: ë°ì´í„° ê°€ì‹œí™”
# ============================================
def render_visualization_tab():
    st.header("ğŸ“Š ë°ì´í„° ê°€ì‹œí™”")
    
    # Parquet íŒŒì¼ ì—…ë¡œë“œ ì˜µì…˜
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_parquet = st.file_uploader(
            "Parquet/HDF5 íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ì„ íƒì‚¬í•­)",
            type=['parquet', 'h5', 'hdf5'],
            key='viz_parquet'
        )
    
    with col2:
        if uploaded_parquet:
            file_name = uploaded_parquet.name
            file_ext = file_name.split('.')[-1].lower()
            
            button_label = "ğŸ“¥ HDF5 ë¡œë“œ" if file_ext in ['h5', 'hdf5'] else "ğŸ“¥ Parquet ë¡œë“œ"
            
            if st.button(button_label, use_container_width=True):
                try:
                    if file_ext in ['h5', 'hdf5']:
                        # HDF5 íŒŒì¼ ë¡œë“œ
                        import tempfile
                        import os
                        
                        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (HDF5ëŠ” íŒŒì¼ ê²½ë¡œ í•„ìš”)
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp:
                            tmp.write(uploaded_parquet.read())
                            tmp_path = tmp.name
                        
                        try:
                            # HDF5ì—ì„œ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë¡œë“œ
                            df = load_from_hdf5_with_metadata(tmp_path, key='data')
                            
                            # ë©”íƒ€ë°ì´í„°ë¥¼ session_stateì— ì €ì¥
                            st.session_state.metadata = {
                                'source_name': df.attrs.get('source_name', 'hdf5_file'),
                                'header_metadata': df.attrs.get('header_metadata', {}),
                                'removed_columns': df.attrs.get('removed_columns', []),  # ì œê±°ëœ ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
                                'shape': df.shape,
                                'columns': df.columns.tolist(),
                                'dtypes': {str(k): str(v) for k, v in df.dtypes.items()}
                            }
                            
                            header_meta = df.attrs.get('header_metadata', {})
                            
                            msg_parts = ["âœ… HDF5 ë¡œë“œ ì™„ë£Œ!"]
                            if header_meta:
                                msg_parts.append(f"ë©”íƒ€ë°ì´í„°: {len(header_meta)}ê°œ í•„ë“œ")
                            
                            st.success(" | ".join(msg_parts))
                            
                            st.session_state.loaded_data = df
                            st.rerun()
                        finally:
                            if os.path.exists(tmp_path):
                                os.unlink(tmp_path)
                    
                    else:
                        # Parquet íŒŒì¼ ë¡œë“œ
                        import pyarrow.parquet as pq
                        import json
                        
                        # PyArrowë¡œ Parquet ì½ê¸° (ë©”íƒ€ë°ì´í„° í¬í•¨)
                        parquet_file = pq.read_table(uploaded_parquet)
                        
                        # DataFrameìœ¼ë¡œ ë³€í™˜
                        df = parquet_file.to_pandas()
                        
                        # ë©”íƒ€ë°ì´í„° ë³µì›
                        if parquet_file.schema.metadata:
                            metadata_bytes = parquet_file.schema.metadata
                            if b'pandas_attrs' in metadata_bytes:
                                attrs_json = metadata_bytes[b'pandas_attrs'].decode('utf-8')
                                df.attrs = json.loads(attrs_json)
                        
                        # ë©”íƒ€ë°ì´í„°ë¥¼ session_stateì— ì €ì¥
                        st.session_state.metadata = {
                            'source_name': df.attrs.get('source_name', 'parquet_file'),
                            'header_metadata': df.attrs.get('header_metadata', {}),
                            'removed_columns': df.attrs.get('removed_columns', []),  # ì œê±°ëœ ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
                            'shape': df.shape,
                            'columns': df.columns.tolist(),
                            'dtypes': {str(k): str(v) for k, v in df.dtypes.items()}
                        }
                        
                        # ë°ì´í„° íƒ€ì… í™•ì¸ ë° í‘œì‹œ
                        bool_cols = df.select_dtypes(include=['bool']).columns.tolist()
                        header_meta = df.attrs.get('header_metadata', {})
                        
                        msg_parts = ["âœ… Parquet ë¡œë“œ ì™„ë£Œ!"]
                        if bool_cols:
                            msg_parts.append(f"Boolean ì»¬ëŸ¼: {', '.join(bool_cols)}")
                        if header_meta:
                            msg_parts.append(f"ë©”íƒ€ë°ì´í„°: {len(header_meta)}ê°œ í•„ë“œ")
                        
                        st.success(" | ".join(msg_parts))
                        
                        st.session_state.loaded_data = df
                        st.rerun()
                        
                except Exception as e:
                    st.error(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    import traceback
                    st.code(traceback.format_exc())
    
    # ë¡œë“œëœ ë°ì´í„° í™•ì¸
    if st.session_state.loaded_data is None:
        st.info("â„¹ï¸ ë¨¼ì € 'ë°ì´í„° ë¡œë”©' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê±°ë‚˜, Parquet íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    data = st.session_state.loaded_data
    
    # ë‹¤ì¤‘ ì‹œíŠ¸ì¸ ê²½ìš° ì„ íƒ
    if isinstance(data, dict):
        selected_sheet = st.selectbox("ì‹œíŠ¸ ì„ íƒ", options=list(data.keys()), key='viz_sheet_select')
        df = data[selected_sheet]
    else:
        df = data
    
    st.divider()
    
    # ë©”íƒ€ë°ì´í„° ì •ë³´
    if st.session_state.metadata:
        with st.expander("â„¹ï¸ í—¤ë” ë©”íƒ€ë°ì´í„° í™•ì¸"):
            if isinstance(st.session_state.metadata, dict):
                if isinstance(data, dict):
                    meta = st.session_state.metadata.get(selected_sheet, {})
                else:
                    meta = st.session_state.metadata
                
                header_meta = meta.get('header_metadata', {})
                if header_meta:
                    st.markdown("**ì‚¬ìš© ê°€ëŠ¥í•œ í—¤ë” ì •ë³´:**")
                    
                    # ê° í—¤ë” íƒ€ì…ë³„ë¡œ í‘œì‹œ
                    if 'description' in header_meta:
                        with st.container():
                            st.markdown("##### ğŸ“ ì„¤ëª…(Description)")
                            desc_list = header_meta['description'][:10]  # ì²˜ìŒ 10ê°œë§Œ
                            st.write(", ".join([str(d) for d in desc_list if pd.notna(d)]))
                    
                    if 'unit' in header_meta:
                        with st.container():
                            st.markdown("##### ğŸ“ ë‹¨ìœ„(Unit)")
                            unit_list = header_meta['unit'][:10]  # ì²˜ìŒ 10ê°œë§Œ
                            st.write(", ".join([str(u) for u in unit_list if pd.notna(u)]))
                    
                    if 'tag_name' in header_meta:
                        with st.container():
                            st.markdown("##### ğŸ·ï¸ íƒœê·¸ëª…(Tag)")
                            tag_list = header_meta['tag_name'][:10]  # ì²˜ìŒ 10ê°œë§Œ
                            st.write(", ".join([str(t) for t in tag_list if pd.notna(t)]))
                    
                    # ì „ì²´ ë©”íƒ€ë°ì´í„° JSONìœ¼ë¡œ í‘œì‹œ
                    if st.checkbox("ì „ì²´ ë©”íƒ€ë°ì´í„° ë³´ê¸°", key='show_full_meta'):
                        st.json(header_meta)
                else:
                    st.info("ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ ì°¾ê¸° (Booleanë„ í¬í•¨)
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    bool_cols = df.select_dtypes(include=['bool']).columns.tolist()
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ ì¤‘ì—ì„œ 0/1ë§Œ ìˆëŠ” ì´ì§„ ë°ì´í„° ì°¾ê¸° (DIO ì‹ í˜¸)
    binary_cols = []
    for col in numeric_cols:
        unique_vals = df[col].dropna().unique()
        if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0, True, False}):
            binary_cols.append(col)
    
    # Boolean ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
    df_plot = df.copy()
    converted_cols = []
    
    if bool_cols:
        for col in bool_cols:
            df_plot[col] = df_plot[col].astype(float)  # True=1.0, False=0.0
            if col not in numeric_cols:
                numeric_cols.append(col)
                converted_cols.append(col)
    
    # ì •ë³´ ë©”ì‹œì§€ í‘œì‹œ
    if converted_cols or binary_cols:
        info_parts = []
        if converted_cols:
            info_parts.append(f"Boolean: {', '.join(converted_cols)}")
        if binary_cols:
            info_parts.append(f"ì´ì§„(0/1): {', '.join(binary_cols)}")
        # st.info(f"â„¹ï¸ DIO ì‹ í˜¸ ê°ì§€ â†’ {' | '.join(info_parts)} â†’ í”Œë¡¯ ê°€ëŠ¥")
    
    if not numeric_cols:
        st.warning("âš ï¸ ìˆ«ìí˜• ë˜ëŠ” Boolean ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹œê°í™” íƒ€ì… ì„ íƒ
    viz_type = st.radio(
        "ì‹œê°í™” íƒ€ì…",
        options=['ì‹œê³„ì—´ ê·¸ë˜í”„', 'ì‚°ì ë„', 'íˆìŠ¤í† ê·¸ë¨', 'ë°•ìŠ¤í”Œë¡¯', 'ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ'],
        horizontal=True
    )
    
    if viz_type == 'ì‹œê³„ì—´ ê·¸ë˜í”„':
        render_timeseries_plot(df_plot, numeric_cols, datetime_cols)
    elif viz_type == 'ì‚°ì ë„':
        render_scatter_plot(df_plot, numeric_cols)
    elif viz_type == 'íˆìŠ¤í† ê·¸ë¨':
        render_histogram(df_plot, numeric_cols)
    elif viz_type == 'ë°•ìŠ¤í”Œë¡¯':
        render_boxplot(df_plot, numeric_cols)
    else:  # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        render_correlation_heatmap(df_plot, numeric_cols)
