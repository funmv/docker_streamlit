"""
Îç∞Ïù¥ÌÑ∞ Ïú†Ìã∏Î¶¨Ìã∞ Î™®Îìà (ÏàòÏ†ïÎ≥∏)
YAML, Parquet, DataFrame Î≥ÄÌôò Í¥ÄÎ†® Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
HDF5 Ï†ÄÏû•/Î°úÎìú Î¨∏Ï†ú ÏàòÏ†ï
"""
import pandas as pd
import yaml
import json
import logging
from typing import Dict
from datetime import datetime, date


def dict_to_yaml_string(data: Dict) -> str:
    """ÎîïÏÖîÎÑàÎ¶¨Î•º YAML Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò"""
    return yaml.dump(data, default_flow_style=False, allow_unicode=True, sort_keys=False)


def yaml_string_to_dict(yaml_str: str) -> Dict:
    """YAML Î¨∏ÏûêÏó¥ÏùÑ ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò"""
    return yaml.safe_load(yaml_str)


def extract_date_range_from_df(df: pd.DataFrame) -> str:
    """
    DataFrameÏóêÏÑú timestamp Ïª¨ÎüºÏùò ÎÇ†Ïßú Î≤îÏúÑ Ï∂îÏ∂ú
    
    Returns:
        ÎÇ†Ïßú Î≤îÏúÑ Î¨∏ÏûêÏó¥ (Ïòà: "20250101_20250131" ÎòêÎäî "20250115")
    """
    ts_col = None
    for col in df.columns:
        if 'timestamp' in str(col).lower() or 'datetime' in str(col).lower() or 'date' in str(col).lower():
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                ts_col = col
                break
    
    if ts_col is None:
        return ""
    
    try:
        valid_dates = df[ts_col].dropna()
        
        if len(valid_dates) == 0:
            return ""
        
        min_date = valid_dates.min()
        max_date = valid_dates.max()
        
        min_str = pd.to_datetime(min_date).strftime('%Y%m%d')
        max_str = pd.to_datetime(max_date).strftime('%Y%m%d')
        
        if min_str == max_str:
            return min_str
        else:
            return f"{min_str}_{max_str}"
    except:
        return ""


def save_to_parquet_with_metadata(df: pd.DataFrame, buffer, compression='snappy'):
    """
    DataFrameÏùÑ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏôÄ Ìï®Íªò ParquetÏúºÎ°ú Ï†ÄÏû•
    
    Args:
        df: Ï†ÄÏû•Ìï† DataFrame (attrs Ìè¨Ìï®)
        buffer: Ï†ÄÏû•Ìï† Î≤ÑÌçº
        compression: ÏïïÏ∂ï Î∞©Ïãù
    """
    import pyarrow as pa
    import pyarrow.parquet as pq
    
    table = pa.Table.from_pandas(df)
    
    metadata = {}
    if hasattr(df, 'attrs') and df.attrs:
        metadata['pandas_attrs'] = json.dumps(df.attrs)
    
    existing_meta = table.schema.metadata or {}
    existing_meta.update({k.encode(): v.encode() if isinstance(v, str) else v 
                        for k, v in metadata.items()})
    
    new_schema = table.schema.with_metadata(existing_meta)
    table = table.cast(new_schema)
    
    pq.write_table(table, buffer, compression=compression)


def save_to_hdf5_with_metadata(df: pd.DataFrame, buffer, key='data', compression='gzip'):
    """
    DataFrameÏùÑ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏôÄ Ìï®Íªò HDF5Î°ú Ï†ÄÏû• (ÏàòÏ†ïÎ≥∏)
    
    Args:
        df: Ï†ÄÏû•Ìï† DataFrame (attrs Ìè¨Ìï®)
        buffer: Ï†ÄÏû•Ìï† Î≤ÑÌçº (ÌååÏùº Í≤ΩÎ°úÎßå ÏßÄÏõê, BytesIO Î∂àÍ∞Ä)
        key: HDF5 ÎÇ¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÇ§
        compression: ÏïïÏ∂ï Î∞©Ïãù ('gzip', 'lzf', 'blosc' Îì±)
    
    Í∞úÏÑ†ÏÇ¨Ìï≠:
        1. table format ÏÇ¨Ïö© (ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞Ïóê ÏïàÏ†ïÏ†Å)
        2. Î™ÖÏãúÏ†Å Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
        3. Ï†ÄÏû• ÌõÑ Ï¶âÏãú Í≤ÄÏ¶ù
    """
    logger = logging.getLogger(__name__)
    
    logger.info(f"=== HDF5 Ï†ÄÏû• ÏãúÏûë ===")
    logger.info(f"DataFrame shape: {df.shape}")
    logger.info(f"DataFrame ÌÅ¨Í∏∞: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # BytesIO Ï≤¥ÌÅ¨
    if hasattr(buffer, 'seek'):
        logger.error("‚ùå HDF5Îäî ÌååÏùº Í≤ΩÎ°úÎßå ÏßÄÏõêÌï©ÎãàÎã§. BytesIOÎäî ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
        raise ValueError("HDF5 Ï†ÄÏû•ÏùÄ ÌååÏùº Í≤ΩÎ°úÎßå ÏßÄÏõêÌï©ÎãàÎã§.")
    
    # üîπ Í∞úÏÑ† 1: table formatÏúºÎ°ú Î≥ÄÍ≤Ω (Îçî ÏïàÏ†ïÏ†Å)
    # fixedÎäî ÏûëÏùÄ Îç∞Ïù¥ÌÑ∞ÏóêÎäî Í¥úÏ∞ÆÏßÄÎßå, tableÏù¥ ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞Ïóê Îçî Ï†ÅÌï©
    try:
        df.to_hdf(
            buffer, 
            key=key, 
            mode='w',
            format='table',  # üîπ fixed ‚Üí table Î≥ÄÍ≤Ω
            complevel=9 if compression == 'gzip' else 0,
            complib=compression if compression != 'gzip' else 'zlib',
            data_columns=True  # üîπ Î™®Îì† Ïª¨ÎüºÏùÑ Îç∞Ïù¥ÌÑ∞ Ïª¨ÎüºÏúºÎ°ú (ÏøºÎ¶¨ Í∞ÄÎä•)
        )
        logger.info(f"‚úÖ DataFrame HDF5 Ï†ÄÏû• ÏôÑÎ£å (table format)")
    except Exception as e:
        logger.error(f"‚ùå table format Ï†ÄÏû• Ïã§Ìå®, fixed format ÏãúÎèÑ: {e}")
        # fallback: fixed format
        df.to_hdf(
            buffer, 
            key=key, 
            mode='w',
            format='fixed',
            complevel=9 if compression == 'gzip' else 0,
            complib=compression if compression != 'gzip' else 'zlib'
        )
        logger.info(f"‚úÖ DataFrame HDF5 Ï†ÄÏû• ÏôÑÎ£å (fixed format - fallback)")
    
    # üîπ Í∞úÏÑ† 2: Ï†ÄÏû• ÏßÅÌõÑ Í≤ÄÏ¶ù
    try:
        import tables
        with tables.open_file(buffer, 'r') as h5file:
            stored_table = h5file.get_node(f'/{key}/table')
            stored_rows = stored_table.nrows
            logger.info(f"üìä Ï†ÄÏû• Í≤ÄÏ¶ù: ÏõêÎ≥∏ {len(df)}Ìñâ ‚Üí HDF5 {stored_rows}Ìñâ")
            
            if stored_rows != len(df):
                logger.error(f"‚ùå Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ Í∞êÏßÄ! ÏõêÎ≥∏: {len(df)}, Ï†ÄÏû•Îê®: {stored_rows}")
            else:
                logger.info(f"‚úÖ Îç∞Ïù¥ÌÑ∞ Î¨¥Í≤∞ÏÑ± ÌôïÏù∏ ÏôÑÎ£å")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Ï†ÄÏû• Í≤ÄÏ¶ù Ïã§Ìå® (Ï†ÄÏû•ÏùÄ ÏôÑÎ£åÎê®): {e}")
    
    # üîπ Í∞úÏÑ† 3: Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Í∑∏Î£π ÏÜçÏÑ±ÏúºÎ°ú Ï†ÄÏû•
    if hasattr(df, 'attrs') and df.attrs:
        try:
            import tables
            
            with tables.open_file(buffer, 'r+') as h5file:
                # Í∑∏Î£π Í∞ÄÏ†∏Ïò§Í∏∞
                group = h5file.get_node(f'/{key}')
                
                # DataFrame.attrsÎ•º JSONÏúºÎ°ú ÏßÅÎ†¨ÌôîÌïòÏó¨ Ï†ÄÏû•
                attrs_json = json.dumps(df.attrs, default=str, ensure_ascii=False)
                group._v_attrs.pandas_attrs = attrs_json
                
                logger.info(f"‚úÖ HDF5 Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å")
                
                # Ï†ÄÏû•Îêú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌôïÏù∏
                if 'header_metadata' in df.attrs:
                    header_meta = df.attrs['header_metadata']
                    saved_count = {}
                    for key_name, values in header_meta.items():
                        if isinstance(values, list):
                            count = len([v for v in values if pd.notna(v) and v is not None])
                            saved_count[key_name] = count
                    logger.info(f"Ï†ÄÏû•Îêú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {saved_count}")
        
        except Exception as e:
            logger.error(f"‚ùå HDF5 Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ïã§Ìå®: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    logger.info(f"‚úÖ HDF5 Ï†ÄÏû• ÏôÑÎ£å: shape={df.shape}, key='{key}'")


def load_from_hdf5_with_metadata(file_path, key='data'):
    """
    HDF5ÏóêÏÑú DataFrameÍ≥º Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ìï®Íªò Î°úÎìú (ÏàòÏ†ïÎ≥∏)
    
    Args:
        file_path: HDF5 ÌååÏùº Í≤ΩÎ°ú
        key: HDF5 ÎÇ¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÇ§
    
    Returns:
        DataFrame (attrsÏóê Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ìè¨Ìï®)
    
    Í∞úÏÑ†ÏÇ¨Ìï≠:
        1. Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î≥¥Ïû•
        2. Î°úÎìú ÌõÑ Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
        3. Î™ÖÏãúÏ†Å ÏóêÎü¨ Ï≤òÎ¶¨
    """
    logger = logging.getLogger(__name__)
    
    logger.info(f"=== HDF5 Î°úÎìú ÏãúÏûë: {file_path} ===")
    
    # üîπ Í∞úÏÑ† 1: ÌååÏùº Ï°¥Ïû¨ Î∞è ÌÅ¨Í∏∞ ÌôïÏù∏
    import os
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"HDF5 ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {file_path}")
    
    file_size = os.path.getsize(file_path)
    logger.info(f"HDF5 ÌååÏùº ÌÅ¨Í∏∞: {file_size / 1024**2:.2f} MB")
    
    # üîπ Í∞úÏÑ† 2: HDF5 ÌååÏùº Ï†ïÎ≥¥ Î®ºÏ†Ä ÌôïÏù∏
    try:
        import tables
        with tables.open_file(file_path, 'r') as h5file:
            if f'/{key}' not in h5file:
                available_keys = [node._v_pathname for node in h5file.walk_nodes("/")]
                logger.error(f"ÌÇ§ '{key}'Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÌÇ§: {available_keys}")
                raise KeyError(f"ÌÇ§ '{key}'Í∞Ä HDF5 ÌååÏùºÏóê ÏóÜÏäµÎãàÎã§")
            
            # table formatÏù∏ÏßÄ ÌôïÏù∏
            try:
                table_node = h5file.get_node(f'/{key}/table')
                expected_rows = table_node.nrows
                logger.info(f"HDF5 ÌååÏùº Ï†ïÎ≥¥: {expected_rows}Ìñâ (table format)")
            except:
                # fixed format
                logger.info(f"HDF5 ÌååÏùº Ï†ïÎ≥¥: fixed format")
                expected_rows = None
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è HDF5 ÌååÏùº Ï†ïÎ≥¥ ÌôïÏù∏ Ïã§Ìå® (Î°úÎìúÎäî Í≥ÑÏÜç ÏãúÎèÑ): {e}")
        expected_rows = None
    
    # üîπ Í∞úÏÑ† 3: DataFrame Î°úÎìú
    try:
        df = pd.read_hdf(file_path, key=key)
        logger.info(f"‚úÖ DataFrame Î°úÎìú ÏôÑÎ£å: shape={df.shape}")
        
        # Î°úÎìúÎêú Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
        if expected_rows is not None and len(df) != expected_rows:
            logger.error(f"‚ùå Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ Í∞êÏßÄ! ÏòàÏÉÅ: {expected_rows}Ìñâ, Î°úÎìúÎê®: {len(df)}Ìñâ")
        else:
            logger.info(f"‚úÖ Îç∞Ïù¥ÌÑ∞ Î¨¥Í≤∞ÏÑ± ÌôïÏù∏ ÏôÑÎ£å: {len(df)}Ìñâ")
    
    except Exception as e:
        logger.error(f"‚ùå DataFrame Î°úÎìú Ïã§Ìå®: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise
    
    # üîπ Í∞úÏÑ† 4: Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú (Í∑∏Î£π ÏÜçÏÑ±ÏóêÏÑú)
    try:
        import tables
        
        with tables.open_file(file_path, 'r') as h5file:
            group = h5file.get_node(f'/{key}')
            
            # pandas_attrs ÏÜçÏÑ± ÌôïÏù∏
            if hasattr(group._v_attrs, 'pandas_attrs'):
                attrs_json = group._v_attrs.pandas_attrs
                df.attrs = json.loads(attrs_json)
                
                logger.info(f"‚úÖ HDF5 Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å")
                logger.info(f"df.attrs keys: {list(df.attrs.keys())}")
                
                if 'header_metadata' in df.attrs:
                    header_meta = df.attrs['header_metadata']
                    loaded_count = {}
                    for key_name, values in header_meta.items():
                        if isinstance(values, list):
                            count = len([v for v in values if v is not None and str(v) != 'None'])
                            loaded_count[key_name] = count
                    logger.info(f"Î°úÎìúÎêú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞: {loaded_count}")
            else:
                logger.warning("‚ö†Ô∏è HDF5 ÌååÏùºÏóê pandas_attrsÍ∞Ä ÏóÜÏäµÎãàÎã§.")
    
    except Exception as e:
        logger.error(f"‚ùå HDF5 Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    logger.info(f"=== HDF5 Î°úÎìú ÏôÑÎ£å ===")
    return df


def safe_display_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    DataFrameÏùÑ st.dataframe()Ïóê ÏïàÏ†ÑÌïòÍ≤å ÌëúÏãúÌï† Ïàò ÏûàÎèÑÎ°ù Î≥ÄÌôò
    Î™®Îì† datetimeÏùÑ stringÏúºÎ°ú Î≥ÄÌôòÌïòÏó¨ PyArrow Ïò§Î•ò Î∞©ÏßÄ
    """
    logger = logging.getLogger(__name__)
    
    # attrs Î∞±ÏóÖ
    original_attrs = df.attrs.copy() if hasattr(df, 'attrs') else {}
    
    new_data = {}
    
    for col in df.columns:
        # datetime ÌÉÄÏûÖ ‚Üí string
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            new_data[col] = df[col].astype(str).replace('NaT', '').replace('None', '')
        
        # object ÌÉÄÏûÖ Ï≤òÎ¶¨
        elif df[col].dtype == 'object':
            non_null = df[col].dropna()
            if len(non_null) > 0:
                first_val = non_null.iloc[0]
                
                if isinstance(first_val, (pd.Timestamp, type(pd.NaT), datetime, date)):
                    new_data[col] = df[col].astype(str).replace('NaT', '').replace('None', '')
                else:
                    try:
                        new_data[col] = df[col].astype(str)
                    except:
                        new_data[col] = df[col].copy()
            else:
                new_data[col] = df[col].copy()
        else:
            new_data[col] = df[col].copy()
    
    df_safe = pd.DataFrame(new_data, index=df.index)
    
    # attrs Î≥µÏõê
    df_safe.attrs = original_attrs
    
    return df_safe


def prepare_df_for_parquet(df: pd.DataFrame) -> pd.DataFrame:
    """
    DataFrameÏùÑ Parquet Ï†ÄÏû• Í∞ÄÎä•ÌïòÎèÑÎ°ù Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Ï†ïÎ¶¨
    Mixed object dtype Î¨∏Ï†ú Ìï¥Í≤∞
    """
    logger = logging.getLogger(__name__)
    
    # attrs Î∞±ÏóÖ
    original_attrs = df.attrs.copy() if hasattr(df, 'attrs') else {}
    
    df_copy = df.copy()
    
    logger.info(f"Parquet Ï§ÄÎπÑ: {df_copy.shape}")
    
    for col in df_copy.columns:
        # boolean ÌÉÄÏûÖÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
        if df_copy[col].dtype == 'bool':
            continue
        
        # Ïà´Ïûê ÌÉÄÏûÖÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
        if pd.api.types.is_numeric_dtype(df_copy[col]):
            continue
        
        # datetime ÌÉÄÏûÖÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
        if pd.api.types.is_datetime64_any_dtype(df_copy[col]):
            continue
        
        # object ÌÉÄÏûÖ Ï≤òÎ¶¨ (mixed ÌÉÄÏûÖ Ìè¨Ìï®)
        if df_copy[col].dtype == 'object':
            try:
                # Î™®Îì† Í∞íÏùÑ Î¨∏ÏûêÏó¥Î°ú Í∞ïÏ†ú Î≥ÄÌôò
                df_copy[col] = df_copy[col].astype(str)
                
                # 'nan', 'None', 'NaT' Î¨∏ÏûêÏó¥ÏùÑ Ïã§Ï†ú NaNÏúºÎ°ú Î≥ÄÌôò
                df_copy[col] = df_copy[col].replace(['nan', 'None', 'NaT', ''], pd.NA)
                
                # ÏÉòÌîå ÌôïÏù∏ ÌõÑ boolean Î≥ÄÌôò ÏãúÎèÑ
                sample = df_copy[col].dropna().head(100)
                if len(sample) > 0:
                    unique_vals = set(sample.str.lower().unique())
                    
                    # booleanÏúºÎ°ú Î≥ÄÌôò Í∞ÄÎä•ÌïúÏßÄ ÌôïÏù∏
                    if unique_vals.issubset({'true', 'false'}):
                        df_copy[col] = df_copy[col].str.lower().map({
                            'true': True, 
                            'false': False
                        })
                    else:
                        # Î¨∏ÏûêÏó¥Î°ú Ïú†ÏßÄ (Ïù¥ÎØ∏ Î≥ÄÌôòÎê®)
                        pass
            except Exception as e:
                # Î≥ÄÌôò Ïã§Ìå® Ïãú Í∞ïÏ†úÎ°ú Î¨∏ÏûêÏó¥ Î≥ÄÌôò
                logger.warning(f"Ïª¨Îüº '{col}' Î≥ÄÌôò Ïã§Ìå®, Î¨∏ÏûêÏó¥Î°ú Í∞ïÏ†ú Î≥ÄÌôò: {e}")
                df_copy[col] = df_copy[col].apply(lambda x: str(x) if pd.notna(x) else None)
    
    # attrs Î≥µÏõê
    df_copy.attrs = original_attrs
    
    return df_copy


def prepare_df_for_display(df: pd.DataFrame) -> pd.DataFrame:
    """
    DataFrameÏùÑ Streamlit ÌëúÏãú Í∞ÄÎä•ÌïòÎèÑÎ°ù ÌÉÄÏûÖ Ï†ïÎ¶¨
    """
    # attrs Î∞±ÏóÖ
    original_attrs = df.attrs.copy() if hasattr(df, 'attrs') else {}
    
    df_copy = df.copy(deep=True)
    
    for col in df_copy.columns:
        # datetime ÌÉÄÏûÖ Ï≤òÎ¶¨
        if pd.api.types.is_datetime64_any_dtype(df_copy[col]):
            try:
                df_copy[col] = pd.to_datetime(
                    df_copy[col].dt.strftime('%Y-%m-%d %H:%M:%S'),
                    format='%Y-%m-%d %H:%M:%S',
                    errors='coerce'
                )
            except:
                pass
        
        # object ÌÉÄÏûÖ Ï≤òÎ¶¨
        elif df_copy[col].dtype == 'object':
            try:
                non_null = df_copy[col].dropna()
                if len(non_null) == 0:
                    continue
                
                sample = non_null.head(100)
                has_timestamp = any(isinstance(x, (pd.Timestamp, type(pd.NaT))) for x in sample)
                
                if has_timestamp:
                    temp_col = pd.to_datetime(df_copy[col], errors='coerce')
                    df_copy[col] = pd.to_datetime(
                        temp_col.dt.strftime('%Y-%m-%d %H:%M:%S'),
                        format='%Y-%m-%d %H:%M:%S',
                        errors='coerce'
                    )
                    continue
                
                if 'timestamp' in str(col).lower() or 'datetime' in str(col).lower() or 'date' in str(col).lower():
                    try:
                        temp_col = pd.to_datetime(df_copy[col], errors='coerce')
                        if temp_col.notna().sum() > 0:
                            df_copy[col] = pd.to_datetime(
                                temp_col.dt.strftime('%Y-%m-%d %H:%M:%S'),
                                format='%Y-%m-%d %H:%M:%S',
                                errors='coerce'
                            )
                    except:
                        pass
            except:
                pass
    
    # attrs Î≥µÏõê
    df_copy.attrs = original_attrs
    
    return df_copy