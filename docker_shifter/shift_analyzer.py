import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import numpy as np
from typing import List, Dict, Optional, Tuple  
import matplotlib.pyplot as plt
import io
import zipfile
import json                    
from datetime import datetime, timedelta  


# =================================================================================
# í•œê¸€ í°íŠ¸ ì„¤ì •
# =================================================================================
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì • í•¨ìˆ˜"""
    try:
        from matplotlib import font_manager, rc
        # Windows í™˜ê²½
        font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
        rc('font', family=font_name)
        plt.rcParams['axes.unicode_minus'] = False
    except:
        try:
            # Linux í™˜ê²½
            from matplotlib import font_manager, rc
            font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
            font_name = font_manager.FontProperties(fname=font_path).get_name()
            rc('font', family=font_name)  
            plt.rcParams['axes.unicode_minus'] = False
        except:
            # í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
            plt.rcParams['axes.unicode_minus'] = False

# matplotlib ê²½ê³  ì œê±°ë¥¼ ìœ„í•œ ì„¤ì •
plt.rcParams['figure.max_open_warning'] = 50

# =================================================================================
# í˜ì´ì§€ ì„¤ì • ë° ì´ˆê¸°í™”
# =================================================================================
st.set_page_config(page_title="ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„", layout="wide")
setup_korean_font()

# =================================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =================================================================================
def load_feather_file(uploaded_file) -> pd.DataFrame:
    """Feather íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        df = pd.read_feather(uploaded_file)
        return df
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def apply_time_delay(df: pd.DataFrame, column: str, delay: int) -> pd.Series:
    """ì‹œê³„ì—´ ë°ì´í„°ì— ì‹œê°„ ì§€ì—°ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜"""
    if delay == 0:
        return df[column]
    elif delay > 0:
        # ì–‘ìˆ˜ ì§€ì—°: ë¯¸ë˜ ê°’ì„ í˜„ì¬ë¡œ ì´ë™ (ì•ìª½ì— NaN ì¶”ê°€)
        delayed_series = df[column].shift(-delay)
    else:
        # ìŒìˆ˜ ì§€ì—°: ê³¼ê±° ê°’ì„ í˜„ì¬ë¡œ ì´ë™ (ë’¤ìª½ì— NaN ì¶”ê°€)
        delayed_series = df[column].shift(-delay)
    
    return delayed_series

def get_data_segment(df: pd.DataFrame, num_segments: int = 3, selected_segment: int = 0) -> pd.DataFrame:
    """ë°ì´í„°ë¥¼ ë“±ë¶„í•˜ì—¬ ì„ íƒëœ êµ¬ê°„ë§Œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    total_length = len(df)
    segment_length = total_length // num_segments
    
    start_idx = selected_segment * segment_length
    
    # ë§ˆì§€ë§‰ êµ¬ê°„ì˜ ê²½ìš° ë‚¨ì€ ëª¨ë“  ë°ì´í„° í¬í•¨
    if selected_segment == num_segments - 1:
        end_idx = total_length
    else:
        end_idx = start_idx + segment_length
    
    return df.iloc[start_idx:end_idx].copy()

def create_multivariate_plot(df: pd.DataFrame, selected_cols: List[str], 
                           delays: Dict[str, int], downsample_rate: int = 1, 
                           crosshair: bool = True, num_segments: int = 3, 
                           selected_segment: int = 0) -> go.Figure:
    """ê¸°ë³¸ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ í”Œë¡¯ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    # ë°ì´í„° êµ¬ê°„ ì„ íƒ
    df_segment = get_data_segment(df, num_segments, selected_segment)
    
    fig = go.Figure()
    
    for col in selected_cols:
        delay = delays.get(col, 0)
        
        # 1ë‹¨ê³„: ì„ íƒëœ êµ¬ê°„ì—ì„œ ì‹œê°„ ì§€ì—° ì ìš©
        y_data = apply_time_delay(df_segment, col, delay)
        
        # 2ë‹¨ê³„: ì§€ì—° ì ìš©ëœ ë°ì´í„°ì— ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
        y = y_data.iloc[::downsample_rate]
        x = df_segment.index[::downsample_rate]
        
        # ì§€ì—°ê°’ì´ ìˆëŠ” ê²½ìš° ë ˆì´ë¸”ì— í‘œì‹œ
        label = f"{col} (delay: {delay})" if delay != 0 else col
        
        fig.add_trace(go.Scattergl(
            x=x,
            y=y,
            mode='lines',
            name=label,
            showlegend=True,
            hoverinfo='x',
            hovertemplate=''
        ))
    
    # êµ¬ê°„ ì •ë³´ë¥¼ ì œëª©ì— ì¶”ê°€
    segment_info = f"êµ¬ê°„ {selected_segment + 1}/{num_segments}"
    fig.update_layout(
        title=f"ğŸ“Š ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì‹ í˜¸ ë¶„ì„ ({segment_info})",
        dragmode="zoom",
        xaxis=dict(
            rangeslider=dict(visible=False),
            title="ì‹œê°„ ì¸ë±ìŠ¤"
        ),
        yaxis=dict(
            title="ì‹ í˜¸ ê°’"
        ),
        height=600
    )
    
    if crosshair:
        fig.update_layout(
            hovermode="x",
            xaxis=dict(
                showspikes=True,
                spikemode='across',
                spikesnap='cursor',
                spikecolor="red",
                spikethickness=1,
                title="ì‹œê°„ ì¸ë±ìŠ¤"
            ),
            yaxis=dict(
                showspikes=True,
                spikemode='across',
                spikesnap='cursor',
                spikecolor="blue",
                spikethickness=1,
                title="ì‹ í˜¸ ê°’"
            )
        )
    
    return fig

def create_combined_plot(df: pd.DataFrame, delay_cols: List[str], 
                        delays: Dict[str, int], reference_cols: List[str] = None,
                        downsample_rate: int = 1, crosshair: bool = True,
                        num_segments: int = 3, selected_segment: int = 0) -> go.Figure:
    """ì§€ì—° ì ìš©ëœ ì»¬ëŸ¼ê³¼ ê¸°ì¤€ ì»¬ëŸ¼ì„ í•¨ê»˜ í‘œì‹œí•˜ëŠ” í”Œë¡¯ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    # ë°ì´í„° êµ¬ê°„ ì„ íƒ
    df_segment = get_data_segment(df, num_segments, selected_segment)
    
    fig = go.Figure()
    
    # ì§€ì—° ì ìš©ëœ ì»¬ëŸ¼ë“¤ ì¶”ê°€
    for col in delay_cols:
        delay = delays.get(col, 0)
        
        # 1ë‹¨ê³„: ì„ íƒëœ êµ¬ê°„ì—ì„œ ì‹œê°„ ì§€ì—° ì ìš©
        y_data = apply_time_delay(df_segment, col, delay)
        
        # 2ë‹¨ê³„: ì§€ì—° ì ìš©ëœ ë°ì´í„°ì— ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
        y = y_data.iloc[::downsample_rate]
        x = df_segment.index[::downsample_rate]
        
        # ì§€ì—°ê°’ì´ ìˆëŠ” ê²½ìš° ë ˆì´ë¸”ì— í‘œì‹œ
        label = f"{col} (delay: {delay:+d})" if delay != 0 else f"{col} (original)"
        
        fig.add_trace(go.Scattergl(
            x=x,
            y=y,
            mode='lines',
            name=label,
            showlegend=True,
            hoverinfo='x',
            hovertemplate='',
            line=dict(width=2)  # ì§€ì—° ì ìš©ëœ ì‹ í˜¸ëŠ” ë‘êº¼ìš´ ì„ 
        ))
    
    # ê¸°ì¤€ ì»¬ëŸ¼ë“¤ ì¶”ê°€ (ì§€ì—° ì ìš© ì•ˆë¨)
    if reference_cols:
        for col in reference_cols:
            # 1ë‹¨ê³„: ì„ íƒëœ êµ¬ê°„ì˜ ì›ë³¸ ë°ì´í„° (ì§€ì—° ì ìš© ì•ˆí•¨)
            y_data = df_segment[col]
            
            # 2ë‹¨ê³„: ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
            y = y_data.iloc[::downsample_rate]
            x = df_segment.index[::downsample_rate]
            
            fig.add_trace(go.Scattergl(
                x=x,
                y=y,
                mode='lines',
                name=f"{col} (reference)",
                showlegend=True,
                hoverinfo='x',
                hovertemplate='',
                line=dict(width=1, dash='dot')  # ê¸°ì¤€ ì‹ í˜¸ëŠ” ì ì„ ìœ¼ë¡œ êµ¬ë¶„
            ))
    
    # êµ¬ê°„ ì •ë³´ë¥¼ ì œëª©ì— ì¶”ê°€
    segment_info = f"êµ¬ê°„ {selected_segment + 1}/{num_segments}"
    fig.update_layout(
        title=f"ğŸ“Š ì‹œê°„ ì§€ì—° ì ìš© ì‹ í˜¸ vs ê¸°ì¤€ ì‹ í˜¸ ë¹„êµ ({segment_info})",
        dragmode="zoom",
        xaxis=dict(
            rangeslider=dict(visible=False),
            title="ì‹œê°„ ì¸ë±ìŠ¤"
        ),
        yaxis=dict(
            title="ì‹ í˜¸ ê°’"
        ),
        height=600,
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1,
            xanchor="left",
            x=1.02
        )
    )
    
    if crosshair:
        fig.update_layout(
            hovermode="x",
            xaxis=dict(
                showspikes=True,
                spikemode='across',
                spikesnap='cursor',
                spikecolor="red",
                spikethickness=1,
                title="ì‹œê°„ ì¸ë±ìŠ¤"
            ),
            yaxis=dict(
                showspikes=True,
                spikemode='across',
                spikesnap='cursor',
                spikecolor="blue",
                spikethickness=1,
                title="ì‹ í˜¸ ê°’"
            )
        )
    
    return fig

def handle_file_upload(uploaded_files) -> None:
    """íŒŒì¼ ì—…ë¡œë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (íƒ­1ìš©)"""
    if uploaded_files:
        st.session_state.uploaded_files = uploaded_files
        st.session_state.current_file_index = 0
        st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

def handle_batch_file_upload(uploaded_files) -> None:
    """ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (íƒ­2ìš©)"""
    if uploaded_files:
        st.session_state.batch_uploaded_files = uploaded_files
        st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ì´ ë°°ì¹˜ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

def handle_multi_file_upload(uploaded_files) -> None:
    """ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (íƒ­3ìš©)"""
    if uploaded_files:
        st.session_state.multi_uploaded_files = uploaded_files
        st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ì´ ë‹¤ì¤‘ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

def create_multi_file_plot(selected_files: List, selected_features: List[str], 
                          downsample_rate: int = 1, crosshair: bool = True,
                          num_segments: int = 3, selected_segment: int = 0) -> go.Figure:
    """ì„ íƒëœ íŒŒì¼ë“¤ì˜ íŠ¹ì§•ë“¤ì„ í”Œë¡¯í•˜ëŠ” í•¨ìˆ˜ (íƒ­1,2 ë°©ì‹ê³¼ ë™ì¼)"""
    fig = go.Figure()
    
    # íŒŒì¼ë³„ë¡œ ì²˜ë¦¬
    for file in selected_files:
        try:
            df = load_feather_file(file)
            if df is None:
                continue
            
            # ë°ì´í„° êµ¬ê°„ ì„ íƒ
            df_segment = get_data_segment(df, num_segments, selected_segment)
            
            # ì„ íƒëœ íŠ¹ì§•ë“¤ ì²˜ë¦¬
            for feature in selected_features:
                if feature in df.columns:
                    # 1ë‹¨ê³„: ì„ íƒëœ êµ¬ê°„ì˜ ì›ë³¸ ë°ì´í„°
                    y_data = df_segment[feature]
                    
                    # 2ë‹¨ê³„: ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
                    y = y_data.iloc[::downsample_rate]
                    x = df_segment.index[::downsample_rate]
                    
                    # íŒŒì¼ëª…ê³¼ íŠ¹ì§•ëª…ì„ í¬í•¨í•œ ë ˆì´ë¸”
                    file_name = file.name.split('.')[0]  # í™•ì¥ì ì œê±°
                    label = f"{file_name}_{feature}"
                    
                    fig.add_trace(go.Scattergl(
                        x=x,
                        y=y,
                        mode='lines',
                        name=label,
                        showlegend=True,
                        hoverinfo='x',
                        hovertemplate=''
                    ))
                    
        except Exception as e:
            st.warning(f"âš ï¸ {file.name} í”Œë¡¯ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    # êµ¬ê°„ ì •ë³´ë¥¼ ì œëª©ì— ì¶”ê°€
    segment_info = f"êµ¬ê°„ {selected_segment + 1}/{num_segments}"
    fig.update_layout(
        title=f"ğŸ“Š ë‹¤ì¤‘ íŒŒì¼ íŠ¹ì§• ë¹„êµ ({segment_info})",
        dragmode="zoom",
        xaxis=dict(
            rangeslider=dict(visible=False),
            title="ì‹œê°„ ì¸ë±ìŠ¤"
        ),
        yaxis=dict(
            title="ì‹ í˜¸ ê°’"
        ),
        height=600
    )
    
    if crosshair:
        fig.update_layout(
            hovermode="x",
            xaxis=dict(
                showspikes=True,
                spikemode='across',
                spikesnap='cursor',
                spikecolor="red",
                spikethickness=1,
                title="ì‹œê°„ ì¸ë±ìŠ¤"
            ),
            yaxis=dict(
                showspikes=True,
                spikemode='across',
                spikesnap='cursor',
                spikecolor="blue",
                spikethickness=1,
                title="ì‹ í˜¸ ê°’"
            )
        )
    
    return fig

def process_batch_files(files: List, selected_features: List[str], delays: Dict[str, int]) -> List[Dict]:
    """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ íŒŒì¼ì— ì§€ì—° ì²˜ë¦¬ë¥¼ ì ìš©í•˜ëŠ” í•¨ìˆ˜"""
    processed_files = []
    
    for i, file in enumerate(files):
        try:
            # íŒŒì¼ ë¡œë“œ
            df = load_feather_file(file)
            if df is None:
                continue
            
            # ì„ íƒëœ íŠ¹ì§•ë“¤ì´ íŒŒì¼ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            missing_features = [feat for feat in selected_features if feat not in df.columns]
            if missing_features:
                st.warning(f"âš ï¸ {file.name}ì—ì„œ ëˆ„ë½ëœ íŠ¹ì§•: {missing_features}")
                continue
            
            # ì§€ì—° ì²˜ë¦¬ ì ìš©
            processed_df = df.copy()
            for feature in selected_features:
                delay = delays.get(feature, 0)
                if delay != 0:
                    shifted_series = apply_time_delay(df, feature, delay)
                    processed_df[feature] = shifted_series
            
            # ì²˜ë¦¬ëœ ë°ì´í„° ì •ë³´ ì €ì¥
            processed_files.append({
                'original_name': file.name,
                'processed_name': f"{file.name.split('.')[0]}_batch_shifted.feather",
                'dataframe': processed_df,
                'shape': processed_df.shape,
                'applied_delays': {feat: delays[feat] for feat in selected_features if delays.get(feat, 0) != 0}
            })
            
        except Exception as e:
            st.error(f"âŒ {file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    return processed_files

def create_zip_download(processed_files: List[Dict], zip_filename: str) -> bytes:
    """ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì„ ZIPìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“œëŠ” í•¨ìˆ˜"""
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for file_info in processed_files:
            # DataFrameì„ feather í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            feather_buffer = io.BytesIO()
            file_info['dataframe'].reset_index(drop=True).to_feather(feather_buffer)
            feather_buffer.seek(0)
            
            # ZIPì— íŒŒì¼ ì¶”ê°€
            zip_file.writestr(file_info['processed_name'], feather_buffer.getvalue())
    
    zip_buffer.seek(0)
    return zip_buffer.getvalue()



# DNN ê´€ë ¨ ëª¨ë“  í•¨ìˆ˜ë“¤ - create_zip_download í•¨ìˆ˜ ë’¤ì— ì¶”ê°€í•˜ì„¸ìš”
def create_positional_encoding(position: int, d_model: int = 8) -> np.ndarray:
    """ì‹œê°„ í¬ì§€ì…˜ì— ëŒ€í•œ positional encoding ìƒì„±"""
    pe = np.zeros(d_model)
    for i in range(0, d_model, 2):
        pe[i] = np.sin(position / (10000 ** (i / d_model)))
        if i + 1 < d_model:
            pe[i + 1] = np.cos(position / (10000 ** (i / d_model)))
    return pe

def extract_time_features(timestamp_value, use_positional_encoding: bool = True) -> np.ndarray:
    """timestampë¡œë¶€í„° ì‹œê°„ íŠ¹ì§• ì¶”ì¶œ"""
    
    # Timestamp íƒ€ì…ì„ ìˆ«ìë¡œ ë³€í™˜
    if hasattr(timestamp_value, 'timestamp'):
        # pandas Timestamp ê°ì²´ì¸ ê²½ìš°
        timestamp_seconds = timestamp_value.timestamp()
    elif isinstance(timestamp_value, (int, float)):
        # ì´ë¯¸ ìˆ«ìì¸ ê²½ìš°
        timestamp_seconds = float(timestamp_value)
    else:
        try:
            # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì¸ ê²½ìš° pandasë¡œ ë³€í™˜ ì‹œë„
            timestamp_seconds = pd.to_datetime(timestamp_value).timestamp()
        except:
            # ë³€í™˜ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
            timestamp_seconds = 0.0
    
    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì§• (ì‹œ, ë¶„, ì´ˆ)
    hours = int((timestamp_seconds // 3600) % 24)
    minutes = int((timestamp_seconds % 3600) // 60) 
    seconds = int(timestamp_seconds % 60)
    
    # ì •ê·œí™”ëœ ì‹œê°„ íŠ¹ì§• (0-1 ë²”ìœ„)
    time_features = np.array([
        hours / 23.0,           # ì‹œê°„ (0-23 -> 0-1)
        minutes / 59.0,         # ë¶„ (0-59 -> 0-1)
        seconds / 59.0          # ì´ˆ (0-59 -> 0-1)
    ])
    
    if use_positional_encoding:
        # Positional encoding ì¶”ê°€
        pe = create_positional_encoding(int(timestamp_seconds // 5))  # 5ì´ˆ ë‹¨ìœ„
        time_features = np.concatenate([time_features, pe])
    
    return time_features

def split_files_train_val(files: List, train_ratio: float = 0.8) -> Tuple[List, List]:
    """íŒŒì¼ë“¤ì„ í›ˆë ¨ìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• """
    total_files = len(files)
    train_size = int(total_files * train_ratio)
    
    # íŒŒì¼ë“¤ì„ ì„ì–´ì„œ ë¶„í• 
    import random
    shuffled_files = files.copy()
    random.shuffle(shuffled_files)
    
    train_files = shuffled_files[:train_size]
    val_files = shuffled_files[train_size:]
    
    return train_files, val_files



def extract_dnn_samples_optimized(df: pd.DataFrame, start_pos: int, end_pos: int, 
                                  lookback: int, horizon: int, step_gap: int = 1,
                                  timestamp_col: str = None, use_positional_encoding: bool = True) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:
    """ìµœì í™”ëœ ë‹¨ì¼ íŒŒì¼ì—ì„œ DNN í•™ìŠµìš© ìƒ˜í”Œ ì¶”ì¶œ (ë²¡í„°í™” ì—°ì‚° ì‚¬ìš©)"""
    
    # timestamp ì»¬ëŸ¼ í™•ì¸
    if timestamp_col is None:
        # timestamp ê´€ë ¨ ì»¬ëŸ¼ ìë™ ê²€ìƒ‰
        timestamp_candidates = [col for col in df.columns if 'time' in col.lower() or 'timestamp' in col.lower()]
        if timestamp_candidates:
            timestamp_col = timestamp_candidates[0]
        else:
            timestamp_col = df.columns[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ timestampë¡œ ì‚¬ìš©
    
    # íŠ¹ì§• ì»¬ëŸ¼ë“¤ (timestamp ì œì™¸)
    feature_cols = [col for col in df.columns if col != timestamp_col]
    
    # ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ í–¥ìƒ)
    data_features_array = df[feature_cols].values.astype(np.float32)  # float32ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬ (í•œ ë²ˆì— ì²˜ë¦¬)
    data_features_array = np.nan_to_num(data_features_array, nan=0.0)
    
    # timestamp ë°°ì—´ ì¤€ë¹„
    if timestamp_col in df.columns:
        timestamp_array = df[timestamp_col].values
        # timestamp ê²°ì¸¡ê°’ ì²˜ë¦¬
        nan_mask = pd.isna(timestamp_array)
        if nan_mask.any():
            # ê²°ì¸¡ê°’ì„ ì¸ë±ìŠ¤ * 5ì´ˆë¡œ ëŒ€ì²´
            timestamp_array = np.where(nan_mask, np.arange(len(df)) * 5, timestamp_array)
    else:
        # timestamp ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ * 5ì´ˆë¡œ ìƒì„±
        timestamp_array = np.arange(len(df)) * 5
    
    # ì‹œê°„ íŠ¹ì§• ë°°ì—´ ë¯¸ë¦¬ ê³„ì‚° (ë²¡í„°í™”)
    time_features_array = extract_time_features_vectorized(timestamp_array, use_positional_encoding)
    
    # ë°ì´í„°ì™€ ì‹œê°„ íŠ¹ì§• ê²°í•©
    combined_features_array = np.concatenate([time_features_array, data_features_array], axis=1)
    
    # ìƒ˜í”Œ ì¶”ì¶œ ë²”ìœ„ ê³„ì‚°
    max_pos = min(end_pos, len(df) - horizon)
    actual_start = max(start_pos, lookback)
    
    # ìœ íš¨í•œ ìƒ˜í”Œ ìœ„ì¹˜ë“¤ ê³„ì‚°
    sample_positions = np.arange(actual_start, max_pos, step_gap)
    
    if len(sample_positions) == 0:
        return np.array([]), np.array([]), []
    
    # ì…ë ¥ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ìƒì„± (ë²¡í„°í™”)
    # shape: (num_samples, lookback)
    input_indices = sample_positions[:, np.newaxis] - np.arange(lookback, 0, -1)[np.newaxis, :]
    
    # ì¶œë ¥ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ìƒì„± (ë²¡í„°í™”)
    # shape: (num_samples, horizon)
    output_indices = sample_positions[:, np.newaxis] + np.arange(horizon)[np.newaxis, :]
    
    # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
    valid_input_mask = (input_indices >= 0) & (input_indices < len(combined_features_array))
    valid_output_mask = (output_indices >= 0) & (output_indices < len(combined_features_array))
    valid_samples_mask = valid_input_mask.all(axis=1) & valid_output_mask.all(axis=1)
    
    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì„ íƒ
    valid_sample_positions = sample_positions[valid_samples_mask]
    valid_input_indices = input_indices[valid_samples_mask]
    valid_output_indices = output_indices[valid_samples_mask]
    
    if len(valid_sample_positions) == 0:
        return np.array([]), np.array([]), []
    
    # ë²¡í„°í™”ëœ ì¸ë±ì‹±ìœ¼ë¡œ ìƒ˜í”Œ ì¶”ì¶œ
    # input_samples shape: (num_samples, lookback, features)
    input_samples = combined_features_array[valid_input_indices]
    
    # output_samples shape: (num_samples, horizon, features)
    output_samples = combined_features_array[valid_output_indices]
    
    # ìƒ˜í”Œ ì •ë³´ ìƒì„± (ë²¡í„°í™”)
    sample_info = []
    for i, pos in enumerate(valid_sample_positions):
        sample_info.append({
            'sample_index': i,
            'input_start': int(pos - lookback),
            'input_end': int(pos),
            'output_start': int(pos),
            'output_end': int(pos + horizon),
            'current_position': int(pos)
        })
    
    return input_samples.astype(np.float32), output_samples.astype(np.float32), sample_info


def extract_time_features_vectorized(timestamp_array: np.ndarray, use_positional_encoding: bool = True) -> np.ndarray:
    """ë²¡í„°í™”ëœ ì‹œê°„ íŠ¹ì§• ì¶”ì¶œ"""
    
    # Timestamp ë°°ì—´ì„ ìˆ«ìë¡œ ë³€í™˜
    timestamp_seconds = np.zeros_like(timestamp_array, dtype=np.float64)
    
    for i, timestamp_value in enumerate(timestamp_array):
        if hasattr(timestamp_value, 'timestamp'):
            # pandas Timestamp ê°ì²´ì¸ ê²½ìš°
            timestamp_seconds[i] = timestamp_value.timestamp()
        elif isinstance(timestamp_value, (int, float)):
            # ì´ë¯¸ ìˆ«ìì¸ ê²½ìš°
            timestamp_seconds[i] = float(timestamp_value)
        else:
            try:
                # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì¸ ê²½ìš° pandasë¡œ ë³€í™˜ ì‹œë„
                timestamp_seconds[i] = pd.to_datetime(timestamp_value).timestamp()
            except:
                # ë³€í™˜ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                timestamp_seconds[i] = 0.0
    
    # ë²¡í„°í™”ëœ ì‹œê°„ íŠ¹ì§• ê³„ì‚°
    hours = ((timestamp_seconds // 3600) % 24) / 23.0
    minutes = ((timestamp_seconds % 3600) // 60) / 59.0
    seconds = (timestamp_seconds % 60) / 59.0
    
    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì§•
    time_features = np.column_stack([hours, minutes, seconds])
    
    if use_positional_encoding:
        # Positional encoding ë²¡í„°í™”
        positions = (timestamp_seconds // 5).astype(int)  # 5ì´ˆ ë‹¨ìœ„
        pe_array = create_positional_encoding_vectorized(positions, d_model=8)
        time_features = np.concatenate([time_features, pe_array], axis=1)
    
    return time_features.astype(np.float32)


def create_positional_encoding_vectorized(positions: np.ndarray, d_model: int = 8) -> np.ndarray:
    """ë²¡í„°í™”ëœ positional encoding ìƒì„±"""
    
    # positions shape: (n,) -> (n, 1)
    pos = positions[:, np.newaxis]
    
    # ì¸ë±ìŠ¤ ë°°ì—´ ìƒì„±
    i = np.arange(0, d_model, 2)[np.newaxis, :]  # shape: (1, d_model//2)
    
    # ê°ë„ ê³„ì‚° (ë²¡í„°í™”)
    angles = pos / (10000 ** (i / d_model))  # shape: (n, d_model//2)
    
    # PE ë°°ì—´ ì´ˆê¸°í™”
    pe = np.zeros((len(positions), d_model), dtype=np.float32)
    
    # sinê³¼ cos ê³„ì‚° (ë²¡í„°í™”)
    pe[:, 0::2] = np.sin(angles)  # ì§ìˆ˜ ì¸ë±ìŠ¤
    if d_model % 2 == 1:
        pe[:, 1::2] = np.cos(angles[:, :-1])  # í™€ìˆ˜ ì¸ë±ìŠ¤ (ë§ˆì§€ë§‰ ì œì™¸)
    else:
        pe[:, 1::2] = np.cos(angles)  # í™€ìˆ˜ ì¸ë±ìŠ¤
    
    return pe


# ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ìµœì í™”ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ë˜í¼ í•¨ìˆ˜
def extract_dnn_samples(df: pd.DataFrame, start_pos: int, end_pos: int, 
                       lookback: int, horizon: int, step_gap: int = 1,
                       timestamp_col: str = None) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:
    """ê¸°ì¡´ í•¨ìˆ˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìµœì í™”ëœ ë²„ì „ í˜¸ì¶œ"""
    
    # use_positional_encodingì€ ì „ì—­ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ True ì‚¬ìš©
    try:
        # Streamlit ì„¸ì…˜ ìƒíƒœì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        import streamlit as st
        use_positional_encoding = st.session_state.get('dnn_pos_encoding', True)
    except:
        use_positional_encoding = True
    
    return extract_dnn_samples_optimized(
        df, start_pos, end_pos, lookback, horizon, step_gap, 
        timestamp_col, use_positional_encoding
    )



def process_all_files_for_dnn(train_files: List, val_files: List, 
                             start_pos: int, end_pos: int, lookback: int, 
                             horizon: int, step_gap: int) -> Dict:
    """ëª¨ë“  íŒŒì¼ì—ì„œ DNN ë°ì´í„° ì¶”ì¶œ"""
    
    train_inputs = []
    train_outputs = []
    train_info = []
    
    val_inputs = []
    val_outputs = []
    val_info = []
    
    # Training íŒŒì¼ë“¤ ì²˜ë¦¬
    st.write("ğŸ”„ Training ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    for i, file in enumerate(train_files):
        try:
            df = load_feather_file(file)
            if df is not None:
                input_arr, output_arr, info = extract_dnn_samples(
                    df, start_pos, end_pos, lookback, horizon, step_gap
                )
                
                if len(input_arr) > 0:
                    train_inputs.append(input_arr)
                    train_outputs.append(output_arr)
                    
                    # íŒŒì¼ ì •ë³´ ì¶”ê°€
                    for sample_info in info:
                        sample_info['file_name'] = file.name
                        sample_info['file_index'] = i
                        sample_info['split'] = 'train'
                    train_info.extend(info)
                    
                st.write(f"   âœ… {file.name}: {len(input_arr)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
        except Exception as e:
            st.error(f"   âŒ {file.name}: ì²˜ë¦¬ ì‹¤íŒ¨ - {str(e)}")
    
    # Validation íŒŒì¼ë“¤ ì²˜ë¦¬
    st.write("ğŸ”„ Validation ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    for i, file in enumerate(val_files):
        try:
            df = load_feather_file(file)
            if df is not None:
                input_arr, output_arr, info = extract_dnn_samples(
                    df, start_pos, end_pos, lookback, horizon, step_gap
                )
                
                if len(input_arr) > 0:
                    val_inputs.append(input_arr)
                    val_outputs.append(output_arr)
                    
                    # íŒŒì¼ ì •ë³´ ì¶”ê°€
                    for sample_info in info:
                        sample_info['file_name'] = file.name
                        sample_info['file_index'] = i
                        sample_info['split'] = 'validation'
                    val_info.extend(info)
                    
                st.write(f"   âœ… {file.name}: {len(input_arr)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
        except Exception as e:
            st.error(f"   âŒ {file.name}: ì²˜ë¦¬ ì‹¤íŒ¨ - {str(e)}")
    
    # ë°ì´í„° ê²°í•©
    final_train_inputs = np.concatenate(train_inputs, axis=0) if train_inputs else np.array([])
    final_train_outputs = np.concatenate(train_outputs, axis=0) if train_outputs else np.array([])
    
    final_val_inputs = np.concatenate(val_inputs, axis=0) if val_inputs else np.array([])
    final_val_outputs = np.concatenate(val_outputs, axis=0) if val_outputs else np.array([])
    
    return {
        'train_inputs': final_train_inputs,
        'train_outputs': final_train_outputs,
        'train_info': train_info,
        'val_inputs': final_val_inputs,
        'val_outputs': final_val_outputs,
        'val_info': val_info
    }

def save_dnn_dataset(dataset: Dict, metadata: Dict, filename: str) -> bytes:
    """DNN ë°ì´í„°ì…‹ì„ NPY í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    
    # ì „ì²´ ë°ì´í„° êµ¬ì„±
    full_dataset = {
        'metadata': metadata,
        'train_inputs': dataset['train_inputs'],
        'train_outputs': dataset['train_outputs'],
        'train_info': dataset['train_info'],
        'val_inputs': dataset['val_inputs'],
        'val_outputs': dataset['val_outputs'],
        'val_info': dataset['val_info']
    }
    
    # numpy save í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”
    buffer = io.BytesIO()
    np.save(buffer, full_dataset, allow_pickle=True)
    buffer.seek(0)
    
    return buffer.getvalue()




# =================================================================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# =================================================================================
def main():
    st.title("ğŸ“ˆ í•™ìŠµìš© ì‹œê³„ì—´ ë°ì´í„° ì¶”ì¶œ íˆ´")
    
    # íƒ­ ìƒì„± - ì¶”í›„ í™•ì¥ì„ ìœ„í•œ êµ¬ì¡°
    tab1, tab2, tab3 = st.tabs(["ğŸ” ì‹ í˜¸ ê´€ì°°", "ğŸ“Š ì´ë™ ì‹¤í–‰", "ğŸ“¦ ë°ì´í„° ì¶”ì¶œ"])
    
    # =================================================================================
    # íƒ­ 1: ì‹ í˜¸ ë¶„ì„ (ë©”ì¸ ê¸°ëŠ¥)
    # =================================================================================
    with tab1:
        st.header("ğŸš€ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì‹ í˜¸ ê´€ì°° ë° ë¶„ì„")
        
        # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
        st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown("**FTR/Feather íŒŒì¼ì„ ì§ì ‘ ì—…ë¡œë“œí•˜ì„¸ìš”:**")
            uploaded_files = st.file_uploader(
                "FTR/Feather íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
                type=['ftr', 'feather'],
                accept_multiple_files=True
            )
        
        with col2:
            if uploaded_files:
                if st.button("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬", key="upload_btn"):
                    handle_file_upload(uploaded_files)
        
        # íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš° ë¶„ì„ ì‹œì‘
        if 'uploaded_files' in st.session_state and st.session_state.uploaded_files:
            files = st.session_state.uploaded_files
            
            # íŒŒì¼ ì„ íƒ (ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ íŒŒì¼)
            st.subheader("ğŸ“‚ ë¶„ì„í•  íŒŒì¼ ì„ íƒ")
            file_names = [f.name for f in files]
            selected_file_index = st.selectbox(
                "ë¶„ì„í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:",
                range(len(files)),
                format_func=lambda x: file_names[x],
                index=0
            )
            
            # ì„ íƒëœ íŒŒì¼ ë¡œë“œ
            selected_file = files[selected_file_index]
            df = load_feather_file(selected_file)
            
            if df is not None:
                st.success(f"âœ… {selected_file.name} ë¡œë”© ì™„ë£Œ! Shape: {df.shape}")
                
                # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                    st.dataframe(df.head())
                    st.write(f"**ì»¬ëŸ¼ ì •ë³´:** {list(df.columns)}")
                    st.write(f"**ë°ì´í„° íƒ€ì…:** {df.dtypes.to_dict()}")
                
                # ê¸°ë³¸ ì‹ í˜¸ ê´€ì°°
                st.subheader("ğŸ“ˆ ê¸°ë³¸ ì‹ í˜¸ ê´€ì°°")
                
                # ì»¬ëŸ¼ ì„ íƒ
                selected_cols = st.multiselect(
                    "ğŸ“Š Plotí•  ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”",
                    df.columns.tolist(),
                    default=df.columns.tolist()[:3] if len(df.columns) >= 3 else df.columns.tolist()
                )
                
                if selected_cols:
                    # ê¸°ë³¸ ì„¤ì •
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        downsample_rate = st.slider(
                            "ğŸ“‰ ë‹¤ìš´ìƒ˜í”Œ ë¹„ìœ¨ (1/N)", 
                            min_value=1, max_value=100, value=10
                        )
                    with col2:
                        num_segments = st.selectbox(
                            "ğŸ“Š ë°ì´í„° ë¶„í•  ìˆ˜",
                            options=[1, 2, 3, 4, 5],
                            index=2,  # ê¸°ë³¸ê°’: 3ë“±ë¶„
                            help="ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë“±ë¶„í• ì§€ ì„ íƒ"
                        )
                    with col3:
                        selected_segment = st.selectbox(
                            "ğŸ¯ ë¶„ì„ êµ¬ê°„ ì„ íƒ",
                            options=list(range(num_segments)),
                            format_func=lambda x: f"êµ¬ê°„ {x+1}",
                            index=0,  # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ êµ¬ê°„
                            help="ë¶„ì„í•  êµ¬ê°„ì„ ì„ íƒ"
                        )
                    
                    # ë°ì´í„° êµ¬ê°„ ì •ë³´ í‘œì‹œ
                    total_length = len(df)
                    segment_length = total_length // num_segments
                    start_idx = selected_segment * segment_length
                    end_idx = start_idx + segment_length if selected_segment < num_segments - 1 else total_length
                    
                    st.info(f"ğŸ“Š **ì„ íƒëœ êµ¬ê°„**: {start_idx:,} ~ {end_idx:,} (ì´ {end_idx - start_idx:,}ê°œ í¬ì¸íŠ¸, ì „ì²´ì˜ {((end_idx - start_idx) / total_length * 100):.1f}%)")
                    
                    crosshair = st.checkbox("â–¶ï¸ ì‹­ìì„  Hover í™œì„±í™”", value=True)
                    
                    # ê¸°ë³¸ í”Œë¡¯ ìƒì„±
                    basic_delays = {col: 0 for col in selected_cols}
                    fig_basic = create_multivariate_plot(
                        df, selected_cols, basic_delays, downsample_rate, crosshair,
                        num_segments, selected_segment
                    )
                    fig_basic.update_layout(title="ğŸ“Š ê¸°ë³¸ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì‹ í˜¸")
                    st.plotly_chart(fig_basic, use_container_width=True)
                
                # ì‹œê°„ ì§€ì—° ë¶„ì„
                st.subheader("â±ï¸ ì‹œê°„ ì§€ì—° ë¶„ì„")
                st.markdown("ì„ íƒëœ ì†ì„±ì— ì‹œê°„ ì§€ì—°ì„ ì ìš©í•˜ì—¬ ì‹ í˜¸ì˜ ìƒí˜¸ê´€ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                # ì§€ì—° ë¶„ì„ìš© ì»¬ëŸ¼ ì„ íƒ
                delay_cols = st.multiselect(
                    "ğŸ¯ ì§€ì—° ë¶„ì„í•  ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”",
                    df.columns.tolist(),
                    key="delay_analysis_cols"
                )
                
                if delay_cols:                    
                    # ê° ì»¬ëŸ¼ë³„ ì§€ì—°ê°’ ì…ë ¥
                    delays = {}
                    cols_per_row = 3
                    
                    for i in range(0, len(delay_cols), cols_per_row):
                        cols = st.columns(cols_per_row)
                        for j, col_name in enumerate(delay_cols[i:i+cols_per_row]):
                            with cols[j]:
                                delays[col_name] = st.number_input(
                                    f"ğŸ”„ {col_name}",
                                    min_value=-1000,
                                    max_value=1000,
                                    value=0,
                                    step=1,
                                    key=f"delay_{col_name}"
                                )
                    
                    # ì ìš© ë²„íŠ¼ê³¼ í”Œë¡¯
                    col1, col2, col3 = st.columns([1, 2, 1])
                    with col2:
                        if st.button("ğŸš€ ì‹œê°„ ì§€ì—° ì ìš© ë° í”Œë¡¯ ìƒì„±", key="apply_delays_btn"):
                            st.session_state.delays_applied = True
                            st.session_state.current_delays = delays.copy()
                            st.session_state.current_delay_cols = delay_cols.copy()
                    
                    # ì§€ì—° ì ìš©ëœ í”Œë¡¯ í‘œì‹œ
                    if (hasattr(st.session_state, 'delays_applied') and 
                        st.session_state.delays_applied and 
                        hasattr(st.session_state, 'current_delays')):
                        
                        st.markdown("---")
                        st.subheader("ğŸ“Š ì‹œê°„ ì§€ì—° ì ìš© ê²°ê³¼")
                        
                        # ì ìš©ëœ ì§€ì—°ê°’ ì •ë³´ í‘œì‹œ
                        delay_info = []
                        for col, delay in st.session_state.current_delays.items():
                            if delay != 0:
                                delay_info.append(f"**{col}**: {delay:+d}")
                        
                        if delay_info:
                            st.info(f"ì ìš©ëœ ì§€ì—°ê°’: {', '.join(delay_info)}")
                        
                        # í•¨ê»˜ í‘œì‹œí•  ê¸°ì¤€ ì»¬ëŸ¼ ì„ íƒ (ê²°ê³¼ í™•ì¸ í›„ ì„ íƒ ê°€ëŠ¥)
                        available_reference_cols = [col for col in df.columns.tolist() 
                                                  if col not in st.session_state.current_delay_cols]
                        
                        reference_cols = st.multiselect(
                            "ğŸ“Š í•¨ê»˜ ë¹„êµí•  ê¸°ì¤€ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš” (ì§€ì—° ì ìš© ì•ˆë¨)",
                            available_reference_cols,
                            key="reference_cols_result"
                        )
                        
                        if reference_cols:
                            st.info(f"ê¸°ì¤€ ì‹ í˜¸ (ì ì„ ): {', '.join(reference_cols)}")
                        
                        # ì§€ì—° ì ìš©ëœ í”Œë¡¯ ìƒì„± (ê¸°ì¤€ ì»¬ëŸ¼ê³¼ í•¨ê»˜)
                        fig_delayed = create_combined_plot(
                            df, 
                            st.session_state.current_delay_cols,
                            st.session_state.current_delays,
                            reference_cols,
                            downsample_rate,
                            crosshair,
                            num_segments,
                            selected_segment
                        )
                        st.plotly_chart(fig_delayed, use_container_width=True)
                        
                        # ì§€ì—° ì ìš©ëœ ë°ì´í„° ì €ì¥/ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ì¶”ê°€
                        st.markdown("---")
                        st.subheader("ğŸ’¾ ì§€ì—° ì ìš© ë°ì´í„° ì €ì¥")
                        st.caption("ì›ë³¸ì—ì„œ shift ì„ íƒëœ íŠ¹ì§•ì„ ì œì™¸í•˜ê³ , shift ì²˜ë¦¬ëœ íŠ¹ì§•ì„ í¬í•¨í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.")
                        
                        # íŒŒì¼ëª… ì…ë ¥
                        default_filename = f"{selected_file.name.split('.')[0]}_shifted"
                        save_filename = st.text_input(
                            "ì €ì¥í•  íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)",
                            value=default_filename,
                            help="feather í˜•ì‹ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤"
                        )
                        
                        # ë°ì´í„° ìƒì„± ë° ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        if st.button("ğŸ”„ ì§€ì—° ì ìš© ë°ì´í„° ìƒì„± ë° ë‹¤ìš´ë¡œë“œ", key="generate_shifted_data"):
                            try:
                                # ì›ë³¸ ë°ì´í„° ë³µì‚¬
                                shifted_df = df.copy()
                                
                                # shift ì„ íƒëœ íŠ¹ì§•ë“¤ì„ ì§€ì—° ì²˜ë¦¬ëœ ë²„ì „ìœ¼ë¡œ êµì²´
                                for col in st.session_state.current_delay_cols:
                                    delay = st.session_state.current_delays[col]
                                    shifted_series = apply_time_delay(df, col, delay)
                                    
                                    # ì›ë³¸ ì»¬ëŸ¼ì„ ì§€ì—° ì ìš©ëœ ë°ì´í„°ë¡œ êµì²´
                                    shifted_df[col] = shifted_series
                                
                                # ê²°ì¸¡ê°’ ì •ë³´ í‘œì‹œ
                                total_na = shifted_df.isna().sum().sum()
                                if total_na > 0:
                                    st.warning(f"âš ï¸ ì‹œê°„ ì§€ì—°ìœ¼ë¡œ ì¸í•´ {total_na:,}ê°œì˜ ê²°ì¸¡ê°’ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                
                                # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                                with st.expander("ğŸ“‹ ìƒì„±ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                                    st.write(f"**Shape**: {shifted_df.shape}")
                                    st.write(f"**ì»¬ëŸ¼**: {list(shifted_df.columns)}")
                                    st.dataframe(shifted_df.head(10))
                                    
                                    # ì§€ì—° ì ìš© ì •ë³´ ìš”ì•½
                                    st.write("**ì§€ì—° ì ìš©ëœ íŠ¹ì§•:**")
                                    for col, delay in st.session_state.current_delays.items():
                                        st.write(f"- {col}: {delay:+d}í‹± ì§€ì—° ì ìš©")
                                    
                                    # ë³€ê²½ë˜ì§€ ì•Šì€ íŠ¹ì§•ë“¤
                                    unchanged_cols = [col for col in df.columns if col not in st.session_state.current_delay_cols]
                                    if unchanged_cols:
                                        st.write("**ì›ë³¸ ìœ ì§€ëœ íŠ¹ì§•:**")
                                        st.write(f"- {', '.join(unchanged_cols)}")
                                
                                # feather í˜•ì‹ìœ¼ë¡œ ì €ì¥
                                output_buffer = io.BytesIO()
                                shifted_df.reset_index(drop=True).to_feather(output_buffer)
                                output_buffer.seek(0)
                                
                                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                                st.download_button(
                                    label="ğŸ’¾ Feather íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                                    data=output_buffer.getvalue(),
                                    file_name=f"{save_filename}.feather",
                                    mime="application/octet-stream",
                                    help="ì§€ì—°ì´ ì ìš©ëœ ë°ì´í„°ë¥¼ feather í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"
                                )
                                
                                st.success(f"âœ… ì§€ì—° ì ìš©ëœ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                
                            except Exception as e:
                                st.error(f"âŒ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                        
                        # ì¶”ê°€ ì €ì¥ ì˜µì…˜ (CSV)
                        with st.expander("ğŸ“„ ì¶”ê°€ ì €ì¥ ì˜µì…˜"):
                            st.markdown("**CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ ê°€ëŠ¥:**")
                            if st.button("ğŸ“Š CSV í˜•ì‹ìœ¼ë¡œ ìƒì„±", key="generate_csv"):
                                try:
                                    # ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ë°ì´í„° ìƒì„±
                                    shifted_df = df.copy()
                                    for col in st.session_state.current_delay_cols:
                                        delay = st.session_state.current_delays[col]
                                        shifted_series = apply_time_delay(df, col, delay)
                                        shifted_df[col] = shifted_series
                                    
                                    # CSVë¡œ ë³€í™˜
                                    csv_buffer = io.StringIO()
                                    shifted_df.to_csv(csv_buffer, index=True)
                                    csv_data = csv_buffer.getvalue()
                                    
                                    # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                                    st.download_button(
                                        label="ğŸ“„ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                                        data=csv_data,
                                        file_name=f"{save_filename}.csv",
                                        mime="text/csv",
                                        help="ì§€ì—°ì´ ì ìš©ëœ ë°ì´í„°ë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"
                                    )
                                    
                                except Exception as e:
                                    st.error(f"âŒ CSV ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # =================================================================================
    # íƒ­ 2: ë°°ì¹˜ ì§€ì—° ì²˜ë¦¬ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
    # =================================================================================
    with tab2:
        st.header("ğŸ”„ ë°°ì¹˜ ì§€ì—° ì²˜ë¦¬")
        st.markdown("ì—¬ëŸ¬ ê°œì˜ FTR íŒŒì¼ì— ë™ì¼í•œ ì§€ì—° ì„¤ì •ì„ ì¼ê´„ ì ìš©í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        # ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
        st.subheader("ğŸ“ ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ")
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown("**ì—¬ëŸ¬ ê°œì˜ FTR/Feather íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:**")
            batch_uploaded_files = st.file_uploader(
                "ë°°ì¹˜ ì²˜ë¦¬í•  FTR/Feather íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
                type=['ftr', 'feather'],
                accept_multiple_files=True,
                key="batch_file_uploader"
            )
        
        with col2:
            if batch_uploaded_files:
                if st.button("ğŸ“¤ ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬", key="batch_upload_btn"):
                    handle_batch_file_upload(batch_uploaded_files)
        
        # ë°°ì¹˜ íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš° ì²˜ë¦¬ ì‹œì‘
        if 'batch_uploaded_files' in st.session_state and st.session_state.batch_uploaded_files:
            batch_files = st.session_state.batch_uploaded_files
            
            st.success(f"âœ… {len(batch_files)}ê°œ íŒŒì¼ì´ ë°°ì¹˜ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ì²« ë²ˆì§¸ íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ íŠ¹ì§• ëª©ë¡ í™•ì¸
            first_file = batch_files[0]
            reference_df = load_feather_file(first_file)
            
            if reference_df is not None:
                st.subheader("ğŸ“Š ê¸°ì¤€ íŒŒì¼ ì •ë³´")
                st.info(f"**ê¸°ì¤€ íŒŒì¼**: {first_file.name} (Shape: {reference_df.shape})")
                
                # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ğŸ“‹ ê¸°ì¤€ íŒŒì¼ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                    st.dataframe(reference_df.head())
                    st.write(f"**ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì§•**: {list(reference_df.columns)}")
                
                # íŠ¹ì§• ì„ íƒ
                st.subheader("ğŸ¯ ì§€ì—° ì ìš©í•  íŠ¹ì§• ì„ íƒ")
                selected_features = st.multiselect(
                    "ë°°ì¹˜ ì²˜ë¦¬ì— ì ìš©í•  íŠ¹ì§•ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
                    reference_df.columns.tolist(),
                    default=[reference_df.columns[0]] if len(reference_df.columns) > 0 else [],
                    key="batch_feature_selection"
                )
                
                if selected_features:
                    # ì§€ì—°ê°’ ì„¤ì •
                    st.subheader("â±ï¸ ì§€ì—°ê°’ ì„¤ì •")
                    st.caption("ëª¨ë“  íŒŒì¼ì— ë™ì¼í•œ ì§€ì—°ê°’ì´ ì ìš©ë©ë‹ˆë‹¤. ì–‘ìˆ˜: ë¯¸ë˜â†’í˜„ì¬, ìŒìˆ˜: ê³¼ê±°â†’í˜„ì¬")
                    
                    batch_delays = {}
                    cols_per_row = 3
                    
                    for i in range(0, len(selected_features), cols_per_row):
                        cols = st.columns(cols_per_row)
                        for j, feature_name in enumerate(selected_features[i:i+cols_per_row]):
                            with cols[j]:
                                batch_delays[feature_name] = st.number_input(
                                    f"ğŸ”„ {feature_name}",
                                    min_value=-1000,
                                    max_value=1000,
                                    value=0,
                                    step=1,
                                    key=f"batch_delay_{feature_name}"
                                )
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì • ìš”ì•½
                    st.subheader("ğŸ“‹ ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì • ìš”ì•½")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼:**")
                        for i, file in enumerate(batch_files):
                            st.write(f"{i+1}. {file.name}")
                    
                    with col2:
                        st.markdown("**ì ìš©í•  ì§€ì—° ì„¤ì •:**")
                        for feature, delay in batch_delays.items():
                            if delay != 0:
                                st.write(f"â€¢ {feature}: {delay:+d}í‹±")
                            else:
                                st.write(f"â€¢ {feature}: ì§€ì—° ì—†ìŒ")
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                    st.markdown("---")
                    col1, col2, col3 = st.columns([1, 2, 1])
                    
                    with col2:
                        if st.button("ğŸš€ ë°°ì¹˜ ì§€ì—° ì²˜ë¦¬ ì‹œì‘", key="start_batch_processing"):
                            st.session_state.batch_processing_done = False
                            
                            with st.spinner("ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                                processed_files = process_batch_files(batch_files, selected_features, batch_delays)
                                
                                if processed_files:
                                    st.session_state.processed_batch_files = processed_files
                                    st.session_state.batch_processing_done = True
                                    st.session_state.batch_selected_features = selected_features
                                    st.session_state.batch_delays = batch_delays
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ
                    if (hasattr(st.session_state, 'batch_processing_done') and 
                        st.session_state.batch_processing_done and 
                        hasattr(st.session_state, 'processed_batch_files')):
                        
                        st.markdown("---")
                        st.subheader("âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
                        
                        processed_files = st.session_state.processed_batch_files
                        
                        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
                        st.success(f"ğŸ‰ {len(processed_files)}ê°œ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                        # ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
                        with st.expander("ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼ ìƒì„¸ ì •ë³´"):
                            for i, file_info in enumerate(processed_files):
                                st.markdown(f"**{i+1}. {file_info['original_name']}**")
                                st.write(f"   â€¢ ìƒˆ íŒŒì¼ëª…: {file_info['processed_name']}")
                                st.write(f"   â€¢ ë°ì´í„° í¬ê¸°: {file_info['shape']}")
                                if file_info['applied_delays']:
                                    st.write(f"   â€¢ ì ìš©ëœ ì§€ì—°: {file_info['applied_delays']}")
                                else:
                                    st.write(f"   â€¢ ì ìš©ëœ ì§€ì—°: ì—†ìŒ")
                                st.write("")
                        
                        # í†µê³„ ì •ë³´
                        total_features_processed = sum(len(f['applied_delays']) for f in processed_files)
                        st.info(f"ğŸ“ˆ **ì²˜ë¦¬ í†µê³„**: {len(processed_files)}ê°œ íŒŒì¼, {total_features_processed}ê°œ íŠ¹ì§•ì— ì§€ì—° ì ìš©")
                        
                        # ë‹¤ìš´ë¡œë“œ ì„¹ì…˜
                        st.subheader("ğŸ’¾ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                        
                        # ZIP íŒŒì¼ëª… ì„¤ì •
                        default_zip_name = f"batch_shifted_files_{len(processed_files)}files"
                        zip_filename = st.text_input(
                            "ZIP íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)",
                            value=default_zip_name,
                            key="zip_filename_input"
                        )
                        
                        # ZIP ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        if st.button("ğŸ“¦ ZIP íŒŒì¼ë¡œ ì¼ê´„ ë‹¤ìš´ë¡œë“œ", key="download_batch_zip"):
                            try:
                                with st.spinner("ğŸ“¦ ZIP íŒŒì¼ ìƒì„± ì¤‘..."):
                                    zip_data = create_zip_download(processed_files, f"{zip_filename}.zip")
                                
                                st.download_button(
                                    label="ğŸ’¾ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                                    data=zip_data,
                                    file_name=f"{zip_filename}.zip",
                                    mime="application/zip",
                                    help="ëª¨ë“  ì²˜ë¦¬ëœ íŒŒì¼ì„ ZIPìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ë‹¤ìš´ë¡œë“œ"
                                )
                                
                                st.success("âœ… ZIP íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                                
                            except Exception as e:
                                st.error(f"âŒ ZIP íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        
                        # ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜µì…˜
                        with st.expander("ğŸ“„ ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"):
                            st.markdown("**ê°œë³„ íŒŒì¼ì„ ë”°ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:**")
                            
                            for i, file_info in enumerate(processed_files):
                                col1, col2 = st.columns([3, 1])
                                
                                with col1:
                                    st.write(f"**{file_info['processed_name']}** ({file_info['shape'][0]:,} Ã— {file_info['shape'][1]})")
                                
                                with col2:
                                    # ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                                    feather_buffer = io.BytesIO()
                                    file_info['dataframe'].reset_index(drop=True).to_feather(feather_buffer)
                                    feather_buffer.seek(0)
                                    
                                    st.download_button(
                                        label="ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
                                        data=feather_buffer.getvalue(),
                                        file_name=file_info['processed_name'],
                                        mime="application/octet-stream",
                                        key=f"individual_download_{i}"
                                    )
    


    # =================================================================================
    # íƒ­ 3: ë‹¤ì¤‘ íŒŒì¼ ì‹œê°í™” (ìƒˆë¡œìš´ ê¸°ëŠ¥)
    # =================================================================================
    with tab3:
        st.header("ğŸ“Š ë‹¤ì¤‘ íŒŒì¼ ì‹œê°í™”")
        st.markdown("ì—¬ëŸ¬ ê°œì˜ FTR íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë™ì¼í•œ íŠ¹ì§•ë“¤ì„ ë¹„êµ ì‹œê°í™”í•©ë‹ˆë‹¤.")
        
        # ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
        st.subheader("ğŸ“ ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ")
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown("**ì—¬ëŸ¬ ê°œì˜ FTR/Feather íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:**")
            multi_uploaded_files = st.file_uploader(
                "ì‹œê°í™”í•  FTR/Feather íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
                type=['ftr', 'feather'],
                accept_multiple_files=True,
                key="multi_file_uploader"
            )
        
        with col2:
            if multi_uploaded_files:
                if st.button("ğŸ“¤ ë‹¤ì¤‘ íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬", key="multi_upload_btn"):
                    handle_multi_file_upload(multi_uploaded_files)
        
        # ë‹¤ì¤‘ íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš° ì‹œê°í™” ì‹œì‘
        if 'multi_uploaded_files' in st.session_state and st.session_state.multi_uploaded_files:
            multi_files = st.session_state.multi_uploaded_files
            
            st.success(f"âœ… {len(multi_files)}ê°œ íŒŒì¼ì´ ë‹¤ì¤‘ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ì²« ë²ˆì§¸ íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ íŠ¹ì§• ëª©ë¡ í™•ì¸
            first_file = multi_files[0]
            reference_df = load_feather_file(first_file)
            
            if reference_df is not None:
                st.subheader("ğŸ“Š ê¸°ì¤€ íŒŒì¼ ì •ë³´")
                st.info(f"**ê¸°ì¤€ íŒŒì¼**: {first_file.name} (Shape: {reference_df.shape})")
                
                # ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
                with st.expander("ğŸ“‹ ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡"):
                    for i, file in enumerate(multi_files):
                        try:
                            temp_df = load_feather_file(file)
                            if temp_df is not None:
                                st.write(f"{i+1}. **{file.name}** - Shape: {temp_df.shape}")
                            else:
                                st.write(f"{i+1}. **{file.name}** - âŒ ë¡œë“œ ì‹¤íŒ¨")
                        except:
                            st.write(f"{i+1}. **{file.name}** - âŒ ë¡œë“œ ì‹¤íŒ¨")
                
                # íŠ¹ì§• ì„ íƒ
                st.subheader("ğŸ¯ ì‹œê°í™”í•  íŠ¹ì§• ì„ íƒ")
                multi_selected_features = st.multiselect(
                    "ë¹„êµí•  íŠ¹ì§•ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
                    reference_df.columns.tolist(),
                    default=[reference_df.columns[0]] if len(reference_df.columns) > 0 else [],
                    key="multi_feature_selection",
                    help="ì„ íƒëœ íŠ¹ì§•ë“¤ì´ ì„ íƒëœ íŒŒì¼ë“¤ì—ì„œ ë¹„êµ ì‹œê°í™”ë©ë‹ˆë‹¤."
                )
                
                # í”Œë¡¯í•  íŒŒì¼ ì„ íƒ ì¶”ê°€
                st.subheader("ğŸ“‚ í”Œë¡¯í•  íŒŒì¼ ì„ íƒ")
                file_names = [f.name for f in multi_files]
                selected_file_indices = st.multiselect(
                    "í”Œë¡¯ì— í¬í•¨í•  íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
                    range(len(multi_files)),
                    format_func=lambda x: file_names[x],
                    default=[0] if len(multi_files) > 0 else [], 
                    key="multi_file_selection",
                    help="ì„ íƒëœ íŒŒì¼ë“¤ë§Œ í”Œë¡¯ì— í‘œì‹œë©ë‹ˆë‹¤."
                )
                
                # ì„ íƒëœ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
                selected_files = [multi_files[i] for i in selected_file_indices]
                
                if multi_selected_features and selected_files:
                    # ì‹œê°í™” ì„¤ì • (íƒ­1ê³¼ ë™ì¼í•œ êµ¬ì¡°)
                    st.subheader("âš™ï¸ ì‹œê°í™” ì„¤ì •")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        multi_downsample_rate = st.slider(
                            "ğŸ“‰ ë‹¤ìš´ìƒ˜í”Œ ë¹„ìœ¨ (1/N)", 
                            min_value=1, max_value=100, value=10,
                            key="multi_downsample"
                        )
                    with col2:
                        multi_num_segments = st.selectbox(
                            "ğŸ“Š ë°ì´í„° ë¶„í•  ìˆ˜",
                            options=[1, 2, 3, 4, 5],
                            index=2,  # ê¸°ë³¸ê°’: 3ë“±ë¶„
                            help="ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë“±ë¶„í• ì§€ ì„ íƒ",
                            key="multi_segments"
                        )
                    with col3:
                        multi_selected_segment = st.selectbox(
                            "ğŸ¯ ë¶„ì„ êµ¬ê°„ ì„ íƒ",
                            options=list(range(multi_num_segments)),
                            format_func=lambda x: f"êµ¬ê°„ {x+1}",
                            index=0,  # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ êµ¬ê°„
                            help="ë¶„ì„í•  êµ¬ê°„ì„ ì„ íƒ",
                            key="multi_segment_select"
                        )
                    
                    # ë°ì´í„° êµ¬ê°„ ì •ë³´ í‘œì‹œ (ê¸°ì¤€ íŒŒì¼ ê¸°ì¤€)
                    total_length = len(reference_df)
                    segment_length = total_length // multi_num_segments
                    start_idx = multi_selected_segment * segment_length
                    end_idx = start_idx + segment_length if multi_selected_segment < multi_num_segments - 1 else total_length
                    
                    st.info(f"ğŸ“Š **ì„ íƒëœ êµ¬ê°„**: {start_idx:,} ~ {end_idx:,} (ì´ {end_idx - start_idx:,}ê°œ í¬ì¸íŠ¸, ì „ì²´ì˜ {((end_idx - start_idx) / total_length * 100):.1f}%)")
                    
                    multi_crosshair = st.checkbox("â–¶ï¸ ì‹­ìì„  Hover í™œì„±í™”", value=True, key="multi_crosshair")
                    
                    # ë‹¤ì¤‘ íŒŒì¼ ì‹œê°í™” ìƒì„±
                    st.subheader("ğŸ“ˆ ë‹¤ì¤‘ íŒŒì¼ íŠ¹ì§• ë¹„êµ")
                    
                    try:
                        # ë‹¤ì¤‘ íŒŒì¼ í”Œë¡¯ ìƒì„± (ì„ íƒëœ íŒŒì¼ë“¤ë§Œ)
                        multi_fig = create_multi_file_plot(
                            selected_files,
                            multi_selected_features,
                            multi_downsample_rate,
                            multi_crosshair,
                            multi_num_segments,
                            multi_selected_segment
                        )
                        
                        st.plotly_chart(multi_fig, use_container_width=True)
                        
                        # ì¶”ê°€ ì •ë³´ í‘œì‹œ
                        st.subheader("ğŸ“‹ ì‹œê°í™” ìš”ì•½")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown("**ì‹œê°í™”ëœ íŒŒì¼:**")
                            for i, file in enumerate(selected_files):
                                st.write(f"{i+1}. {file.name}")
                        
                        with col2:
                            st.markdown("**ì‹œê°í™”ëœ íŠ¹ì§•:**")
                            for feature in multi_selected_features:
                                st.write(f"â€¢ {feature}")
                        
                        # ë°ì´í„° íŠ¹ì„± ë¶„ì„ (ì„ íƒëœ íŒŒì¼ë“¤ë§Œ)
                        with st.expander("ğŸ“Š íŒŒì¼ë³„ ë°ì´í„° íŠ¹ì„± ë¹„êµ"):
                            comparison_data = []
                            
                            for file in selected_files:
                                try:
                                    df = load_feather_file(file)
                                    if df is not None:
                                        # ì„ íƒëœ êµ¬ê°„ì—ì„œ í†µê³„ ê³„ì‚°
                                        df_segment = get_data_segment(df, multi_num_segments, multi_selected_segment)
                                        
                                        for feature in multi_selected_features:
                                            if feature in df.columns:
                                                feature_data = df_segment[feature]
                                                comparison_data.append({
                                                    'íŒŒì¼ëª…': file.name,
                                                    'íŠ¹ì§•': feature,
                                                    'í‰ê· ': f"{feature_data.mean():.4f}",
                                                    'í‘œì¤€í¸ì°¨': f"{feature_data.std():.4f}",
                                                    'ìµœì†Œê°’': f"{feature_data.min():.4f}",
                                                    'ìµœëŒ€ê°’': f"{feature_data.max():.4f}",
                                                    'ë°ì´í„° í¬ì¸íŠ¸': f"{len(feature_data):,}",
                                                    'ê²°ì¸¡ê°’': feature_data.isna().sum()
                                                })
                                except Exception as e:
                                    st.warning(f"âš ï¸ {file.name} í†µê³„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            
                            if comparison_data:
                                comparison_df = pd.DataFrame(comparison_data)
                                st.dataframe(comparison_df, use_container_width=True)
                            else:
                                st.warning("âš ï¸ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        
                    except Exception as e:
                        st.error(f"âŒ ë‹¤ì¤‘ íŒŒì¼ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        
                elif not multi_selected_features:
                    st.info("ğŸ¯ ì‹œê°í™”í•  íŠ¹ì§•ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                elif not selected_files:
                    st.info("ğŸ“‚ í”Œë¡¯í•  íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                st.error("âŒ ê¸°ì¤€ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ“ ë‹¤ì¤‘ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œê°í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        


        # =================================================================================
        # DNN í•™ìŠµ ë°ì´í„° ì¶”ì¶œ ê¸°ëŠ¥ ì¶”ê°€
        # =================================================================================
        if 'multi_uploaded_files' in st.session_state and st.session_state.multi_uploaded_files:
            st.markdown("---")
            st.header("ğŸ¤– DNN í•™ìŠµ ë°ì´í„° ì¶”ì¶œ")
            st.markdown("ì—…ë¡œë“œëœ FTR íŒŒì¼ë“¤ë¡œë¶€í„° DNN í•™ìŠµìš© ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
            
            # ë°ì´í„° ì¶”ì¶œ íŒŒë¼ë¯¸í„° ì„¤ì •
            st.subheader("âš™ï¸ ë°ì´í„° ì¶”ì¶œ ì„¤ì •")
            
            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            col1, col2 = st.columns(2)
            with col1:
                start_position = st.number_input(
                    "ğŸ¯ ì‹œì‘ ìœ„ì¹˜ (í‹±)",
                    min_value=0,
                    max_value=100000,
                    value=300,
                    step=1,
                    help="ë°ì´í„° ì¶”ì¶œì„ ì‹œì‘í•  ìœ„ì¹˜ (0ë¶€í„° ì‹œì‘)",
                    key="dnn_start_pos"
                )
                
                lookback_length = st.number_input(
                    "ğŸ“ˆ ê³¼ê±° ì°¸ì¡° ê¸¸ì´ (í‹±)",
                    min_value=1,
                    max_value=1000,
                    value=60,
                    step=1,
                    help="ê° ì‹œì ì—ì„œ ê³¼ê±° ëª‡ í‹±ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í• ì§€",
                    key="dnn_lookback"
                )
            
            with col2:
                end_position = st.number_input(
                    "ğŸ ì¢…ë£Œ ìœ„ì¹˜ (í‹±)",
                    min_value=start_position + 100,
                    max_value=100000,
                    value=start_position + 1700,
                    step=1,
                    help="ë°ì´í„° ì¶”ì¶œì„ ì¢…ë£Œí•  ìœ„ì¹˜",
                    key="dnn_end_pos"
                )
                
                horizon_length = st.number_input(
                    "ğŸ”® ì˜ˆì¸¡ êµ¬ê°„ ê¸¸ì´ (í‹±)",
                    min_value=1,
                    max_value=100,
                    value=24,
                    step=1,
                    help="ë¯¸ë˜ ëª‡ í‹±ì„ ì˜ˆì¸¡ ëŒ€ìƒìœ¼ë¡œ í• ì§€",
                    key="dnn_horizon"
                )
            
            # ì¶”ê°€ íŒŒë¼ë¯¸í„°
            col3, col4 = st.columns(2)
            with col3:
                step_gap = st.number_input(
                    "â­ï¸ ìŠ¤í… ê°„ê²©",
                    min_value=1,
                    max_value=50,
                    value=2,
                    step=1,
                    help="ìƒ˜í”Œ ì¶”ì¶œ ì‹œ ëª‡ í‹±ì”© ê±´ë„ˆë›¸ì§€",
                    key="dnn_step_gap"
                )
            
            with col4:
                train_ratio = st.slider(
                    "ğŸ“ í›ˆë ¨/ê²€ì¦ ë¹„ìœ¨",
                    min_value=0.5,
                    max_value=0.95,
                    value=0.8,
                    step=0.05,
                    help="í›ˆë ¨ìš© íŒŒì¼ì˜ ë¹„ìœ¨ (ë‚˜ë¨¸ì§€ëŠ” ê²€ì¦ìš©)",
                    key="dnn_train_ratio"
                )
            
            # ì‹œê°„ ì •ë³´ ì„¤ì •
            st.subheader("ğŸ• ì‹œê°„ íŠ¹ì§• ì„¤ì •")
            col5, col6 = st.columns(2)
            with col5:
                use_positional_encoding = st.checkbox(
                    "Positional Encoding ì‚¬ìš©",
                    value=True,
                    help="ì‹œê°„ ì •ë³´ì— positional encoding ì¶”ê°€",
                    key="dnn_pos_encoding"
                )
            
            with col6:
                tick_interval = st.number_input(
                    "í‹± ê°„ê²© (ì´ˆ)",
                    min_value=1,
                    max_value=60,
                    value=5,
                    step=1,
                    help="ê° í‹± ê°„ì˜ ì‹œê°„ ê°„ê²©",
                    key="dnn_tick_interval"
                )
            
            # íŒŒë¼ë¯¸í„° ìš”ì•½ í‘œì‹œ
            st.subheader("ğŸ“‹ ì¶”ì¶œ ì„¤ì • ìš”ì•½")
            with st.expander("ğŸ” ìƒì„¸ ì„¤ì • í™•ì¸"):
                summary_data = {
                    'íŒŒë¼ë¯¸í„°': [
                        'ì‹œì‘ ìœ„ì¹˜', 'ì¢…ë£Œ ìœ„ì¹˜', 'ê³¼ê±° ì°¸ì¡° ê¸¸ì´', 'ì˜ˆì¸¡ êµ¬ê°„ ê¸¸ì´',
                        'ìŠ¤í… ê°„ê²©', 'í›ˆë ¨ ë¹„ìœ¨', 'ê²€ì¦ ë¹„ìœ¨', 'Positional Encoding',
                        'í‹± ê°„ê²©', 'ì´ ì—…ë¡œë“œ íŒŒì¼ ìˆ˜'
                    ],
                    'ê°’': [
                        f"{start_position:,}",
                        f"{end_position:,}",
                        f"{lookback_length}",
                        f"{horizon_length}",
                        f"{step_gap}",
                        f"{train_ratio:.1%}",
                        f"{1-train_ratio:.1%}",
                        "ì‚¬ìš©" if use_positional_encoding else "ë¯¸ì‚¬ìš©",
                        f"{tick_interval}ì´ˆ",
                        f"{len(st.session_state.multi_uploaded_files)}ê°œ"
                    ]
                }
                summary_df = pd.DataFrame(summary_data)
                st.dataframe(summary_df, use_container_width=True)
                
                # ì˜ˆìƒ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
                total_samples_per_file = (end_position - start_position - lookback_length - horizon_length) // step_gap
                if total_samples_per_file > 0:
                    estimated_train_samples = total_samples_per_file * int(len(st.session_state.multi_uploaded_files) * train_ratio)
                    estimated_val_samples = total_samples_per_file * (len(st.session_state.multi_uploaded_files) - int(len(st.session_state.multi_uploaded_files) * train_ratio))
                    
                    st.info(f"ğŸ“Š **ì˜ˆìƒ ìƒ˜í”Œ ìˆ˜**: í›ˆë ¨ìš© ~{estimated_train_samples:,}ê°œ, ê²€ì¦ìš© ~{estimated_val_samples:,}ê°œ")
                else:
                    st.warning("âš ï¸ í˜„ì¬ ì„¤ì •ìœ¼ë¡œëŠ” ìƒ˜í”Œì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”.")
            
            # DNN ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰
            st.subheader("ğŸš€ ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰")
            
            # íŒŒì¼ëª… ì„¤ì •
            default_dataset_name = f"dnn_dataset_{lookback_length}to{horizon_length}_{len(st.session_state.multi_uploaded_files)}files"
            dataset_filename = st.text_input(
                "ë°ì´í„°ì…‹ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)",
                value=default_dataset_name,
                help="ìƒì„±ë  ë°ì´í„°ì…‹ íŒŒì¼ì˜ ì´ë¦„",
                key="dnn_dataset_filename"
            )
            
            # ì¶”ì¶œ ë²„íŠ¼
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button("ğŸ¤– DNN ë°ì´í„° ì¶”ì¶œ ì‹œì‘", key="start_dnn_extraction"):
                    if total_samples_per_file <= 0:
                        st.error("âŒ í˜„ì¬ ì„¤ì •ìœ¼ë¡œëŠ” ìƒ˜í”Œì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.session_state.dnn_extraction_done = False
                        
                        with st.spinner("ğŸ”„ DNN í•™ìŠµ ë°ì´í„° ì¶”ì¶œ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                            try:
                                # íŒŒì¼ë“¤ì„ í›ˆë ¨/ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• 
                                train_files, val_files = split_files_train_val(
                                    st.session_state.multi_uploaded_files, train_ratio
                                )
                                
                                st.write(f"ğŸ“‚ **íŒŒì¼ ë¶„í•  ì™„ë£Œ**: í›ˆë ¨ìš© {len(train_files)}ê°œ, ê²€ì¦ìš© {len(val_files)}ê°œ")
                                
                                # ëª¨ë“  íŒŒì¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                                dataset = process_all_files_for_dnn(
                                    train_files, val_files,
                                    start_position, end_position,
                                    lookback_length, horizon_length, step_gap
                                )
                                
                                if len(dataset['train_inputs']) > 0 or len(dataset['val_inputs']) > 0:
                                    # ë©”íƒ€ë°ì´í„° ìƒì„±
                                    metadata = {
                                        'extraction_params': {
                                            'start_position': start_position,
                                            'end_position': end_position,
                                            'lookback_length': lookback_length,
                                            'horizon_length': horizon_length,
                                            'step_gap': step_gap,
                                            'train_ratio': train_ratio,
                                            'use_positional_encoding': use_positional_encoding,
                                            'tick_interval': tick_interval
                                        },
                                        'data_info': {
                                            'train_samples': len(dataset['train_inputs']),
                                            'val_samples': len(dataset['val_inputs']),
                                            'input_shape': dataset['train_inputs'].shape if len(dataset['train_inputs']) > 0 else None,
                                            'output_shape': dataset['train_outputs'].shape if len(dataset['train_outputs']) > 0 else None,
                                            'train_files': [f.name for f in train_files],
                                            'val_files': [f.name for f in val_files],
                                            'total_files': len(st.session_state.multi_uploaded_files)
                                        },
                                        'creation_time': datetime.now().isoformat(),
                                        'feature_info': {
                                            'time_features': 3 + (8 if use_positional_encoding else 0),
                                            'time_feature_names': ['hour_norm', 'minute_norm', 'second_norm'] + 
                                                                ([f'pos_enc_{i}' for i in range(8)] if use_positional_encoding else []),
                                            'data_features': len(reference_df.columns) - 1,  # timestamp ì œì™¸
                                            'data_feature_names': [col for col in reference_df.columns if col != 
                                                                (reference_df.columns[0] if 'time' not in reference_df.columns[0].lower() 
                                                                and 'timestamp' not in reference_df.columns[0].lower() 
                                                                else next((col for col in reference_df.columns 
                                                                            if 'time' in col.lower() or 'timestamp' in col.lower()), 
                                                                        reference_df.columns[0]))],
                                            'total_features': len(dataset['train_inputs'].shape) > 2 and dataset['train_inputs'].shape[2] or 0,
                                            'feature_order': 'time_features_first_then_data_features'
                                        }
                                    }

                                    # ì„¸ì…˜ì— ì €ì¥ (ìˆ˜ì •ëœ ë¶€ë¶„)
                                    st.session_state.dnn_dataset = dataset
                                    st.session_state.dnn_metadata = metadata
                                    st.session_state.dnn_extraction_done = True
                                    st.session_state.dnn_dataset_name = dataset_filename  # filenameì„ nameìœ¼ë¡œ ë³€ê²½
                                    
                                else:
                                    st.error("âŒ ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                                    
                            except Exception as e:
                                st.error(f"âŒ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            # DNN ë°ì´í„° ì¶”ì¶œ ê²°ê³¼ í‘œì‹œ
            if (hasattr(st.session_state, 'dnn_extraction_done') and 
                st.session_state.dnn_extraction_done and 
                hasattr(st.session_state, 'dnn_dataset')):
                
                st.markdown("---")
                st.subheader("âœ… DNN ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
                
                dataset = st.session_state.dnn_dataset
                metadata = st.session_state.dnn_metadata
                
                # ì¶”ì¶œ ê²°ê³¼ ìš”ì•½
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ“ í›ˆë ¨ ìƒ˜í”Œ", f"{len(dataset['train_inputs']):,}")
                with col2:
                    st.metric("ğŸ”¬ ê²€ì¦ ìƒ˜í”Œ", f"{len(dataset['val_inputs']):,}")
                with col3:
                    st.metric("ğŸ“Š ì´ ìƒ˜í”Œ", f"{len(dataset['train_inputs']) + len(dataset['val_inputs']):,}")
                
                # ë°ì´í„° í˜•íƒœ ì •ë³´
                with st.expander("ğŸ“Š ë°ì´í„° í˜•íƒœ ì •ë³´"):
                    if len(dataset['train_inputs']) > 0:
                        st.write(f"**í›ˆë ¨ ì…ë ¥ í˜•íƒœ**: {dataset['train_inputs'].shape}")
                        st.write(f"**í›ˆë ¨ ì¶œë ¥ í˜•íƒœ**: {dataset['train_outputs'].shape}")
                        
                    if len(dataset['val_inputs']) > 0:
                        st.write(f"**ê²€ì¦ ì…ë ¥ í˜•íƒœ**: {dataset['val_inputs'].shape}")
                        st.write(f"**ê²€ì¦ ì¶œë ¥ í˜•íƒœ**: {dataset['val_outputs'].shape}")
                    
                    st.write(f"**ì‹œê°„ íŠ¹ì§• ìˆ˜**: {metadata['feature_info']['time_features']}")
                    st.write(f"**ì „ì²´ íŠ¹ì§• ìˆ˜**: {dataset['train_inputs'].shape[-1] if len(dataset['train_inputs']) > 0 else 'N/A'}")
                
                # íŒŒì¼ë³„ ìƒ˜í”Œ ìˆ˜ ì •ë³´
                with st.expander("ğŸ“ íŒŒì¼ë³„ ìƒ˜í”Œ ì •ë³´"):
                    # í›ˆë ¨ íŒŒì¼ ì •ë³´
                    st.markdown("**í›ˆë ¨ìš© íŒŒì¼:**")
                    train_file_counts = {}
                    for info in dataset['train_info']:
                        file_name = info['file_name']
                        train_file_counts[file_name] = train_file_counts.get(file_name, 0) + 1
                    
                    for file_name, count in train_file_counts.items():
                        st.write(f"  â€¢ {file_name}: {count:,}ê°œ ìƒ˜í”Œ")
                    
                    # ê²€ì¦ íŒŒì¼ ì •ë³´
                    st.markdown("**ê²€ì¦ìš© íŒŒì¼:**")
                    val_file_counts = {}
                    for info in dataset['val_info']:
                        file_name = info['file_name']
                        val_file_counts[file_name] = val_file_counts.get(file_name, 0) + 1
                    
                    for file_name, count in val_file_counts.items():
                        st.write(f"  â€¢ {file_name}: {count:,}ê°œ ìƒ˜í”Œ")
                

                # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ìˆ˜ì •ëœ ë¶€ë¶„)
                st.subheader("ğŸ’¾ DNN ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")

                if st.button("ğŸ“¦ ë°ì´í„°ì…‹ íŒŒì¼ ìƒì„±", key="generate_dnn_dataset"):
                    try:
                        with st.spinner("ğŸ“¦ ë°ì´í„°ì…‹ íŒŒì¼ ìƒì„± ì¤‘..."):
                            # ìœ„ì ¯ì—ì„œ í˜„ì¬ ê°’ ê°€ì ¸ì˜¤ê¸° (ìˆ˜ì •ëœ ë¶€ë¶„)
                            current_filename = st.session_state.get('dnn_dataset_filename', 'dnn_dataset')
                            dataset_data = save_dnn_dataset(
                                dataset, metadata, current_filename
                            )
                        
                        st.download_button(
                            label="ğŸ’¾ DNN ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ",
                            data=dataset_data,
                            file_name=f"{current_filename}.npy",  # ìˆ˜ì •ëœ ë¶€ë¶„
                            mime="application/octet-stream",
                            help="DNN í•™ìŠµìš© ë°ì´í„°ì…‹ì„ numpy í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"
                        )
                        
                        st.success("âœ… ë°ì´í„°ì…‹ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                        
                        # ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ í‘œì‹œ (NPY í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •)
                        with st.expander("ğŸ Python ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ"):
                            st.code(f"""
                import numpy as np

                # ë°ì´í„°ì…‹ ë¡œë“œ
                dataset = np.load('{current_filename}.npy', allow_pickle=True).item()  # ìˆ˜ì •ëœ ë¶€ë¶„

                # ë°ì´í„° ì ‘ê·¼
                train_inputs = dataset['train_inputs']    # Shape: (samples, lookback, features)
                train_outputs = dataset['train_outputs']  # Shape: (samples, horizon, features)
                val_inputs = dataset['val_inputs']        # Shape: (samples, lookback, features)
                val_outputs = dataset['val_outputs']      # Shape: (samples, horizon, features)

                # ë©”íƒ€ë°ì´í„° í™•ì¸
                metadata = dataset['metadata']
                print("ì¶”ì¶œ íŒŒë¼ë¯¸í„°:", metadata['extraction_params'])
                print("ë°ì´í„° ì •ë³´:", metadata['data_info'])

                # ìƒ˜í”Œ ì •ë³´
                train_info = dataset['train_info']  # ê° ìƒ˜í”Œì˜ ìƒì„¸ ì •ë³´
                val_info = dataset['val_info']      # ê° ìƒ˜í”Œì˜ ìƒì„¸ ì •ë³´

                print(f"í›ˆë ¨ ìƒ˜í”Œ: {{train_inputs.shape[0]:,}}ê°œ")
                print(f"ê²€ì¦ ìƒ˜í”Œ: {{val_inputs.shape[0]:,}}ê°œ")
                print(f"ì…ë ¥ í˜•íƒœ: {{train_inputs.shape}}")
                print(f"ì¶œë ¥ í˜•íƒœ: {{train_outputs.shape}}")

                # PyTorchì—ì„œ ì‚¬ìš© ì˜ˆì‹œ
                # import torch
                # train_dataset = torch.utils.data.TensorDataset(
                #     torch.FloatTensor(train_inputs), 
                #     torch.FloatTensor(train_outputs)
                # )

                # ê°œë³„ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ì€ ê²½ìš°
                # np.save('train_inputs.npy', train_inputs)
                # np.save('train_outputs.npy', train_outputs)
                # np.save('val_inputs.npy', val_inputs)
                # np.save('val_outputs.npy', val_outputs)
                # np.save('metadata.npy', metadata)
                """, language="python")
                        
                    except Exception as e:
                        st.error(f"âŒ ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                

                # ë°ì´í„° ì‹œê°í™” ì˜µì…˜ (ìˆ˜ì •ëœ ë¶€ë¶„)
                with st.expander("ğŸ“ˆ ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                    if len(dataset['train_inputs']) > 0:
                        sample_idx = st.selectbox(
                            "ë¯¸ë¦¬ë³¼ ìƒ˜í”Œ ì„ íƒ",
                            range(min(10, len(dataset['train_inputs']))),
                            key="sample_preview_idx"
                        )
                        
                        # ì „ì²´ íŠ¹ì§• ìˆ˜ í™•ì¸
                        total_features = dataset['train_inputs'].shape[2]
                        
                        # ë©”íƒ€ë°ì´í„°ì—ì„œ íŠ¹ì§• ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                        feature_names = []
                        if 'feature_info' in metadata:
                            time_feature_names = metadata['feature_info'].get('time_feature_names', [])
                            data_feature_names = metadata['feature_info'].get('data_feature_names', [])
                            feature_names = time_feature_names + data_feature_names
                        
                        # íŠ¹ì§• ì´ë¦„ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
                        if len(feature_names) != total_features:
                            feature_names = [f"Feature {i+1}" for i in range(total_features)]
                        
                        # ì‹œê°í™”í•  íŠ¹ì§• ì„ íƒ (ìµœëŒ€ 10ê°œ)
                        max_features_to_show = min(10, total_features)
                        selected_feature_indices = st.multiselect(
                            f"ì‹œê°í™”í•  íŠ¹ì§• ì„ íƒ (ì „ì²´ {total_features}ê°œ ì¤‘ ìµœëŒ€ {max_features_to_show}ê°œ)",
                            range(total_features),
                            default=list(range(min(5, total_features))),  # ê¸°ë³¸ê°’: ì²˜ìŒ 5ê°œ íŠ¹ì§•
                            format_func=lambda x: f"{x+1}: {feature_names[x]}" if x < len(feature_names) else f"Feature {x+1}",
                            key="preview_feature_selection"
                        )
                        
                        if selected_feature_indices:
                            # ì„ íƒëœ íŠ¹ì§•ì´ ìµœëŒ€ ê°œìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì œí•œ
                            if len(selected_feature_indices) > max_features_to_show:
                                st.warning(f"âš ï¸ ìµœëŒ€ {max_features_to_show}ê°œ íŠ¹ì§•ë§Œ ì„ íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì²˜ìŒ {max_features_to_show}ê°œë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
                                selected_feature_indices = selected_feature_indices[:max_features_to_show]
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**ì…ë ¥ ì‹œí€€ìŠ¤ (Input)**")
                                input_sample = dataset['train_inputs'][sample_idx]
                                st.write(f"í˜•íƒœ: {input_sample.shape}")
                                
                                # ì…ë ¥ ë°ì´í„° ì‹œê°í™” (ì„ íƒëœ íŠ¹ì§•ë§Œ)
                                fig_input = go.Figure()
                                for i, feature_idx in enumerate(selected_feature_indices):
                                    feature_name = feature_names[feature_idx] if feature_idx < len(feature_names) else f'Feature {feature_idx+1}'
                                    fig_input.add_trace(go.Scatter(
                                        y=input_sample[:, feature_idx],
                                        mode='lines+markers',
                                        name=feature_name,
                                        line=dict(width=2)
                                    ))
                                
                                fig_input.update_layout(
                                    title=f"ì…ë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ {len(selected_feature_indices)}ê°œ íŠ¹ì§•)",
                                    xaxis_title="Time Steps",
                                    yaxis_title="Feature Values",
                                    height=300
                                )
                                st.plotly_chart(fig_input, use_container_width=True)
                            
                            with col2:
                                st.markdown("**ì¶œë ¥ ì‹œí€€ìŠ¤ (Target)**")
                                output_sample = dataset['train_outputs'][sample_idx]
                                st.write(f"í˜•íƒœ: {output_sample.shape}")
                                
                                # ì¶œë ¥ ë°ì´í„° ì‹œê°í™” (ì„ íƒëœ íŠ¹ì§•ë§Œ)
                                fig_output = go.Figure()
                                for i, feature_idx in enumerate(selected_feature_indices):
                                    feature_name = feature_names[feature_idx] if feature_idx < len(feature_names) else f'Feature {feature_idx+1}'
                                    fig_output.add_trace(go.Scatter(
                                        y=output_sample[:, feature_idx],
                                        mode='lines+markers',
                                        name=feature_name,
                                        line=dict(width=2)
                                    ))
                                
                                fig_output.update_layout(
                                    title=f"ì¶œë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ {len(selected_feature_indices)}ê°œ íŠ¹ì§•)",
                                    xaxis_title="Time Steps", 
                                    yaxis_title="Feature Values",
                                    height=300
                                )
                                st.plotly_chart(fig_output, use_container_width=True)
                            
                            # ìƒ˜í”Œ ì •ë³´ í‘œì‹œ
                            sample_info = dataset['train_info'][sample_idx]
                            st.json(sample_info)
                            
                            # ì„ íƒëœ íŠ¹ì§•ë“¤ì˜ í†µê³„ ì •ë³´ (expander ì œê±°)
                            st.markdown("**ğŸ“Š ì„ íƒëœ íŠ¹ì§•ë“¤ì˜ í†µê³„ ì •ë³´**")
                            stats_data = []
                            for feature_idx in selected_feature_indices:
                                input_feature_data = input_sample[:, feature_idx]
                                output_feature_data = output_sample[:, feature_idx]
                                feature_name = feature_names[feature_idx] if feature_idx < len(feature_names) else f'Feature {feature_idx+1}'
                                
                                stats_data.append({
                                    'íŠ¹ì§•': feature_name,
                                    'ì…ë ¥ í‰ê· ': f"{input_feature_data.mean():.4f}",
                                    'ì…ë ¥ í‘œì¤€í¸ì°¨': f"{input_feature_data.std():.4f}",
                                    'ì¶œë ¥ í‰ê· ': f"{output_feature_data.mean():.4f}",
                                    'ì¶œë ¥ í‘œì¤€í¸ì°¨': f"{output_feature_data.std():.4f}",
                                    'ì…ë ¥ ë²”ìœ„': f"{input_feature_data.min():.4f} ~ {input_feature_data.max():.4f}",
                                    'ì¶œë ¥ ë²”ìœ„': f"{output_feature_data.min():.4f} ~ {output_feature_data.max():.4f}"
                                })
                            
                            stats_df = pd.DataFrame(stats_data)
                            st.dataframe(stats_df, use_container_width=True)
                            
                            # ë©”íƒ€ë°ì´í„° ì •ë³´ í‘œì‹œ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
                            st.markdown("---")
                            st.markdown("**ğŸ“‹ ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì •ë³´**")
                            
                            # ë©”íƒ€ë°ì´í„°ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬
                            meta_col1, meta_col2 = st.columns(2)
                            
                            with meta_col1:
                                st.markdown("**ğŸ”§ ì¶”ì¶œ íŒŒë¼ë¯¸í„°**")
                                extraction_params = metadata.get('extraction_params', {})
                                param_df = pd.DataFrame([
                                    {'íŒŒë¼ë¯¸í„°': 'ì‹œì‘ ìœ„ì¹˜', 'ê°’': f"{extraction_params.get('start_position', 'N/A'):,}"},
                                    {'íŒŒë¼ë¯¸í„°': 'ì¢…ë£Œ ìœ„ì¹˜', 'ê°’': f"{extraction_params.get('end_position', 'N/A'):,}"},
                                    {'íŒŒë¼ë¯¸í„°': 'ê³¼ê±° ì°¸ì¡° ê¸¸ì´', 'ê°’': extraction_params.get('lookback_length', 'N/A')},
                                    {'íŒŒë¼ë¯¸í„°': 'ì˜ˆì¸¡ êµ¬ê°„ ê¸¸ì´', 'ê°’': extraction_params.get('horizon_length', 'N/A')},
                                    {'íŒŒë¼ë¯¸í„°': 'ìŠ¤í… ê°„ê²©', 'ê°’': extraction_params.get('step_gap', 'N/A')},
                                    {'íŒŒë¼ë¯¸í„°': 'í›ˆë ¨ ë¹„ìœ¨', 'ê°’': f"{extraction_params.get('train_ratio', 0):.1%}"},
                                    {'íŒŒë¼ë¯¸í„°': 'Positional Encoding', 'ê°’': 'ì‚¬ìš©' if extraction_params.get('use_positional_encoding', False) else 'ë¯¸ì‚¬ìš©'},
                                    {'íŒŒë¼ë¯¸í„°': 'í‹± ê°„ê²©', 'ê°’': f"{extraction_params.get('tick_interval', 'N/A')}ì´ˆ"}
                                ])
                                st.dataframe(param_df, use_container_width=True, hide_index=True)
                            
                            with meta_col2:
                                st.markdown("**ğŸ“Š ë°ì´í„° ì •ë³´**")
                                data_info = metadata.get('data_info', {})
                                feature_info = metadata.get('feature_info', {})
                                info_df = pd.DataFrame([
                                    {'í•­ëª©': 'í›ˆë ¨ ìƒ˜í”Œ ìˆ˜', 'ê°’': f"{data_info.get('train_samples', 0):,}"},
                                    {'í•­ëª©': 'ê²€ì¦ ìƒ˜í”Œ ìˆ˜', 'ê°’': f"{data_info.get('val_samples', 0):,}"},
                                    {'í•­ëª©': 'ì…ë ¥ í˜•íƒœ', 'ê°’': str(data_info.get('input_shape', 'N/A'))},
                                    {'í•­ëª©': 'ì¶œë ¥ í˜•íƒœ', 'ê°’': str(data_info.get('output_shape', 'N/A'))},
                                    {'í•­ëª©': 'ì‹œê°„ íŠ¹ì§• ìˆ˜', 'ê°’': feature_info.get('time_features', 'N/A')},
                                    {'í•­ëª©': 'ë°ì´í„° íŠ¹ì§• ìˆ˜', 'ê°’': feature_info.get('data_features', 'N/A')},
                                    {'í•­ëª©': 'ì „ì²´ íŠ¹ì§• ìˆ˜', 'ê°’': feature_info.get('total_features', 'N/A')},
                                    {'í•­ëª©': 'ìƒì„± ì‹œê°„', 'ê°’': metadata.get('creation_time', 'N/A')[:19] if metadata.get('creation_time') else 'N/A'}
                                ])
                                st.dataframe(info_df, use_container_width=True, hide_index=True)
                            
                            # íŠ¹ì§• ì´ë¦„ ë§¤í•‘ í‘œì‹œ
                            if 'feature_info' in metadata and len(feature_names) == total_features:
                                st.markdown("**ğŸ·ï¸ íŠ¹ì§• ì´ë¦„ ë§¤í•‘**")
                                
                                # ì‹œê°„ íŠ¹ì§•ê³¼ ë°ì´í„° íŠ¹ì§•ì„ ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ
                                time_features_count = metadata['feature_info'].get('time_features', 0)
                                
                                feature_mapping = []
                                for i, name in enumerate(feature_names):
                                    feature_type = "ì‹œê°„ íŠ¹ì§•" if i < time_features_count else "ë°ì´í„° íŠ¹ì§•"
                                    feature_mapping.append({
                                        'ì¸ë±ìŠ¤': i,
                                        'íŠ¹ì§•ëª…': name,
                                        'íƒ€ì…': feature_type
                                    })
                                
                                mapping_df = pd.DataFrame(feature_mapping)
                                st.dataframe(mapping_df, use_container_width=True, hide_index=True)
                            
                            # íŒŒì¼ ì •ë³´
                            if 'data_info' in metadata:
                                train_files = metadata['data_info'].get('train_files', [])
                                val_files = metadata['data_info'].get('val_files', [])
                                
                                if train_files or val_files:
                                    st.markdown("**ğŸ“ ì‚¬ìš©ëœ íŒŒì¼ ì •ë³´**")
                                    file_col1, file_col2 = st.columns(2)
                                    
                                    with file_col1:
                                        if train_files:
                                            st.markdown("*í›ˆë ¨ìš© íŒŒì¼:*")
                                            for i, file_name in enumerate(train_files, 1):
                                                st.write(f"{i}. {file_name}")
                                    
                                    with file_col2:
                                        if val_files:
                                            st.markdown("*ê²€ì¦ìš© íŒŒì¼:*")
                                            for i, file_name in enumerate(val_files, 1):
                                                st.write(f"{i}. {file_name}")
                        
                        else:
                            st.info("ğŸ¯ ì‹œê°í™”í•  íŠ¹ì§•ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    else:
                        st.info("ğŸ“Š ì¶”ì¶œëœ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")




# =================================================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
# =================================================================================
if __name__ == "__main__":
    main()




