"""
DNN ê´€ë ¨ ìœ í‹¸ë¦¬í‹°
"""
import streamlit as st
import pandas as pd
import numpy as np
import io
from typing import List, Dict, Tuple
from .file_utils import load_data_file


def create_positional_encoding(position: int, d_model: int = 8) -> np.ndarray:
    """ì‹œê°„ í¬ì§€ì…˜ì— ëŒ€í•œ positional encoding ìƒì„±"""
    pe = np.zeros(d_model)
    for i in range(0, d_model, 2):
        pe[i] = np.sin(position / (10000 ** (i / d_model)))
        if i + 1 < d_model:
            pe[i + 1] = np.cos(position / (10000 ** (i / d_model)))
    return pe


def extract_time_features(timestamp_value, use_positional_encoding: bool = True) -> np.ndarray:
    """timestampë¡œë¶€í„° ì‹œê°„ íŠ¹ì§• ì¶”ì¶œ"""

    # Timestamp íƒ€ì…ì„ ìˆ«ìë¡œ ë³€í™˜
    if hasattr(timestamp_value, 'timestamp'):
        # pandas Timestamp ê°ì²´ì¸ ê²½ìš°
        timestamp_seconds = timestamp_value.timestamp()
    elif isinstance(timestamp_value, (int, float)):
        # ì´ë¯¸ ìˆ«ìì¸ ê²½ìš°
        timestamp_seconds = float(timestamp_value)
    else:
        try:
            # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì¸ ê²½ìš° pandasë¡œ ë³€í™˜ ì‹œë„
            timestamp_seconds = pd.to_datetime(timestamp_value).timestamp()
        except:
            # ë³€í™˜ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
            timestamp_seconds = 0.0

    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì§• (ì‹œ, ë¶„, ì´ˆ)
    hours = int((timestamp_seconds // 3600) % 24)
    minutes = int((timestamp_seconds % 3600) // 60)
    seconds = int(timestamp_seconds % 60)

    # ì •ê·œí™”ëœ ì‹œê°„ íŠ¹ì§• (0-1 ë²”ìœ„)
    time_features = np.array([
        hours / 23.0,           # ì‹œê°„ (0-23 -> 0-1)
        minutes / 59.0,         # ë¶„ (0-59 -> 0-1)
        seconds / 59.0          # ì´ˆ (0-59 -> 0-1)
    ])

    if use_positional_encoding:
        # Positional encoding ì¶”ê°€
        pe = create_positional_encoding(int(timestamp_seconds // 5))  # 5ì´ˆ ë‹¨ìœ„
        time_features = np.concatenate([time_features, pe])

    return time_features


def extract_time_features_vectorized(timestamp_array: np.ndarray, use_positional_encoding: bool = True) -> np.ndarray:
    """ë²¡í„°í™”ëœ ì‹œê°„ íŠ¹ì§• ì¶”ì¶œ"""

    # Timestamp ë°°ì—´ì„ ìˆ«ìë¡œ ë³€í™˜
    timestamp_seconds = np.zeros_like(timestamp_array, dtype=np.float64)

    for i, timestamp_value in enumerate(timestamp_array):
        if hasattr(timestamp_value, 'timestamp'):
            # pandas Timestamp ê°ì²´ì¸ ê²½ìš°
            timestamp_seconds[i] = timestamp_value.timestamp()
        elif isinstance(timestamp_value, (int, float)):
            # ì´ë¯¸ ìˆ«ìì¸ ê²½ìš°
            timestamp_seconds[i] = float(timestamp_value)
        else:
            try:
                # ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì¸ ê²½ìš° pandasë¡œ ë³€í™˜ ì‹œë„
                timestamp_seconds[i] = pd.to_datetime(timestamp_value).timestamp()
            except:
                # ë³€í™˜ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                timestamp_seconds[i] = 0.0

    # ë²¡í„°í™”ëœ ì‹œê°„ íŠ¹ì§• ê³„ì‚°
    hours = ((timestamp_seconds // 3600) % 24) / 23.0
    minutes = ((timestamp_seconds % 3600) // 60) / 59.0
    seconds = (timestamp_seconds % 60) / 59.0

    # ê¸°ë³¸ ì‹œê°„ íŠ¹ì§•
    time_features = np.column_stack([hours, minutes, seconds])

    if use_positional_encoding:
        # Positional encoding ë²¡í„°í™”
        positions = (timestamp_seconds // 5).astype(int)  # 5ì´ˆ ë‹¨ìœ„
        pe_array = create_positional_encoding_vectorized(positions, d_model=8)
        time_features = np.concatenate([time_features, pe_array], axis=1)

    return time_features.astype(np.float32)


def create_positional_encoding_vectorized(positions: np.ndarray, d_model: int = 8) -> np.ndarray:
    """ë²¡í„°í™”ëœ positional encoding ìƒì„±"""

    # positions shape: (n,) -> (n, 1)
    pos = positions[:, np.newaxis]

    # ì¸ë±ìŠ¤ ë°°ì—´ ìƒì„±
    i = np.arange(0, d_model, 2)[np.newaxis, :]  # shape: (1, d_model//2)

    # ê°ë„ ê³„ì‚° (ë²¡í„°í™”)
    angles = pos / (10000 ** (i / d_model))  # shape: (n, d_model//2)

    # PE ë°°ì—´ ì´ˆê¸°í™”
    pe = np.zeros((len(positions), d_model), dtype=np.float32)

    # sinê³¼ cos ê³„ì‚° (ë²¡í„°í™”)
    pe[:, 0::2] = np.sin(angles)  # ì§ìˆ˜ ì¸ë±ìŠ¤
    if d_model % 2 == 1:
        pe[:, 1::2] = np.cos(angles[:, :-1])  # í™€ìˆ˜ ì¸ë±ìŠ¤ (ë§ˆì§€ë§‰ ì œì™¸)
    else:
        pe[:, 1::2] = np.cos(angles)  # í™€ìˆ˜ ì¸ë±ìŠ¤

    return pe


def extract_dnn_samples_optimized(df: pd.DataFrame, start_pos: int, end_pos: int,
                                  lookback: int, horizon: int, step_gap: int = 1,
                                  timestamp_col: str = None, use_positional_encoding: bool = True) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:
    """ìµœì í™”ëœ ë‹¨ì¼ íŒŒì¼ì—ì„œ DNN í•™ìŠµìš© ìƒ˜í”Œ ì¶”ì¶œ (ë²¡í„°í™” ì—°ì‚° ì‚¬ìš©)"""

    # timestamp ì»¬ëŸ¼ í™•ì¸
    if timestamp_col is None:
        # timestamp ê´€ë ¨ ì»¬ëŸ¼ ìë™ ê²€ìƒ‰
        timestamp_candidates = [col for col in df.columns if 'time' in col.lower() or 'timestamp' in col.lower()]
        if timestamp_candidates:
            timestamp_col = timestamp_candidates[0]
        else:
            timestamp_col = df.columns[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ timestampë¡œ ì‚¬ìš©

    # íŠ¹ì§• ì»¬ëŸ¼ë“¤ (timestamp ì œì™¸)
    feature_cols = [col for col in df.columns if col != timestamp_col]

    # ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ í–¥ìƒ)
    data_features_array = df[feature_cols].values.astype(np.float32)  # float32ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½

    # ê²°ì¸¡ê°’ ì²˜ë¦¬ (í•œ ë²ˆì— ì²˜ë¦¬)
    data_features_array = np.nan_to_num(data_features_array, nan=0.0)

    # timestamp ë°°ì—´ ì¤€ë¹„
    if timestamp_col in df.columns:
        timestamp_array = df[timestamp_col].values
        # timestamp ê²°ì¸¡ê°’ ì²˜ë¦¬
        nan_mask = pd.isna(timestamp_array)
        if nan_mask.any():
            # ê²°ì¸¡ê°’ì„ ì¸ë±ìŠ¤ * 5ì´ˆë¡œ ëŒ€ì²´
            timestamp_array = np.where(nan_mask, np.arange(len(df)) * 5, timestamp_array)
    else:
        # timestamp ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ * 5ì´ˆë¡œ ìƒì„±
        timestamp_array = np.arange(len(df)) * 5

    # ì‹œê°„ íŠ¹ì§• ë°°ì—´ ë¯¸ë¦¬ ê³„ì‚° (ë²¡í„°í™”)
    time_features_array = extract_time_features_vectorized(timestamp_array, use_positional_encoding)

    # ë°ì´í„°ì™€ ì‹œê°„ íŠ¹ì§• ê²°í•©
    combined_features_array = np.concatenate([time_features_array, data_features_array], axis=1)

    # ìƒ˜í”Œ ì¶”ì¶œ ë²”ìœ„ ê³„ì‚°
    max_pos = min(end_pos, len(df) - horizon)
    actual_start = max(start_pos, lookback)

    # ìœ íš¨í•œ ìƒ˜í”Œ ìœ„ì¹˜ë“¤ ê³„ì‚°
    sample_positions = np.arange(actual_start, max_pos, step_gap)

    if len(sample_positions) == 0:
        return np.array([]), np.array([]), []

    # ì…ë ¥ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ìƒì„± (ë²¡í„°í™”)
    # shape: (num_samples, lookback)
    input_indices = sample_positions[:, np.newaxis] - np.arange(lookback, 0, -1)[np.newaxis, :]

    # ì¶œë ¥ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ ìƒì„± (ë²¡í„°í™”)
    # shape: (num_samples, horizon)
    output_indices = sample_positions[:, np.newaxis] + np.arange(horizon)[np.newaxis, :]

    # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
    valid_input_mask = (input_indices >= 0) & (input_indices < len(combined_features_array))
    valid_output_mask = (output_indices >= 0) & (output_indices < len(combined_features_array))
    valid_samples_mask = valid_input_mask.all(axis=1) & valid_output_mask.all(axis=1)

    # ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì„ íƒ
    valid_sample_positions = sample_positions[valid_samples_mask]
    valid_input_indices = input_indices[valid_samples_mask]
    valid_output_indices = output_indices[valid_samples_mask]

    if len(valid_sample_positions) == 0:
        return np.array([]), np.array([]), []

    # ë²¡í„°í™”ëœ ì¸ë±ì‹±ìœ¼ë¡œ ìƒ˜í”Œ ì¶”ì¶œ
    # input_samples shape: (num_samples, lookback, features)
    input_samples = combined_features_array[valid_input_indices]

    # output_samples shape: (num_samples, horizon, features)
    output_samples = combined_features_array[valid_output_indices]

    # ìƒ˜í”Œ ì •ë³´ ìƒì„± (ë²¡í„°í™”)
    sample_info = []
    for i, pos in enumerate(valid_sample_positions):
        sample_info.append({
            'sample_index': i,
            'input_start': int(pos - lookback),
            'input_end': int(pos),
            'output_start': int(pos),
            'output_end': int(pos + horizon),
            'current_position': int(pos)
        })

    return input_samples.astype(np.float32), output_samples.astype(np.float32), sample_info


def extract_dnn_samples(df: pd.DataFrame, start_pos: int, end_pos: int,
                       lookback: int, horizon: int, step_gap: int = 1,
                       timestamp_col: str = None) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:
    """ê¸°ì¡´ í•¨ìˆ˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìµœì í™”ëœ ë²„ì „ í˜¸ì¶œ"""

    # use_positional_encodingì€ ì „ì—­ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ True ì‚¬ìš©
    try:
        # Streamlit ì„¸ì…˜ ìƒíƒœì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        use_positional_encoding = st.session_state.get('dnn_pos_encoding', True)
    except:
        use_positional_encoding = True

    return extract_dnn_samples_optimized(
        df, start_pos, end_pos, lookback, horizon, step_gap,
        timestamp_col, use_positional_encoding
    )


def process_all_files_for_dnn(train_files: List, val_files: List,
                             start_pos: int, end_pos: int, lookback: int,
                             horizon: int, step_gap: int) -> Dict:
    """ëª¨ë“  íŒŒì¼ì—ì„œ DNN ë°ì´í„° ì¶”ì¶œ"""

    train_inputs = []
    train_outputs = []
    train_info = []

    val_inputs = []
    val_outputs = []
    val_info = []

    # Training íŒŒì¼ë“¤ ì²˜ë¦¬
    st.write("ğŸ”„ Training ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    for i, file in enumerate(train_files):
        try:
            df = load_data_file(file)
            if df is not None:
                input_arr, output_arr, info = extract_dnn_samples(
                    df, start_pos, end_pos, lookback, horizon, step_gap
                )

                if len(input_arr) > 0:
                    train_inputs.append(input_arr)
                    train_outputs.append(output_arr)

                    # íŒŒì¼ ì •ë³´ ì¶”ê°€
                    for sample_info in info:
                        sample_info['file_name'] = file.name
                        sample_info['file_index'] = i
                        sample_info['split'] = 'train'
                    train_info.extend(info)

                st.write(f"   âœ… {file.name}: {len(input_arr)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
        except Exception as e:
            st.error(f"   âŒ {file.name}: ì²˜ë¦¬ ì‹¤íŒ¨ - {str(e)}")

    # Validation íŒŒì¼ë“¤ ì²˜ë¦¬
    st.write("ğŸ”„ Validation ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    for i, file in enumerate(val_files):
        try:
            df = load_data_file(file)
            if df is not None:
                input_arr, output_arr, info = extract_dnn_samples(
                    df, start_pos, end_pos, lookback, horizon, step_gap
                )

                if len(input_arr) > 0:
                    val_inputs.append(input_arr)
                    val_outputs.append(output_arr)

                    # íŒŒì¼ ì •ë³´ ì¶”ê°€
                    for sample_info in info:
                        sample_info['file_name'] = file.name
                        sample_info['file_index'] = i
                        sample_info['split'] = 'validation'
                    val_info.extend(info)

                st.write(f"   âœ… {file.name}: {len(input_arr)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
        except Exception as e:
            st.error(f"   âŒ {file.name}: ì²˜ë¦¬ ì‹¤íŒ¨ - {str(e)}")

    # ë°ì´í„° ê²°í•©
    final_train_inputs = np.concatenate(train_inputs, axis=0) if train_inputs else np.array([])
    final_train_outputs = np.concatenate(train_outputs, axis=0) if train_outputs else np.array([])

    final_val_inputs = np.concatenate(val_inputs, axis=0) if val_inputs else np.array([])
    final_val_outputs = np.concatenate(val_outputs, axis=0) if val_outputs else np.array([])

    return {
        'train_inputs': final_train_inputs,
        'train_outputs': final_train_outputs,
        'train_info': train_info,
        'val_inputs': final_val_inputs,
        'val_outputs': final_val_outputs,
        'val_info': val_info
    }


def save_dnn_dataset(dataset: Dict, metadata: Dict, filename: str) -> bytes:
    """DNN ë°ì´í„°ì…‹ì„ NPY í˜•ì‹ìœ¼ë¡œ ì €ì¥"""

    # ì „ì²´ ë°ì´í„° êµ¬ì„±
    full_dataset = {
        'metadata': metadata,
        'train_inputs': dataset['train_inputs'],
        'train_outputs': dataset['train_outputs'],
        'train_info': dataset['train_info'],
        'val_inputs': dataset['val_inputs'],
        'val_outputs': dataset['val_outputs'],
        'val_info': dataset['val_info']
    }

    # numpy save í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”
    buffer = io.BytesIO()
    np.save(buffer, full_dataset, allow_pickle=True)
    buffer.seek(0)

    return buffer.getvalue()
