import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
from datetime import datetime
import json

# =================================================================================
# í•œê¸€ í°íŠ¸ ì„¤ì •
# =================================================================================
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì • í•¨ìˆ˜"""
    try:
        from matplotlib import font_manager, rc
        # Windows í™˜ê²½
        font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
        rc('font', family=font_name)
        plt.rcParams['axes.unicode_minus'] = False
    except:
        try:
            # Linux í™˜ê²½
            from matplotlib import font_manager, rc
            font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
            font_name = font_manager.FontProperties(fname=font_path).get_name()
            rc('font', family=font_name)  
            plt.rcParams['axes.unicode_minus'] = False
        except:
            # í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
            plt.rcParams['axes.unicode_minus'] = False

# matplotlib ê²½ê³  ì œê±°ë¥¼ ìœ„í•œ ì„¤ì •
plt.rcParams['figure.max_open_warning'] = 50

# =================================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =================================================================================
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'dataset' not in st.session_state:
        st.session_state.dataset = None
    if 'metadata' not in st.session_state:
        st.session_state.metadata = None
    if 'feature_names' not in st.session_state:
        st.session_state.feature_names = []
    if 'normalization_params' not in st.session_state:
        st.session_state.normalization_params = None
    if 'normalized_dataset' not in st.session_state:
        st.session_state.normalized_dataset = None
    if 'shifted_dataset' not in st.session_state:
        st.session_state.shifted_dataset = None
    if 'reshaped_dataset' not in st.session_state:
        st.session_state.reshaped_dataset = None
    if 'selected_features' not in st.session_state:
        st.session_state.selected_features = None

def load_npy_dataset(uploaded_file):
    """NPY íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
    try:
        # NPY íŒŒì¼ ë¡œë“œ
        dataset = np.load(uploaded_file, allow_pickle=True).item()
        
        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        required_keys = ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs', 'metadata']
        missing_keys = [key for key in required_keys if key not in dataset]
        
        if missing_keys:
            st.error(f"âŒ ë°ì´í„°ì…‹ì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {missing_keys}")
            return None, None
        
        # íŠ¹ì§• ì´ë¦„ ì¶”ì¶œ
        metadata = dataset['metadata']
        feature_names = []
        
        if 'feature_info' in metadata:
            time_feature_names = metadata['feature_info'].get('time_feature_names', [])
            data_feature_names = metadata['feature_info'].get('data_feature_names', [])
            feature_names = time_feature_names + data_feature_names
        
        # íŠ¹ì§• ì´ë¦„ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
        if not feature_names and len(dataset['train_inputs']) > 0:
            total_features = dataset['train_inputs'].shape[2]
            feature_names = [f"Feature_{i+1}" for i in range(total_features)]
        
        return dataset, feature_names
        
    except Exception as e:
        st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None

def display_dataset_info(dataset, feature_names):
    """ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ"""
    st.subheader("ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´")
    
    # ê¸°ë³¸ ì •ë³´
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“ í›ˆë ¨ ìƒ˜í”Œ", f"{len(dataset['train_inputs']):,}")
    with col2:
        st.metric("ğŸ”¬ ê²€ì¦ ìƒ˜í”Œ", f"{len(dataset['val_inputs']):,}")
    with col3:
        st.metric("ğŸ“ˆ íŠ¹ì§• ìˆ˜", len(feature_names))
    with col4:
        st.metric("ğŸ“Š ì´ ìƒ˜í”Œ", f"{len(dataset['train_inputs']) + len(dataset['val_inputs']):,}")
    
    # í˜•íƒœ ì •ë³´
    with st.expander("ğŸ“‹ ë°ì´í„° í˜•íƒœ ì •ë³´"):
        col1, col2 = st.columns(2)
        with col1:
            st.write("**í›ˆë ¨ ë°ì´í„°:**")
            st.write(f"- ì…ë ¥ í˜•íƒœ: {dataset['train_inputs'].shape}")
            st.write(f"- ì¶œë ¥ í˜•íƒœ: {dataset['train_outputs'].shape}")
        with col2:
            st.write("**ê²€ì¦ ë°ì´í„°:**")
            st.write(f"- ì…ë ¥ í˜•íƒœ: {dataset['val_inputs'].shape}")
            st.write(f"- ì¶œë ¥ í˜•íƒœ: {dataset['val_outputs'].shape}")
    
    # íŠ¹ì§• ì´ë¦„
    with st.expander("ğŸ·ï¸ íŠ¹ì§• ì´ë¦„ ëª©ë¡"):
        feature_df = pd.DataFrame({
            'ì¸ë±ìŠ¤': range(len(feature_names)),
            'íŠ¹ì§•ëª…': feature_names
        })
        st.dataframe(feature_df, use_container_width=True, hide_index=True)
    
    # ë©”íƒ€ë°ì´í„° ì •ë³´
    if 'metadata' in dataset:
        with st.expander("ğŸ“‹ ë©”íƒ€ë°ì´í„° ì •ë³´"):
            metadata = dataset['metadata']
            st.json(metadata, expanded=False)

def calculate_normalization_params(train_inputs):
    """ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚° (Min-Max ì •ê·œí™”)"""
    num_features = train_inputs.shape[2]
    params = {}
    
    for i in range(num_features):
        feature_data = train_inputs[:, :, i].flatten()
        params[i] = {
            'min': float(np.min(feature_data)),
            'max': float(np.max(feature_data)),
            'range': float(np.max(feature_data) - np.min(feature_data))
        }
    
    return params

def apply_normalization(data, normalization_params):
    """Min-Max ì •ê·œí™” ì ìš©"""
    normalized_data = data.copy()
    
    for i, params in normalization_params.items():
        if params['range'] > 0:  # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€
            normalized_data[:, :, i] = (data[:, :, i] - params['min']) / params['range']
        else:
            normalized_data[:, :, i] = 0  # ìƒìˆ˜ì¸ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
    
    return normalized_data

def apply_feature_shift(data, feature_idx, shift_amount):
    """íŠ¹ì • íŠ¹ì§•ì— ì‹œí”„íŠ¸ ì ìš©"""
    shifted_data = data.copy()
    
    if shift_amount > 0:  # ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì‹œí”„íŠ¸ (ì§€ì—°)
        shifted_data[:, shift_amount:, feature_idx] = data[:, :-shift_amount, feature_idx]
        shifted_data[:, :shift_amount, feature_idx] = data[:, 0:1, feature_idx]  # ì²« ë²ˆì§¸ ê°’ìœ¼ë¡œ íŒ¨ë”©
    elif shift_amount < 0:  # ì™¼ìª½ìœ¼ë¡œ ì‹œí”„íŠ¸ (ì•ë‹¹ê¹€)
        shift_amount = abs(shift_amount)
        shifted_data[:, :-shift_amount, feature_idx] = data[:, shift_amount:, feature_idx]
        shifted_data[:, -shift_amount:, feature_idx] = data[:, -1:, feature_idx]  # ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ íŒ¨ë”©
    
    return shifted_data

def select_features_from_dataset(dataset, input_features, output_features):
    """ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¬êµ¬ì„±"""
    selected_dataset = {}
    
    # ì…ë ¥ íŠ¹ì§• ì„ íƒ
    selected_dataset['train_inputs'] = dataset['train_inputs'][:, :, input_features]
    selected_dataset['val_inputs'] = dataset['val_inputs'][:, :, input_features]
    
    # ì¶œë ¥ íŠ¹ì§• ì„ íƒ
    selected_dataset['train_outputs'] = dataset['train_outputs'][:, :, output_features]
    selected_dataset['val_outputs'] = dataset['val_outputs'][:, :, output_features]
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    if 'metadata' in dataset:
        selected_dataset['metadata'] = dataset['metadata'].copy()
        selected_dataset['metadata']['selected_input_features'] = input_features
        selected_dataset['metadata']['selected_output_features'] = output_features
    
    return selected_dataset

# =================================================================================
# íƒ­ë³„ ë©”ì¸ í•¨ìˆ˜ë“¤
# =================================================================================
def tab_data_input():
    """íƒ­1: ë°ì´í„° ì…ë ¥"""
    st.header("ğŸ“ ë°ì´í„° ì…ë ¥")
    st.markdown("NPY í˜•ì‹ì˜ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    
    uploaded_file = st.file_uploader(
        "NPY íŒŒì¼ ì„ íƒ",
        type=['npy'],
        help="ë”¥ëŸ¬ë‹ í•™ìŠµìš©ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ NPY íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    )
    
    if uploaded_file is not None:
        with st.spinner("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘..."):
            dataset, feature_names = load_npy_dataset(uploaded_file)
            
            if dataset is not None:
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.dataset = dataset
                st.session_state.feature_names = feature_names
                st.session_state.metadata = dataset.get('metadata', {})
                
                st.success("âœ… ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
                display_dataset_info(dataset, feature_names)
                
                # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ğŸ‘€ ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                    if len(dataset['train_inputs']) > 0:
                        # ìƒ˜í”Œ ì„ íƒ ë°©ë²• ì„ íƒ
                        col_method, col_select = st.columns([1, 2])
                        
                        with col_method:
                            selection_method = st.radio(
                                "ìƒ˜í”Œ ì„ íƒ ë°©ë²•",
                                ["ëª©ë¡ì—ì„œ ì„ íƒ", "ì§ì ‘ ì…ë ¥"],
                                key="sample_selection_method"
                            )
                        
                        with col_select:
                            if selection_method == "ëª©ë¡ì—ì„œ ì„ íƒ":
                                sample_idx = st.selectbox(
                                    "ë¯¸ë¦¬ë³¼ ìƒ˜í”Œ ì„ íƒ",
                                    range(min(10, len(dataset['train_inputs']))),
                                    key="sample_preview_select"
                                )
                            else:
                                max_sample = len(dataset['train_inputs']) - 1
                                sample_idx = st.number_input(
                                    f"ìƒ˜í”Œ ë²ˆí˜¸ ì…ë ¥ (0~{max_sample})",
                                    min_value=0,
                                    max_value=max_sample,
                                    value=0,
                                    key="sample_preview_input"
                                )
                        
                        # ì‹œê°í™”í•  íŠ¹ì§• ì„ íƒ
                        max_features = min(5, len(feature_names))
                        selected_features = st.multiselect(
                            f"ì‹œê°í™”í•  íŠ¹ì§• ì„ íƒ (ìµœëŒ€ {max_features}ê°œ)",
                            range(len(feature_names)),
                            default=list(range(max_features)),
                            format_func=lambda x: f"{x}: {feature_names[x]}",
                            key="preview_features"
                        )
                        
                        if selected_features:
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**ì…ë ¥ ì‹œí€€ìŠ¤**")
                                fig_input = go.Figure()
                                input_sample = dataset['train_inputs'][sample_idx]
                                
                                for feature_idx in selected_features[:max_features]:
                                    fig_input.add_trace(go.Scatter(
                                        y=input_sample[:, feature_idx],
                                        mode='lines+markers',
                                        name=feature_names[feature_idx],
                                        line=dict(width=2)
                                    ))
                                
                                fig_input.update_layout(
                                    title="ì…ë ¥ ì‹œí€€ìŠ¤",
                                    xaxis_title="Time Steps",
                                    yaxis_title="Values",
                                    height=300
                                )
                                st.plotly_chart(fig_input, use_container_width=True)
                            
                            with col2:
                                st.markdown("**ì¶œë ¥ ì‹œí€€ìŠ¤**")
                                fig_output = go.Figure()
                                output_sample = dataset['train_outputs'][sample_idx]
                                
                                for feature_idx in selected_features[:max_features]:
                                    fig_output.add_trace(go.Scatter(
                                        y=output_sample[:, feature_idx],
                                        mode='lines+markers',
                                        name=feature_names[feature_idx],
                                        line=dict(width=2)
                                    ))
                                
                                fig_output.update_layout(
                                    title="ì¶œë ¥ ì‹œí€€ìŠ¤",
                                    xaxis_title="Time Steps",
                                    yaxis_title="Values",
                                    height=300
                                )
                                st.plotly_chart(fig_output, use_container_width=True)

def tab_normalization():
    """íƒ­2: ì •ê·œí™”"""
    st.header("ğŸ“ ë°ì´í„° ì •ê·œí™”")
    
    if st.session_state.dataset is None:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    dataset = st.session_state.dataset
    feature_names = st.session_state.feature_names
    
    st.markdown("Min-Max ì •ê·œí™”ë¥¼ í†µí•´ ëª¨ë“  íŠ¹ì§•ì„ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.")
    
    # ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚° ë˜ëŠ” ë¡œë“œ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("**ì •ê·œí™” íŒŒë¼ë¯¸í„° ì„¤ì • ë°©ë²•:**")
        param_source = st.radio(
            "íŒŒë¼ë¯¸í„° ì†ŒìŠ¤ ì„ íƒ",
            ["ë°ì´í„°ì—ì„œ ìë™ ê³„ì‚°", "íŒŒì¼ì—ì„œ ë¡œë“œ"],
            key="param_source_method"
        )
    
    with col2:
        if param_source == "íŒŒì¼ì—ì„œ ë¡œë“œ":
            uploaded_param_file = st.file_uploader(
                "íŒŒë¼ë¯¸í„° íŒŒì¼ ì—…ë¡œë“œ",
                type=['json'],
                help="ì´ì „ì— ì €ì¥í•œ ì •ê·œí™” íŒŒë¼ë¯¸í„° JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
                key="param_file_upload"
            )
    
    # íŒŒë¼ë¯¸í„° ê³„ì‚° ë˜ëŠ” ë¡œë“œ ì‹¤í–‰
    if param_source == "ë°ì´í„°ì—ì„œ ìë™ ê³„ì‚°":
        if st.button("ğŸ”¢ ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚°", key="calc_norm_params"):
            with st.spinner("ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚° ì¤‘..."):
                norm_params = calculate_normalization_params(dataset['train_inputs'])
                st.session_state.normalization_params = norm_params
                st.success("âœ… ì •ê·œí™” íŒŒë¼ë¯¸í„°ê°€ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    elif param_source == "íŒŒì¼ì—ì„œ ë¡œë“œ":
        if uploaded_param_file is not None:
            if st.button("ğŸ“ íŒŒë¼ë¯¸í„° íŒŒì¼ ë¡œë“œ", key="load_norm_params"):
                try:
                    with st.spinner("íŒŒë¼ë¯¸í„° íŒŒì¼ ë¡œë”© ì¤‘..."):
                        param_data = json.load(uploaded_param_file)
                        
                        # íŒŒë¼ë¯¸í„° í˜•ì‹ ê²€ì¦
                        if 'normalization_params' in param_data:
                            loaded_params = param_data['normalization_params']
                            
                            # í‚¤ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (JSONì—ì„œëŠ” ë¬¸ìì—´ë¡œ ì €ì¥ë¨)
                            norm_params = {}
                            for key, value in loaded_params.items():
                                norm_params[int(key)] = value
                            
                            st.session_state.normalization_params = norm_params
                            st.success("âœ… íŒŒë¼ë¯¸í„° íŒŒì¼ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            
                            # ë¡œë“œëœ íŒŒë¼ë¯¸í„° ì •ë³´ í‘œì‹œ
                            st.info(f"ğŸ“Š ë¡œë“œëœ íŠ¹ì§• ìˆ˜: {len(norm_params)}ê°œ")
                            
                        else:
                            st.error("âŒ ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° íŒŒì¼ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
                
                except Exception as e:
                    st.error(f"âŒ íŒŒë¼ë¯¸í„° íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ì •ê·œí™” íŒŒë¼ë¯¸í„° í‘œì‹œ ë° ìˆ˜ì •
    if st.session_state.normalization_params is not None:
        st.subheader("ğŸ“Š ì •ê·œí™” íŒŒë¼ë¯¸í„°")
        
        norm_params = st.session_state.normalization_params.copy()
        
        # íŒŒë¼ë¯¸í„° í‘œì‹œ ë° ìˆ˜ì • ì¸í„°í˜ì´ìŠ¤
        with st.expander("âœï¸ ì •ê·œí™” íŒŒë¼ë¯¸í„° í™•ì¸ ë° ìˆ˜ì •"):
            st.markdown("ê° íŠ¹ì§•ì˜ Min/Max ê°’ì„ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            param_data = []
            for i, params in norm_params.items():
                param_data.append({
                    'íŠ¹ì§• ì¸ë±ìŠ¤': i,
                    'íŠ¹ì§•ëª…': feature_names[i] if i < len(feature_names) else f'Feature_{i}',
                    'ìµœì†Ÿê°’': params['min'],
                    'ìµœëŒ“ê°’': params['max'],
                    'ë²”ìœ„': params['range']
                })
            
            param_df = pd.DataFrame(param_data)
            st.dataframe(param_df, use_container_width=True, hide_index=True)
            
            # ìˆ˜ì • ì¸í„°í˜ì´ìŠ¤
            st.markdown("**íŒŒë¼ë¯¸í„° ìˆ˜ì •:**")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                modify_feature = st.selectbox(
                    "ìˆ˜ì •í•  íŠ¹ì§• ì„ íƒ",
                    range(len(feature_names)),
                    format_func=lambda x: f"{x}: {feature_names[x]}",
                    key="modify_feature_select"
                )
            
            with col2:
                current_min = norm_params[modify_feature]['min']
                new_min = st.number_input(
                    "ìƒˆë¡œìš´ ìµœì†Ÿê°’",
                    value=current_min,
                    key="new_min_value"
                )
            
            with col3:
                current_max = norm_params[modify_feature]['max']
                new_max = st.number_input(
                    "ìƒˆë¡œìš´ ìµœëŒ“ê°’",
                    value=current_max,
                    key="new_max_value"
                )
            
            if st.button("ğŸ“ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸", key="update_params"):
                if new_max > new_min:
                    norm_params[modify_feature]['min'] = new_min
                    norm_params[modify_feature]['max'] = new_max
                    norm_params[modify_feature]['range'] = new_max - new_min
                    st.session_state.normalization_params = norm_params
                    st.success(f"âœ… {feature_names[modify_feature]} íŠ¹ì§•ì˜ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
                else:
                    st.error("âŒ ìµœëŒ“ê°’ì€ ìµœì†Ÿê°’ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì •ê·œí™” íŒŒë¼ë¯¸í„° ì €ì¥ ì˜µì…˜
        st.markdown("---")
        st.subheader("ğŸ’¾ ì •ê·œí™” íŒŒë¼ë¯¸í„° ì €ì¥")
        
        col1, col2 = st.columns([2, 1])
        with col1:
            param_filename = st.text_input(
                "ì €ì¥í•  íŒŒì¼ëª…",
                value="normalization_params",
                key="param_save_filename"
            )
        
        with col2:
            if st.button("ğŸ’¾ íŒŒë¼ë¯¸í„° ì €ì¥", key="save_norm_params"):
                try:
                    # íŒŒë¼ë¯¸í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„
                    save_data = {
                        'normalization_params': st.session_state.normalization_params,
                        'feature_names': feature_names,
                        'creation_time': datetime.now().isoformat(),
                        'total_features': len(feature_names),
                        'data_info': {
                            'train_samples': len(dataset['train_inputs']),
                            'input_shape': list(dataset['train_inputs'].shape),
                            'description': 'Min-Max normalization parameters'
                        }
                    }
                    
                    # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                    json_str = json.dumps(save_data, indent=2, ensure_ascii=False)
                    
                    st.download_button(
                        label="ğŸ“ íŒŒë¼ë¯¸í„° íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=json_str,
                        file_name=f"{param_filename}.json",
                        mime="application/json",
                        help="ì •ê·œí™” íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                    )
                    
                    st.success("âœ… íŒŒë¼ë¯¸í„° íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                except Exception as e:
                    st.error(f"âŒ íŒŒë¼ë¯¸í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # ì €ì¥ëœ íŒŒë¼ë¯¸í„° ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ‘€ ì €ì¥ë  íŒŒë¼ë¯¸í„° ë¯¸ë¦¬ë³´ê¸°"):
            preview_data = {
                'normalization_params': st.session_state.normalization_params,
                'feature_names': feature_names[:5] + (['...'] if len(feature_names) > 5 else []),  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                'total_features': len(feature_names)
            }
            st.json(preview_data)
        
        # ì •ê·œí™” ì ìš©
        if st.button("ğŸ¯ ì •ê·œí™” ì ìš©", key="apply_normalization"):
            with st.spinner("ì •ê·œí™” ì ìš© ì¤‘..."):
                normalized_dataset = {}
                
                # ëª¨ë“  ë°ì´í„°ì— ì •ê·œí™” ì ìš©
                normalized_dataset['train_inputs'] = apply_normalization(
                    dataset['train_inputs'], norm_params
                )
                normalized_dataset['train_outputs'] = apply_normalization(
                    dataset['train_outputs'], norm_params
                )
                normalized_dataset['val_inputs'] = apply_normalization(
                    dataset['val_inputs'], norm_params
                )
                normalized_dataset['val_outputs'] = apply_normalization(
                    dataset['val_outputs'], norm_params
                )
                
                # ë©”íƒ€ë°ì´í„° ë³µì‚¬
                if 'metadata' in dataset:
                    normalized_dataset['metadata'] = dataset['metadata'].copy()
                    normalized_dataset['metadata']['normalization_applied'] = True
                    normalized_dataset['metadata']['normalization_params'] = norm_params
                
                st.session_state.normalized_dataset = normalized_dataset
                st.success("âœ… ì •ê·œí™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì •ê·œí™” ê²°ê³¼ í™•ì¸
        if st.session_state.normalized_dataset is not None:
            st.subheader("ğŸ“ˆ ì •ê·œí™” ê²°ê³¼ í™•ì¸")
            
            normalized_dataset = st.session_state.normalized_dataset
            
            # ì •ê·œí™” ì „í›„ ë¹„êµë¥¼ ìœ„í•œ ì„ íƒ ì˜µì…˜ë“¤
            col1, col2, col3 = st.columns(3)
            
            with col1:
                data_type = st.selectbox(
                    "ë°ì´í„° íƒ€ì… ì„ íƒ",
                    ["train_inputs", "train_outputs", "val_inputs", "val_outputs"],
                    key="norm_compare_data_type"
                )
            
            with col2:
                feature_to_compare = st.selectbox(
                    "ë¹„êµí•  íŠ¹ì§• ì„ íƒ",
                    range(len(feature_names)),
                    format_func=lambda x: f"{x}: {feature_names[x]}",
                    key="norm_compare_feature"
                )
            
            with col3:
                sample_count = st.number_input(
                    "ë¶„ì„í•  ìƒ˜í”Œ ìˆ˜",
                    min_value=10,
                    max_value=min(100000, len(normalized_dataset[data_type])),
                    value=min(100, len(normalized_dataset[data_type])),
                    key="norm_sample_count"
                )
            
            # ë°ì´í„° íƒ€ì… í•œê¸€ í‘œì‹œ ë§¤í•‘
            data_type_korean = {
                "train_inputs": "í›ˆë ¨ìš© ì…ë ¥",
                "train_outputs": "í›ˆë ¨ìš© ë¼ë²¨",
                "val_inputs": "ê²€ì¦ìš© ì…ë ¥", 
                "val_outputs": "ê²€ì¦ìš© ë¼ë²¨"
            }
            
            st.markdown(f"**ì„ íƒëœ ë°ì´í„°**: {data_type_korean[data_type]} - {feature_names[feature_to_compare]}")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ì •ê·œí™” ì „**")
                original_data = dataset[data_type][:sample_count, :, feature_to_compare].flatten()
                
                fig_before = go.Figure()
                fig_before.add_trace(go.Histogram(
                    x=original_data,
                    nbinsx=50,
                    name="ì •ê·œí™” ì „",
                    marker_color='lightblue'
                ))
                fig_before.update_layout(
                    title=f"ì •ê·œí™” ì „ ë¶„í¬<br>{data_type_korean[data_type]}",
                    xaxis_title="ê°’",
                    yaxis_title="ë¹ˆë„",
                    height=350
                )
                st.plotly_chart(fig_before, use_container_width=True)
                
                # í†µê³„ ì •ë³´
                st.markdown("**í†µê³„ ì •ë³´:**")
                stats_before = pd.DataFrame({
                    'í†µê³„ëŸ‰': ['ìµœì†Ÿê°’', 'ìµœëŒ“ê°’', 'í‰ê· ', 'í‘œì¤€í¸ì°¨', 'ì¤‘ì•™ê°’'],
                    'ê°’': [
                        f"{np.min(original_data):.6f}",
                        f"{np.max(original_data):.6f}",
                        f"{np.mean(original_data):.6f}",
                        f"{np.std(original_data):.6f}",
                        f"{np.median(original_data):.6f}"
                    ]
                })
                st.dataframe(stats_before, use_container_width=True, hide_index=True)
            
            with col2:
                st.markdown("**ì •ê·œí™” í›„**")
                normalized_data = normalized_dataset[data_type][:sample_count, :, feature_to_compare].flatten()
                
                fig_after = go.Figure()
                fig_after.add_trace(go.Histogram(
                    x=normalized_data,
                    nbinsx=50,
                    name="ì •ê·œí™” í›„",
                    marker_color='lightgreen'
                ))
                fig_after.update_layout(
                    title=f"ì •ê·œí™” í›„ ë¶„í¬<br>{data_type_korean[data_type]}",
                    xaxis_title="ê°’ (0~1 ë²”ìœ„)",
                    yaxis_title="ë¹ˆë„",
                    height=350
                )
                st.plotly_chart(fig_after, use_container_width=True)
                
                # í†µê³„ ì •ë³´
                st.markdown("**í†µê³„ ì •ë³´:**")
                stats_after = pd.DataFrame({
                    'í†µê³„ëŸ‰': ['ìµœì†Ÿê°’', 'ìµœëŒ“ê°’', 'í‰ê· ', 'í‘œì¤€í¸ì°¨', 'ì¤‘ì•™ê°’'],
                    'ê°’': [
                        f"{np.min(normalized_data):.6f}",
                        f"{np.max(normalized_data):.6f}",
                        f"{np.mean(normalized_data):.6f}",
                        f"{np.std(normalized_data):.6f}",
                        f"{np.median(normalized_data):.6f}"
                    ]
                })
                st.dataframe(stats_after, use_container_width=True, hide_index=True)
            
            # ì •ê·œí™” í’ˆì§ˆ ê²€ì¦
            st.markdown("---")
            st.subheader("ğŸ” ì •ê·œí™” í’ˆì§ˆ ê²€ì¦")
            
            # ëª¨ë“  ë°ì´í„° íƒ€ì…ì— ëŒ€í•œ ì •ê·œí™” ë²”ìœ„ í™•ì¸
            validation_results = []
            for dt in ["train_inputs", "train_outputs", "val_inputs", "val_outputs"]:
                data = normalized_dataset[dt]
                min_val = np.min(data)
                max_val = np.max(data)
                
                # ë²”ìœ„ ë²—ì–´ë‚¨ ì²´í¬ (0~1 ë²”ìœ„)
                out_of_range = (min_val < -0.001) or (max_val > 1.001)  # ì‘ì€ ì˜¤ì°¨ í—ˆìš©
                
                validation_results.append({
                    'ë°ì´í„° íƒ€ì…': data_type_korean[dt],
                    'ìµœì†Ÿê°’': f"{min_val:.6f}",
                    'ìµœëŒ“ê°’': f"{max_val:.6f}",
                    'ì •ê·œí™” ìƒíƒœ': 'âœ… ì •ìƒ' if not out_of_range else 'âŒ ë²”ìœ„ ë²—ì–´ë‚¨'
                })
            
            validation_df = pd.DataFrame(validation_results)
            st.dataframe(validation_df, use_container_width=True, hide_index=True)
            
            # ì •ê·œí™” íŒŒë¼ë¯¸í„° í™•ì¸
            with st.expander("ğŸ“Š ì ìš©ëœ ì •ê·œí™” íŒŒë¼ë¯¸í„°"):
                if st.session_state.normalization_params:
                    param_list = []
                    for i, params in st.session_state.normalization_params.items():
                        param_list.append({
                            'íŠ¹ì§• ì¸ë±ìŠ¤': i,
                            'íŠ¹ì§•ëª…': feature_names[i] if i < len(feature_names) else f'Feature_{i}',
                            'ìµœì†Ÿê°’': f"{params['min']:.6f}",
                            'ìµœëŒ“ê°’': f"{params['max']:.6f}",
                            'ë²”ìœ„': f"{params['range']:.6f}"
                        })
                    
                    param_df = pd.DataFrame(param_list)
                    st.dataframe(param_df, use_container_width=True, hide_index=True)

def tab_feature_shift():
    """íƒ­3: íŠ¹ì§• ì‹œí”„íŠ¸"""
    st.header("â†”ï¸ íŠ¹ì§• ì‹œí”„íŠ¸")
    
    # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²°ì •
    if st.session_state.normalized_dataset is not None:
        current_dataset = st.session_state.normalized_dataset
        dataset_name = "ì •ê·œí™”ëœ ë°ì´í„°ì…‹"
    elif st.session_state.dataset is not None:
        current_dataset = st.session_state.dataset
        dataset_name = "ì›ë³¸ ë°ì´í„°ì…‹"
    else:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    feature_names = st.session_state.feature_names
    st.markdown(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°: **{dataset_name}**")
    st.markdown("íŠ¹ì • íŠ¹ì§•ì„ ì‹œê°„ ì¶•ì—ì„œ ì•ë‹¹ê¸°ê±°ë‚˜ ì§€ì—°ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì‹œí”„íŠ¸ ì„¤ì •
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        data_type = st.selectbox(
            "ë°ì´í„° íƒ€ì… ì„ íƒ",
            ["train_inputs", "train_outputs", "val_inputs", "val_outputs"],
            key="shift_data_type"
        )
    
    with col2:
        shift_feature = st.selectbox(
            "ì‹œí”„íŠ¸í•  íŠ¹ì§• ì„ íƒ",
            range(len(feature_names)),
            format_func=lambda x: f"{x}: {feature_names[x]}",
            key="shift_feature_select"
        )
    
    with col3:
        shift_amount = st.number_input(
            "ì‹œí”„íŠ¸ ì–‘ (í‹±)",
            min_value=-50,
            max_value=50,
            value=0,
            help="ì–‘ìˆ˜: ì˜¤ë¥¸ìª½ ì‹œí”„íŠ¸(ì§€ì—°), ìŒìˆ˜: ì™¼ìª½ ì‹œí”„íŠ¸(ì•ë‹¹ê¹€)",
            key="shift_amount"
        )
    
    with col4:
        st.markdown("**ì‹œí”„íŠ¸ ë°©í–¥:**")
        if shift_amount > 0:
            st.info("ğŸ”œ ì˜¤ë¥¸ìª½ ì‹œí”„íŠ¸ (ì§€ì—°)")
        elif shift_amount < 0:
            st.info("ğŸ”™ ì™¼ìª½ ì‹œí”„íŠ¸ (ì•ë‹¹ê¹€)")
        else:
            st.info("â¡ï¸ ì‹œí”„íŠ¸ ì—†ìŒ")
    
    # ì‹œí”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°
    if shift_amount != 0:
        with st.expander("ğŸ‘€ ì‹œí”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°"):
            sample_data = current_dataset[data_type][0, :, shift_feature]  # ì²« ë²ˆì§¸ ìƒ˜í”Œ
            shifted_sample = apply_feature_shift(
                current_dataset[data_type][:1], shift_feature, shift_amount
            )[0, :, shift_feature]
            
            fig_preview = go.Figure()
            fig_preview.add_trace(go.Scatter(
                y=sample_data,
                mode='lines+markers',
                name='ì›ë³¸',
                line=dict(color='blue', width=2)
            ))
            fig_preview.add_trace(go.Scatter(
                y=shifted_sample,
                mode='lines+markers',
                name=f'ì‹œí”„íŠ¸ ({shift_amount}í‹±)',
                line=dict(color='red', width=2, dash='dash')
            ))
            
            fig_preview.update_layout(
                title=f"{feature_names[shift_feature]} íŠ¹ì§• ì‹œí”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°",
                xaxis_title="Time Steps",
                yaxis_title="ê°’",
                height=400
            )
            st.plotly_chart(fig_preview, use_container_width=True)
    
    # ì‹œí”„íŠ¸ ì ìš©
    if st.button("ğŸ¯ ì‹œí”„íŠ¸ ì ìš©", key="apply_shift"):
        if shift_amount == 0:
            st.warning("âš ï¸ ì‹œí”„íŠ¸ ì–‘ì´ 0ì´ë¯€ë¡œ ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            with st.spinner("ì‹œí”„íŠ¸ ì ìš© ì¤‘..."):
                # í˜„ì¬ ë°ì´í„°ì…‹ ë³µì‚¬
                if st.session_state.shifted_dataset is None:
                    shifted_dataset = {}
                    for key in current_dataset.keys():
                        if isinstance(current_dataset[key], np.ndarray):
                            shifted_dataset[key] = current_dataset[key].copy()
                        else:
                            shifted_dataset[key] = current_dataset[key]
                else:
                    shifted_dataset = st.session_state.shifted_dataset
                
                # ì„ íƒëœ ë°ì´í„°ì— ì‹œí”„íŠ¸ ì ìš©
                shifted_dataset[data_type] = apply_feature_shift(
                    shifted_dataset[data_type], shift_feature, shift_amount
                )
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                if 'metadata' in shifted_dataset:
                    if 'shift_history' not in shifted_dataset['metadata']:
                        shifted_dataset['metadata']['shift_history'] = []
                    
                    shifted_dataset['metadata']['shift_history'].append({
                        'data_type': data_type,
                        'feature_index': shift_feature,
                        'feature_name': feature_names[shift_feature],
                        'shift_amount': shift_amount,
                        'timestamp': datetime.now().isoformat()
                    })
                
                st.session_state.shifted_dataset = shifted_dataset
                st.success(f"âœ… {feature_names[shift_feature]} íŠ¹ì§•ì— {shift_amount}í‹± ì‹œí”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬ í‘œì‹œ
    if (st.session_state.shifted_dataset is not None and 
        'metadata' in st.session_state.shifted_dataset and
        'shift_history' in st.session_state.shifted_dataset['metadata']):
        
        st.subheader("ğŸ“‹ ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬")
        
        shift_history = st.session_state.shifted_dataset['metadata']['shift_history']
        if shift_history:
            history_df = pd.DataFrame(shift_history)
            history_df = history_df[['data_type', 'feature_name', 'shift_amount', 'timestamp']]
            history_df.columns = ['ë°ì´í„° íƒ€ì…', 'íŠ¹ì§•ëª…', 'ì‹œí”„íŠ¸ ì–‘', 'ì ìš© ì‹œê°„']
            
            st.dataframe(history_df, use_container_width=True, hide_index=True)
            
            if st.button("ğŸ—‘ï¸ ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”", key="clear_shift_history"):
                st.session_state.shifted_dataset = None
                st.success("âœ… ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
        else:
            st.info("ğŸ“ ì•„ì§ ì ìš©ëœ ì‹œí”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

def tab_sequence_reshape():
    """íƒ­4: ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •"""
    st.header("ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •")
    
    # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²°ì •
    if st.session_state.shifted_dataset is not None:
        current_dataset = st.session_state.shifted_dataset
        dataset_name = "ì‹œí”„íŠ¸ ì ìš©ëœ ë°ì´í„°ì…‹"
    elif st.session_state.normalized_dataset is not None:
        current_dataset = st.session_state.normalized_dataset
        dataset_name = "ì •ê·œí™”ëœ ë°ì´í„°ì…‹"
    elif st.session_state.dataset is not None:
        current_dataset = st.session_state.dataset
        dataset_name = "ì›ë³¸ ë°ì´í„°ì…‹"
    else:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    feature_names = st.session_state.feature_names
    st.markdown(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°: **{dataset_name}**")
    
    # í˜„ì¬ ë°ì´í„° í˜•íƒœ ì •ë³´ í‘œì‹œ
    st.subheader("ğŸ“Š í˜„ì¬ ë°ì´í„° í˜•íƒœ")
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ì…ë ¥ ë°ì´í„°:**")
        current_input_shape = current_dataset['train_inputs'].shape
        st.write(f"- í›ˆë ¨ ì…ë ¥: {current_input_shape}")
        st.write(f"- ê²€ì¦ ì…ë ¥: {current_dataset['val_inputs'].shape}")
        st.write(f"- í˜„ì¬ Lookback ê¸¸ì´: **{current_input_shape[1]}**")
    
    with col2:
        st.markdown("**ì¶œë ¥ ë°ì´í„°:**")
        current_output_shape = current_dataset['train_outputs'].shape
        st.write(f"- í›ˆë ¨ ì¶œë ¥: {current_output_shape}")
        st.write(f"- ê²€ì¦ ì¶œë ¥: {current_dataset['val_outputs'].shape}")
        st.write(f"- í˜„ì¬ Horizon ê¸¸ì´: **{current_output_shape[1]}**")
    
    st.markdown("---")
    
    # ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
    st.subheader("âš™ï¸ ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        new_lookback = st.number_input(
            "ìƒˆë¡œìš´ Lookback ê¸¸ì´",
            min_value=1,
            max_value=current_input_shape[1],
            value=min(current_input_shape[1], 50),
            help="ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ (ê³¼ê±°ë¥¼ ì–¼ë§ˆë‚˜ ë³¼ ê²ƒì¸ê°€)",
            key="new_lookback_length"
        )
    
    with col2:
        new_horizon = st.number_input(
            "ìƒˆë¡œìš´ Horizon ê¸¸ì´", 
            min_value=1,
            max_value=current_output_shape[1],
            value=min(current_output_shape[1], 10),
            help="ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ (ë¯¸ë˜ë¥¼ ì–¼ë§ˆë‚˜ ì˜ˆì¸¡í•  ê²ƒì¸ê°€)",
            key="new_horizon_length"
        )
    
    with col3:
        st.markdown("**ë°ì´í„° ì¶”ì¶œ ë°©ë²•:**")
        st.info("ğŸ“‹ Lookback: ë’·ë¶€ë¶„ì—ì„œ ì¶”ì¶œ\nğŸ“‹ Horizon: ì•ë¶€ë¶„ì—ì„œ ì¶”ì¶œ")
        st.markdown("*ì¶”ì¶œ ë°©ë²•ì´ ê³ ì •ë˜ì–´ ì‹œí€€ìŠ¤ ì—°ì†ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.*")
    
    # ë³€ê²½ ì‚¬í•­ ë¯¸ë¦¬ë³´ê¸°
    if new_lookback != current_input_shape[1] or new_horizon != current_output_shape[1]:
        st.markdown("### ğŸ”„ ë³€ê²½ ì‚¬í•­ ë¯¸ë¦¬ë³´ê¸°")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ë³€ê²½ ì „ â†’ ë³€ê²½ í›„**")
            change_df = pd.DataFrame({
                'êµ¬ë¶„': ['Lookback ê¸¸ì´', 'Horizon ê¸¸ì´', 'ì´ ì‹œí€€ìŠ¤ ê¸¸ì´'],
                'ë³€ê²½ ì „': [
                    current_input_shape[1],
                    current_output_shape[1], 
                    current_input_shape[1] + current_output_shape[1]
                ],
                'ë³€ê²½ í›„': [
                    new_lookback,
                    new_horizon,
                    new_lookback + new_horizon
                ]
            })
            st.dataframe(change_df, use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("**ì˜ˆìƒ í˜•íƒœ ë³€í™”**")
            shape_df = pd.DataFrame({
                'ë°ì´í„° íƒ€ì…': ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs'],
                'í˜„ì¬ í˜•íƒœ': [
                    str(current_dataset['train_inputs'].shape),
                    str(current_dataset['train_outputs'].shape),
                    str(current_dataset['val_inputs'].shape),
                    str(current_dataset['val_outputs'].shape)
                ],
                'ë³€ê²½ í›„ í˜•íƒœ': [
                    f"({current_dataset['train_inputs'].shape[0]}, {new_lookback}, {current_dataset['train_inputs'].shape[2]})",
                    f"({current_dataset['train_outputs'].shape[0]}, {new_horizon}, {current_dataset['train_outputs'].shape[2]})",
                    f"({current_dataset['val_inputs'].shape[0]}, {new_lookback}, {current_dataset['val_inputs'].shape[2]})",
                    f"({current_dataset['val_outputs'].shape[0]}, {new_horizon}, {current_dataset['val_outputs'].shape[2]})"
                ]
            })
            st.dataframe(shape_df, use_container_width=True, hide_index=True)
        
        # ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ‘€ ë³€ê²½ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"):
            sample_idx = 0  # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ ë¯¸ë¦¬ë³´ê¸°
            
            # í˜„ì¬ ë°ì´í„° ì¶”ì¶œ
            current_input = current_dataset['train_inputs'][sample_idx]
            current_output = current_dataset['train_outputs'][sample_idx]
            
            # ìƒˆë¡œìš´ ê¸¸ì´ë¡œ ë³€í™˜ (ë¯¸ë¦¬ë³´ê¸°)
            # Lookback: ë’·ë¶€ë¶„ì—ì„œ ì¶”ì¶œ, Horizon: ì•ë¶€ë¶„ì—ì„œ ì¶”ì¶œ
            new_input_preview = current_input[-new_lookback:]  # ë’·ë¶€ë¶„ì—ì„œ
            new_output_preview = current_output[:new_horizon]  # ì•ë¶€ë¶„ì—ì„œ
            
            # ì²« ë²ˆì§¸ íŠ¹ì§•ë§Œ ì‹œê°í™”
            feature_idx = 0
            feature_name = feature_names[feature_idx] if feature_idx < len(feature_names) else f'Feature_{feature_idx}'
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig_input = go.Figure()
                fig_input.add_trace(go.Scatter(
                    y=current_input[:, feature_idx],
                    mode='lines+markers',
                    name=f'í˜„ì¬ ì…ë ¥ (ê¸¸ì´: {len(current_input)})',
                    line=dict(color='blue', width=2)
                ))
                fig_input.add_trace(go.Scatter(
                    y=new_input_preview[:, feature_idx],
                    mode='lines+markers',
                    name=f'ë³€ê²½ í›„ ì…ë ¥ (ê¸¸ì´: {len(new_input_preview)})',
                    line=dict(color='red', width=2, dash='dash')
                ))
                fig_input.update_layout(
                    title=f"ì…ë ¥ ì‹œí€€ìŠ¤ ë³€í™” ë¯¸ë¦¬ë³´ê¸°<br>{feature_name}",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_input, use_container_width=True)
            
            with col2:
                fig_output = go.Figure()
                fig_output.add_trace(go.Scatter(
                    y=current_output[:, feature_idx],
                    mode='lines+markers',
                    name=f'í˜„ì¬ ì¶œë ¥ (ê¸¸ì´: {len(current_output)})',
                    line=dict(color='green', width=2)
                ))
                fig_output.add_trace(go.Scatter(
                    y=new_output_preview[:, feature_idx],
                    mode='lines+markers',
                    name=f'ë³€ê²½ í›„ ì¶œë ¥ (ê¸¸ì´: {len(new_output_preview)})',
                    line=dict(color='orange', width=2, dash='dash')
                ))
                fig_output.update_layout(
                    title=f"ì¶œë ¥ ì‹œí€€ìŠ¤ ë³€í™” ë¯¸ë¦¬ë³´ê¸°<br>{feature_name}",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_output, use_container_width=True)
    
    # ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì ìš©
    if st.button("ğŸ¯ ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì ìš©", key="apply_sequence_reshape"):
        if new_lookback == current_input_shape[1] and new_horizon == current_output_shape[1]:
            st.warning("âš ï¸ ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            with st.spinner("ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì ìš© ì¤‘..."):
                try:
                    reshaped_dataset = {}
                    
                    # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ í¬ê¸° ì¡°ì •
                    # Lookback (ì…ë ¥): ë’·ë¶€ë¶„ì—ì„œ ì¶”ì¶œ
                    for data_type in ['train_inputs', 'val_inputs']:
                        current_data = current_dataset[data_type]
                        reshaped_dataset[data_type] = current_data[:, -new_lookback:, :]
                    
                    # Horizon (ì¶œë ¥): ì•ë¶€ë¶„ì—ì„œ ì¶”ì¶œ
                    for data_type in ['train_outputs', 'val_outputs']:
                        current_data = current_dataset[data_type]
                        reshaped_dataset[data_type] = current_data[:, :new_horizon, :]
                    
                    # ë©”íƒ€ë°ì´í„° ë³µì‚¬ ë° ì—…ë°ì´íŠ¸
                    if 'metadata' in current_dataset:
                        reshaped_dataset['metadata'] = current_dataset['metadata'].copy()
                        reshaped_dataset['metadata']['sequence_reshaped'] = True
                        reshaped_dataset['metadata']['reshape_info'] = {
                            'original_lookback': current_input_shape[1],
                            'original_horizon': current_output_shape[1],
                            'new_lookback': new_lookback,
                            'new_horizon': new_horizon,
                            'extraction_method': 'lookback_from_end_horizon_from_start',
                            'reshape_timestamp': datetime.now().isoformat()
                        }
                    
                    # ê¸°íƒ€ ì •ë³´ ë³µì‚¬
                    for key in current_dataset.keys():
                        if key not in reshaped_dataset and key not in ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs']:
                            reshaped_dataset[key] = current_dataset[key]
                    
                    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.reshaped_dataset = reshaped_dataset
                    
                    st.success("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                except Exception as e:
                    st.error(f"âŒ ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ë³€ê²½ ê²°ê³¼ í‘œì‹œ
    if hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None:
        st.markdown("---")
        st.subheader("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì™„ë£Œ")
        
        reshaped_dataset = st.session_state.reshaped_dataset
        
        # ë³€ê²½ í›„ í˜•íƒœ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“ í›ˆë ¨ ìƒ˜í”Œ", f"{len(reshaped_dataset['train_inputs']):,}")
        with col2:
            st.metric("ğŸ”¬ ê²€ì¦ ìƒ˜í”Œ", f"{len(reshaped_dataset['val_inputs']):,}")
        with col3:
            st.metric("ğŸ“ˆ ìƒˆ Lookback", reshaped_dataset['train_inputs'].shape[1])
        with col4:
            st.metric("ğŸ“Š ìƒˆ Horizon", reshaped_dataset['train_outputs'].shape[1])
        
        # ìƒì„¸ ì •ë³´
        with st.expander("ğŸ“‹ ë³€ê²½ëœ ë°ì´í„° ìƒì„¸ ì •ë³´"):
            info_df = pd.DataFrame({
                'ë°ì´í„° íƒ€ì…': ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs'],
                'ë³€ê²½ í›„ í˜•íƒœ': [
                    str(reshaped_dataset['train_inputs'].shape),
                    str(reshaped_dataset['train_outputs'].shape),
                    str(reshaped_dataset['val_inputs'].shape),
                    str(reshaped_dataset['val_outputs'].shape)
                ]
            })
            st.dataframe(info_df, use_container_width=True, hide_index=True)
            
            if 'reshape_info' in reshaped_dataset.get('metadata', {}):
                reshape_info = reshaped_dataset['metadata']['reshape_info']
                st.json(reshape_info)

def tab_feature_selection():
    """íƒ­5: ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •"""
    st.header("ğŸ¯ ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •")
    
    # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²°ì • (ìš°ì„ ìˆœìœ„: reshaped > shifted > normalized > original)
    if hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None:
        current_dataset = st.session_state.reshaped_dataset
        dataset_name = "ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •ëœ ë°ì´í„°ì…‹"
    elif st.session_state.shifted_dataset is not None:
        current_dataset = st.session_state.shifted_dataset
        dataset_name = "ì‹œí”„íŠ¸ ì ìš©ëœ ë°ì´í„°ì…‹"
    elif st.session_state.normalized_dataset is not None:
        current_dataset = st.session_state.normalized_dataset
        dataset_name = "ì •ê·œí™”ëœ ë°ì´í„°ì…‹"
    elif st.session_state.dataset is not None:
        current_dataset = st.session_state.dataset
        dataset_name = "ì›ë³¸ ë°ì´í„°ì…‹"
    else:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    feature_names = st.session_state.feature_names
    st.markdown(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°: **{dataset_name}**")
    st.markdown("ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ì…ë ¥ íŠ¹ì§•ê³¼ ì¶œë ¥ íŠ¹ì§•ì„ ì„ íƒí•˜ì„¸ìš”.")
    
    # íŠ¹ì§• ì„ íƒ ì¸í„°í˜ì´ìŠ¤
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“¥ ì…ë ¥ íŠ¹ì§• ì„ íƒ")
        input_features = st.multiselect(
            "ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  íŠ¹ì§•ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
            range(len(feature_names)),
            default=list(range(min(5, len(feature_names)))),  # ê¸°ë³¸ê°’: ì²˜ìŒ 5ê°œ íŠ¹ì§•
            format_func=lambda x: f"{x}: {feature_names[x]}",
            key="input_features_select"
        )
        
        if input_features:
            st.write(f"**ì„ íƒëœ ì…ë ¥ íŠ¹ì§• ìˆ˜:** {len(input_features)}")
            input_feature_names = [feature_names[i] for i in input_features]
            for i, name in enumerate(input_feature_names):
                st.write(f"  {i+1}. {name}")
    
    with col2:
        st.subheader("ğŸ“¤ ì¶œë ¥ íŠ¹ì§• ì„ íƒ")
        output_features = st.multiselect(
            "ëª¨ë¸ ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•  íŠ¹ì§•ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
            range(len(feature_names)),
            default=list(range(min(3, len(feature_names)))),  # ê¸°ë³¸ê°’: ì²˜ìŒ 3ê°œ íŠ¹ì§•
            format_func=lambda x: f"{x}: {feature_names[x]}",
            key="output_features_select"
        )
        
        if output_features:
            st.write(f"**ì„ íƒëœ ì¶œë ¥ íŠ¹ì§• ìˆ˜:** {len(output_features)}")
            output_feature_names = [feature_names[i] for i in output_features]
            for i, name in enumerate(output_feature_names):
                st.write(f"  {i+1}. {name}")
    
    # íŠ¹ì§• ì„ íƒ ìœ íš¨ì„± ê²€ì‚¬
    if not input_features:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒì˜ ì…ë ¥ íŠ¹ì§•ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        return
    
    if not output_features:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒì˜ ì¶œë ¥ íŠ¹ì§•ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        return
    
    # ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¬êµ¬ì„±
    if st.button("ğŸ¯ ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¬êµ¬ì„±", key="reconstruct_dataset"):
        with st.spinner("ë°ì´í„°ì…‹ ì¬êµ¬ì„± ì¤‘..."):
            try:
                selected_dataset = select_features_from_dataset(
                    current_dataset, input_features, output_features
                )
                
                # ì„ íƒëœ íŠ¹ì§• ì´ë¦„ë“¤ ì €ì¥
                selected_input_names = [feature_names[i] for i in input_features]
                selected_output_names = [feature_names[i] for i in output_features]
                
                selected_dataset['selected_input_names'] = selected_input_names
                selected_dataset['selected_output_names'] = selected_output_names
                
                st.session_state.selected_features = selected_dataset
                st.success("âœ… ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ì´ ì¬êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
            except Exception as e:
                st.error(f"âŒ ë°ì´í„°ì…‹ ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ì¬êµ¬ì„±ëœ ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
    if st.session_state.selected_features is not None:
        st.markdown("---")
        st.subheader("âœ… ì¬êµ¬ì„±ëœ ë°ì´í„°ì…‹ ì •ë³´")
        
        selected_dataset = st.session_state.selected_features
        
        # ê¸°ë³¸ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“ í›ˆë ¨ ìƒ˜í”Œ", f"{len(selected_dataset['train_inputs']):,}")
        with col2:
            st.metric("ğŸ”¬ ê²€ì¦ ìƒ˜í”Œ", f"{len(selected_dataset['val_inputs']):,}")
        with col3:
            st.metric("ğŸ“¥ ì…ë ¥ íŠ¹ì§•", len(input_features))
        with col4:
            st.metric("ğŸ“¤ ì¶œë ¥ íŠ¹ì§•", len(output_features))
        
        # í˜•íƒœ ì •ë³´
        with st.expander("ğŸ“‹ ì¬êµ¬ì„±ëœ ë°ì´í„° í˜•íƒœ"):
            col1, col2 = st.columns(2)
            with col1:
                st.write("**í›ˆë ¨ ë°ì´í„°:**")
                st.write(f"- ì…ë ¥ í˜•íƒœ: {selected_dataset['train_inputs'].shape}")
                st.write(f"- ì¶œë ¥ í˜•íƒœ: {selected_dataset['train_outputs'].shape}")
            with col2:
                st.write("**ê²€ì¦ ë°ì´í„°:**")
                st.write(f"- ì…ë ¥ í˜•íƒœ: {selected_dataset['val_inputs'].shape}")
                st.write(f"- ì¶œë ¥ í˜•íƒœ: {selected_dataset['val_outputs'].shape}")
        
        # ì„ íƒëœ íŠ¹ì§• ìš”ì•½
        with st.expander("ğŸ·ï¸ ì„ íƒëœ íŠ¹ì§• ìš”ì•½"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ì…ë ¥ íŠ¹ì§•:**")
                input_df = pd.DataFrame({
                    'ì¸ë±ìŠ¤': input_features,
                    'íŠ¹ì§•ëª…': selected_dataset['selected_input_names']
                })
                st.dataframe(input_df, use_container_width=True, hide_index=True)
            
            with col2:
                st.markdown("**ì¶œë ¥ íŠ¹ì§•:**")
                output_df = pd.DataFrame({
                    'ì¸ë±ìŠ¤': output_features,
                    'íŠ¹ì§•ëª…': selected_dataset['selected_output_names']
                })
                st.dataframe(output_df, use_container_width=True, hide_index=True)
        
        # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ‘€ ì¬êµ¬ì„±ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
            sample_idx = st.selectbox(
                "ë¯¸ë¦¬ë³¼ ìƒ˜í”Œ ì„ íƒ",
                range(min(5, len(selected_dataset['train_inputs']))),
                key="final_sample_preview"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ì…ë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)**")
                fig_input = go.Figure()
                input_sample = selected_dataset['train_inputs'][sample_idx]
                
                for i, feature_name in enumerate(selected_dataset['selected_input_names']):
                    fig_input.add_trace(go.Scatter(
                        y=input_sample[:, i],
                        mode='lines+markers',
                        name=feature_name,
                        line=dict(width=2)
                    ))
                
                fig_input.update_layout(
                    title="ì…ë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_input, use_container_width=True)
            
            with col2:
                st.markdown("**ì¶œë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)**")
                fig_output = go.Figure()
                output_sample = selected_dataset['train_outputs'][sample_idx]
                
                for i, feature_name in enumerate(selected_dataset['selected_output_names']):
                    fig_output.add_trace(go.Scatter(
                        y=output_sample[:, i],
                        mode='lines+markers',
                        name=feature_name,
                        line=dict(width=2)
                    ))
                
                fig_output.update_layout(
                    title="ì¶œë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_output, use_container_width=True)
        
        # ë°ì´í„°ì…‹ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ
        st.subheader("ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥")
        
        dataset_filename = st.text_input(
            "ì €ì¥í•  íŒŒì¼ëª…",
            value="processed_dataset",
            key="final_dataset_filename"
        )
        
        if st.button("ğŸ“¦ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±", key="generate_final_dataset"):
            try:
                with st.spinner("ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì¤‘..."):
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    final_metadata = selected_dataset.get('metadata', {}).copy()
                    final_metadata.update({
                        'processing_complete': True,
                        'final_input_features': input_features,
                        'final_output_features': output_features,
                        'final_input_feature_names': selected_dataset['selected_input_names'],
                        'final_output_feature_names': selected_dataset['selected_output_names'],
                        'processing_timestamp': datetime.now().isoformat(),
                        'final_shapes': {
                            'train_inputs': selected_dataset['train_inputs'].shape,
                            'train_outputs': selected_dataset['train_outputs'].shape,
                            'val_inputs': selected_dataset['val_inputs'].shape,
                            'val_outputs': selected_dataset['val_outputs'].shape
                        }
                    })
                    
                    # ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
                    final_dataset = {
                        'train_inputs': selected_dataset['train_inputs'],
                        'train_outputs': selected_dataset['train_outputs'],
                        'val_inputs': selected_dataset['val_inputs'],
                        'val_outputs': selected_dataset['val_outputs'],
                        'metadata': final_metadata,
                        'input_feature_names': selected_dataset['selected_input_names'],
                        'output_feature_names': selected_dataset['selected_output_names']
                    }
                    
                    # NPY í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì¤€ë¹„
                    import io
                    buffer = io.BytesIO()
                    np.save(buffer, final_dataset)
                    dataset_data = buffer.getvalue()
                
                st.download_button(
                    label="ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ",
                    data=dataset_data,
                    file_name=f"{dataset_filename}.npy",
                    mime="application/octet-stream",
                    help="ì²˜ë¦¬ëœ ìµœì¢… ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                )
                
                st.success("âœ… ìµœì¢… ë°ì´í„°ì…‹ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                
                # ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ
                with st.expander("ğŸ Python ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ"):
                    st.code(f"""
import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader

# ìµœì¢… ë°ì´í„°ì…‹ ë¡œë“œ
dataset = np.load('{dataset_filename}.npy', allow_pickle=True).item()

# ë°ì´í„° ì ‘ê·¼
train_inputs = dataset['train_inputs']    # Shape: {selected_dataset['train_inputs'].shape}
train_outputs = dataset['train_outputs']  # Shape: {selected_dataset['train_outputs'].shape}
val_inputs = dataset['val_inputs']        # Shape: {selected_dataset['val_inputs'].shape}
val_outputs = dataset['val_outputs']      # Shape: {selected_dataset['val_outputs'].shape}

# íŠ¹ì§• ì´ë¦„ í™•ì¸
input_features = dataset['input_feature_names']   # {selected_dataset['selected_input_names']}
output_features = dataset['output_feature_names'] # {selected_dataset['selected_output_names']}

# ë©”íƒ€ë°ì´í„° í™•ì¸
metadata = dataset['metadata']
print("ì²˜ë¦¬ ì™„ë£Œ:", metadata['processing_complete'])
print("ì„ íƒëœ ì…ë ¥ íŠ¹ì§•:", metadata['final_input_feature_names'])
print("ì„ íƒëœ ì¶œë ¥ íŠ¹ì§•:", metadata['final_output_feature_names'])

# PyTorch ë°ì´í„°ë¡œë” ìƒì„± ì˜ˆì‹œ
train_dataset = TensorDataset(
    torch.FloatTensor(train_inputs),
    torch.FloatTensor(train_outputs)
)
val_dataset = TensorDataset(
    torch.FloatTensor(val_inputs),
    torch.FloatTensor(val_outputs)
)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print(f"í›ˆë ¨ ë°ì´í„°: {{len(train_dataset):,}}ê°œ ìƒ˜í”Œ")
print(f"ê²€ì¦ ë°ì´í„°: {{len(val_dataset):,}}ê°œ ìƒ˜í”Œ")
print(f"ì…ë ¥ ì°¨ì›: {{train_inputs.shape[1:]}}")
print(f"ì¶œë ¥ ì°¨ì›: {{train_outputs.shape[1:]}}")
                    """, language="python")
                
            except Exception as e:
                st.error(f"âŒ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

# =================================================================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# =================================================================================
def main():
    # í˜ì´ì§€ ì„¤ì • ë° ì´ˆê¸°í™”
    st.set_page_config(
        page_title="ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ì•±", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    setup_korean_font()
    initialize_session_state()
    
    # ë©”ì¸ íƒ€ì´í‹€
    st.title("ğŸ”¬ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ë„êµ¬")
    st.markdown("ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì „ì²˜ë¦¬ë¶€í„° ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ê¹Œì§€")
    
    # íƒ­ ìƒì„±
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“ ë°ì´í„° ì…ë ¥", 
        "ğŸ“ ì •ê·œí™”", 
        "â†”ï¸ íŠ¹ì§• ì‹œí”„íŠ¸", 
        "ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •",
        "ğŸ¯ ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •"
    ])
    
    # ê° íƒ­ ì‹¤í–‰
    with tab1:
        tab_data_input()
    
    with tab2:
        tab_normalization()
    
    with tab3:
        tab_feature_shift()
    
    with tab4:
        tab_sequence_reshape()
    
    with tab5:
        tab_feature_selection()
    
    # ì‚¬ì´ë“œë°” ì •ë³´
    with st.sidebar:
        st.header("ğŸ“Š í˜„ì¬ ìƒíƒœ")
        
        # ë°ì´í„° ë¡œë“œ ìƒíƒœ
        if st.session_state.dataset is not None:
            st.success("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        else:
            st.error("âŒ ë°ì´í„° ë¯¸ë¡œë“œ")
        
        # ì •ê·œí™” ìƒíƒœ
        if st.session_state.normalized_dataset is not None:
            st.success("âœ… ì •ê·œí™” ì™„ë£Œ")
        else:
            st.warning("âš ï¸ ì •ê·œí™” ë¯¸ì™„ë£Œ")
        
        # ì‹œí”„íŠ¸ ìƒíƒœ
        if st.session_state.shifted_dataset is not None:
            st.success("âœ… ì‹œí”„íŠ¸ ì ìš©ë¨")
        else:
            st.info("â„¹ï¸ ì‹œí”„íŠ¸ ë¯¸ì ìš©")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • ìƒíƒœ
        if hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None:
            st.success("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • ì™„ë£Œ")
        else:
            st.info("â„¹ï¸ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • ë¯¸ì ìš©")
        
        # íŠ¹ì§• ì„ íƒ ìƒíƒœ
        if st.session_state.selected_features is not None:
            st.success("âœ… íŠ¹ì§• ì„ íƒ ì™„ë£Œ")
        else:
            st.warning("âš ï¸ íŠ¹ì§• ì„ íƒ ë¯¸ì™„ë£Œ")
        
        st.markdown("---")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_steps = [
            ("ë°ì´í„° ë¡œë“œ", st.session_state.dataset is not None),
            ("ì •ê·œí™”", st.session_state.normalized_dataset is not None),
            ("ì‹œí”„íŠ¸ ì ìš©", st.session_state.shifted_dataset is not None),
            ("ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •", hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None),
            ("íŠ¹ì§• ì„ íƒ", st.session_state.selected_features is not None)
        ]
        
        completed_steps = sum(1 for _, completed in progress_steps if completed)
        progress = completed_steps / len(progress_steps)
        
        st.subheader("ğŸ“ˆ ì§„í–‰ë¥ ")
        st.progress(progress)
        st.write(f"{completed_steps}/{len(progress_steps)} ë‹¨ê³„ ì™„ë£Œ ({progress:.0%})")
        
        # ê° ë‹¨ê³„ë³„ ìƒíƒœ
        for step_name, completed in progress_steps:
            if completed:
                st.write(f"âœ… {step_name}")
            else:
                st.write(f"â­• {step_name}")
        
        # ì‚¬ìš© ê°€ì´ë“œ
        st.markdown("---")
        with st.expander("ğŸ“– ì‚¬ìš© ê°€ì´ë“œ"):
            st.markdown("""
            **1ë‹¨ê³„: ë°ì´í„° ì…ë ¥**
            - NPY í˜•ì‹ì˜ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì—…ë¡œë“œ
            - ë°ì´í„° êµ¬ì¡° ë° íŠ¹ì§• í™•ì¸
            
            **2ë‹¨ê³„: ì •ê·œí™”** 
            - Min-Max ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚°
            - í•„ìš”ì‹œ íŒŒë¼ë¯¸í„° ìˆ˜ë™ ì¡°ì •
            - ëª¨ë“  ë°ì´í„°ì— ì •ê·œí™” ì ìš©
            
            **3ë‹¨ê³„: íŠ¹ì§• ì‹œí”„íŠ¸**
            - íŠ¹ì • íŠ¹ì§•ì„ ì‹œê°„ì¶•ì—ì„œ ì´ë™
            - ì–‘ìˆ˜: ì§€ì—°, ìŒìˆ˜: ì•ë‹¹ê¹€
            - ì—¬ëŸ¬ íŠ¹ì§•ì— ìˆœì°¨ì  ì ìš© ê°€ëŠ¥
            
            **4ë‹¨ê³„: ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •**
            - Lookbackê³¼ Horizon ê¸¸ì´ ì‚¬ìš©ì ì •ì˜
            - ë°ì´í„° ì¶”ì¶œ ë°©ë²• ì„ íƒ ê°€ëŠ¥
            - ì‹œí€€ìŠ¤ ë³€í™” ë¯¸ë¦¬ë³´ê¸° ì œê³µ
            
            **5ë‹¨ê³„: ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •**
            - ëª¨ë¸ ì…ë ¥/ì¶œë ¥ íŠ¹ì§• ì„ íƒ
            - ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
            - PyTorch ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ ì œê³µ
            """)
        
        # í’‹ë…¸íŠ¸
        st.markdown(
            """
            <style>
            .bottom-info {
                position: fixed;
                bottom: 0;
                left: 0;
                width: 21rem;
                max-width: 21rem;
                background-color: var(--background-color);
                padding: 1rem;
                border-top: 1px solid var(--border-color);
                z-index: 999;
                box-sizing: border-box;
            }
            .bottom-info hr {
                margin: 0.2rem 0;
                border-color: var(--text-color-light);
                width: 100%;
            }
            </style>
            <div class="bottom-info">
                <hr>
                ğŸ§  <strong>íšŒì‚¬ëª…:</strong> ãˆœíŒŒì‹œë””ì—˜<br>
                ğŸ« <strong>ì—°êµ¬ì‹¤:</strong> visLAB@PNU<br>
                ğŸ‘¨â€ğŸ’» <strong>ì œì‘ì:</strong> (C)Dong2<br>
                ğŸ› ï¸ <strong>ë²„ì „:</strong> V.2.0 (06-12-2025)<br>
                <hr>
            </div>
            """, 
            unsafe_allow_html=True
        )

if __name__ == "__main__":
    main()

