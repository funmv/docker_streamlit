import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
from datetime import datetime
import json

# =================================================================================
# í•œê¸€ í°íŠ¸ ì„¤ì •
# =================================================================================
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì • í•¨ìˆ˜"""
    try:
        from matplotlib import font_manager, rc
        # Windows í™˜ê²½
        font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
        rc('font', family=font_name)
        plt.rcParams['axes.unicode_minus'] = False
    except:
        try:
            # Linux í™˜ê²½
            from matplotlib import font_manager, rc
            font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
            font_name = font_manager.FontProperties(fname=font_path).get_name()
            rc('font', family=font_name)  
            plt.rcParams['axes.unicode_minus'] = False
        except:
            # í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
            plt.rcParams['axes.unicode_minus'] = False

# matplotlib ê²½ê³  ì œê±°ë¥¼ ìœ„í•œ ì„¤ì •
plt.rcParams['figure.max_open_warning'] = 50

# =================================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =================================================================================
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'dataset' not in st.session_state:
        st.session_state.dataset = None
    if 'metadata' not in st.session_state:
        st.session_state.metadata = None
    if 'feature_names' not in st.session_state:
        st.session_state.feature_names = []
    if 'normalization_params' not in st.session_state:
        st.session_state.normalization_params = None
    if 'normalized_dataset' not in st.session_state:
        st.session_state.normalized_dataset = None
    if 'shifted_dataset' not in st.session_state:
        st.session_state.shifted_dataset = None
    if 'reshaped_dataset' not in st.session_state:
        st.session_state.reshaped_dataset = None
    if 'selected_features' not in st.session_state:
        st.session_state.selected_features = None

def load_npy_dataset(uploaded_file):
    """NPY íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
    try:
        # NPY íŒŒì¼ ë¡œë“œ
        dataset = np.load(uploaded_file, allow_pickle=True).item()
        
        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        required_keys = ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs', 'metadata']
        missing_keys = [key for key in required_keys if key not in dataset]
        
        if missing_keys:
            st.error(f"âŒ ë°ì´í„°ì…‹ì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {missing_keys}")
            return None, None
        
        # íŠ¹ì§• ì´ë¦„ ì¶”ì¶œ
        metadata = dataset['metadata']
        feature_names = []
        
        if 'feature_info' in metadata:
            time_feature_names = metadata['feature_info'].get('time_feature_names', [])
            data_feature_names = metadata['feature_info'].get('data_feature_names', [])
            feature_names = time_feature_names + data_feature_names
        
        # íŠ¹ì§• ì´ë¦„ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
        if not feature_names and len(dataset['train_inputs']) > 0:
            total_features = dataset['train_inputs'].shape[2]
            feature_names = [f"Feature_{i+1}" for i in range(total_features)]
        
        return dataset, feature_names
        
    except Exception as e:
        st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None

def display_dataset_info(dataset, feature_names):
    """ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ"""
    st.subheader("ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´")
    
    # ê¸°ë³¸ ì •ë³´
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“ í›ˆë ¨ ìƒ˜í”Œ", f"{len(dataset['train_inputs']):,}")
    with col2:
        st.metric("ğŸ”¬ ê²€ì¦ ìƒ˜í”Œ", f"{len(dataset['val_inputs']):,}")
    with col3:
        st.metric("ğŸ“ˆ íŠ¹ì§• ìˆ˜", len(feature_names))
    with col4:
        st.metric("ğŸ“Š ì´ ìƒ˜í”Œ", f"{len(dataset['train_inputs']) + len(dataset['val_inputs']):,}")
    
    # í˜•íƒœ ì •ë³´
    with st.expander("ğŸ“‹ ë°ì´í„° í˜•íƒœ ì •ë³´"):
        col1, col2 = st.columns(2)
        with col1:
            st.write("**í›ˆë ¨ ë°ì´í„°:**")
            st.write(f"- ì…ë ¥ í˜•íƒœ: {dataset['train_inputs'].shape}")
            st.write(f"- ì¶œë ¥ í˜•íƒœ: {dataset['train_outputs'].shape}")
        with col2:
            st.write("**ê²€ì¦ ë°ì´í„°:**")
            st.write(f"- ì…ë ¥ í˜•íƒœ: {dataset['val_inputs'].shape}")
            st.write(f"- ì¶œë ¥ í˜•íƒœ: {dataset['val_outputs'].shape}")
    
    # íŠ¹ì§• ì´ë¦„
    with st.expander("ğŸ·ï¸ íŠ¹ì§• ì´ë¦„ ëª©ë¡"):
        feature_df = pd.DataFrame({
            'ì¸ë±ìŠ¤': range(len(feature_names)),
            'íŠ¹ì§•ëª…': feature_names
        })
        st.dataframe(feature_df, use_container_width=True, hide_index=True)
    
    # ë©”íƒ€ë°ì´í„° ì •ë³´
    if 'metadata' in dataset:
        with st.expander("ğŸ“‹ ë©”íƒ€ë°ì´í„° ì •ë³´"):
            metadata = dataset['metadata']
            st.json(metadata, expanded=False)

def calculate_normalization_params(train_inputs):
    """ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚° (Min-Max ì •ê·œí™”)"""
    num_features = train_inputs.shape[2]
    params = {}
    
    for i in range(num_features):
        feature_data = train_inputs[:, :, i].flatten()
        params[i] = {
            'min': float(np.min(feature_data)),
            'max': float(np.max(feature_data)),
            'range': float(np.max(feature_data) - np.min(feature_data))
        }
    
    return params

def apply_normalization(data, normalization_params):
    """Min-Max ì •ê·œí™” ì ìš©"""
    normalized_data = data.copy()
    
    for i, params in normalization_params.items():
        if params['range'] > 0:  # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€
            normalized_data[:, :, i] = (data[:, :, i] - params['min']) / params['range']
        else:
            normalized_data[:, :, i] = 0  # ìƒìˆ˜ì¸ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
    
    return normalized_data

def apply_feature_shift(data, feature_idx, shift_amount):
    """íŠ¹ì • íŠ¹ì§•ì— ì‹œí”„íŠ¸ ì ìš©"""
    shifted_data = data.copy()
    
    if shift_amount > 0:  # ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì‹œí”„íŠ¸ (ì§€ì—°)
        shifted_data[:, shift_amount:, feature_idx] = data[:, :-shift_amount, feature_idx]
        shifted_data[:, :shift_amount, feature_idx] = data[:, 0:1, feature_idx]  # ì²« ë²ˆì§¸ ê°’ìœ¼ë¡œ íŒ¨ë”©
    elif shift_amount < 0:  # ì™¼ìª½ìœ¼ë¡œ ì‹œí”„íŠ¸ (ì•ë‹¹ê¹€)
        shift_amount = abs(shift_amount)
        shifted_data[:, :-shift_amount, feature_idx] = data[:, shift_amount:, feature_idx]
        shifted_data[:, -shift_amount:, feature_idx] = data[:, -1:, feature_idx]  # ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ íŒ¨ë”©
    
    return shifted_data

def select_features_from_dataset(dataset, input_features, output_features):
    """ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¬êµ¬ì„±"""
    selected_dataset = {}
    
    # ì…ë ¥ íŠ¹ì§• ì„ íƒ
    selected_dataset['train_inputs'] = dataset['train_inputs'][:, :, input_features]
    selected_dataset['val_inputs'] = dataset['val_inputs'][:, :, input_features]
    
    # ì¶œë ¥ íŠ¹ì§• ì„ íƒ
    selected_dataset['train_outputs'] = dataset['train_outputs'][:, :, output_features]
    selected_dataset['val_outputs'] = dataset['val_outputs'][:, :, output_features]
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    if 'metadata' in dataset:
        selected_dataset['metadata'] = dataset['metadata'].copy()
        selected_dataset['metadata']['selected_input_features'] = input_features
        selected_dataset['metadata']['selected_output_features'] = output_features
    
    return selected_dataset

# =================================================================================
# íƒ­ë³„ ë©”ì¸ í•¨ìˆ˜ë“¤
# =================================================================================
def tab_data_input():
    """íƒ­1: ë°ì´í„° ì…ë ¥"""
    st.header("ğŸ“ ë°ì´í„° ì…ë ¥")
    st.markdown("NPY í˜•ì‹ì˜ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    
    uploaded_file = st.file_uploader(
        "NPY íŒŒì¼ ì„ íƒ",
        type=['npy'],
        help="ë”¥ëŸ¬ë‹ í•™ìŠµìš©ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ NPY íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
    )
    
    if uploaded_file is not None:
        with st.spinner("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘..."):
            dataset, feature_names = load_npy_dataset(uploaded_file)
            
            if dataset is not None:
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.dataset = dataset
                st.session_state.feature_names = feature_names
                st.session_state.metadata = dataset.get('metadata', {})
                
                st.success("âœ… ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
                display_dataset_info(dataset, feature_names)
                
                # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ğŸ‘€ ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                    if len(dataset['train_inputs']) > 0:
                        # ìƒ˜í”Œ ì„ íƒ ë°©ë²• ì„ íƒ
                        col_method, col_select = st.columns([1, 2])
                        
                        with col_method:
                            selection_method = st.radio(
                                "ìƒ˜í”Œ ì„ íƒ ë°©ë²•",
                                ["ëª©ë¡ì—ì„œ ì„ íƒ", "ì§ì ‘ ì…ë ¥"],
                                key="sample_selection_method"
                            )
                        
                        with col_select:
                            if selection_method == "ëª©ë¡ì—ì„œ ì„ íƒ":
                                sample_idx = st.selectbox(
                                    "ë¯¸ë¦¬ë³¼ ìƒ˜í”Œ ì„ íƒ",
                                    range(min(10, len(dataset['train_inputs']))),
                                    key="sample_preview_select"
                                )
                            else:
                                max_sample = len(dataset['train_inputs']) - 1
                                sample_idx = st.number_input(
                                    f"ìƒ˜í”Œ ë²ˆí˜¸ ì…ë ¥ (0~{max_sample})",
                                    min_value=0,
                                    max_value=max_sample,
                                    value=0,
                                    key="sample_preview_input"
                                )
                        
                        # ì‹œê°í™”í•  íŠ¹ì§• ì„ íƒ
                        max_features = min(5, len(feature_names))
                        selected_features = st.multiselect(
                            f"ì‹œê°í™”í•  íŠ¹ì§• ì„ íƒ (ìµœëŒ€ {max_features}ê°œ)",
                            range(len(feature_names)),
                            default=list(range(max_features)),
                            format_func=lambda x: f"{x}: {feature_names[x]}",
                            key="preview_features"
                        )
                        
                        if selected_features:
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**ì…ë ¥ ì‹œí€€ìŠ¤**")
                                fig_input = go.Figure()
                                input_sample = dataset['train_inputs'][sample_idx]
                                
                                for feature_idx in selected_features[:max_features]:
                                    fig_input.add_trace(go.Scatter(
                                        y=input_sample[:, feature_idx],
                                        mode='lines+markers',
                                        name=feature_names[feature_idx],
                                        line=dict(width=2)
                                    ))
                                
                                fig_input.update_layout(
                                    title="ì…ë ¥ ì‹œí€€ìŠ¤",
                                    xaxis_title="Time Steps",
                                    yaxis_title="Values",
                                    height=300
                                )
                                st.plotly_chart(fig_input, use_container_width=True)
                            
                            with col2:
                                st.markdown("**ì¶œë ¥ ì‹œí€€ìŠ¤**")
                                fig_output = go.Figure()
                                output_sample = dataset['train_outputs'][sample_idx]
                                
                                for feature_idx in selected_features[:max_features]:
                                    fig_output.add_trace(go.Scatter(
                                        y=output_sample[:, feature_idx],
                                        mode='lines+markers',
                                        name=feature_names[feature_idx],
                                        line=dict(width=2)
                                    ))
                                
                                fig_output.update_layout(
                                    title="ì¶œë ¥ ì‹œí€€ìŠ¤",
                                    xaxis_title="Time Steps",
                                    yaxis_title="Values",
                                    height=300
                                )
                                st.plotly_chart(fig_output, use_container_width=True)




def tab_normalization():
    """íƒ­2: ë°ì´í„° ì •ê·œí™” - íŠ¹ì§• ë§¤ì¹­ ê¸°ëŠ¥ í¬í•¨"""
    import pandas as pd  # pandas import ì¶”ê°€
    import json  # json import ì¶”ê°€
    
    st.header("ğŸ“ ë°ì´í„° ì •ê·œí™”")
    
    if st.session_state.dataset is None:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    dataset = st.session_state.dataset
    feature_names = st.session_state.feature_names
    
    st.markdown("Min-Max ì •ê·œí™”ë¥¼ í†µí•´ ëª¨ë“  íŠ¹ì§•ì„ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.")
    
    # ì •ê·œí™” íŒŒë¼ë¯¸í„° ìë™ ê³„ì‚°
    if st.session_state.normalization_params is None:
        with st.spinner("ì •ê·œí™” íŒŒë¼ë¯¸í„° ìë™ ê³„ì‚° ì¤‘..."):
            norm_params = calculate_normalization_params(dataset['train_inputs'])
            
            # React ì»´í¬ë„ŒíŠ¸ ë¡œì§ì„ ë”°ë¼ ì¡°ì •ëœ ë²”ìœ„ ê³„ì‚°
            adjusted_params = {}
            for i, params in norm_params.items():
                feature_name = feature_names[i] if i < len(feature_names) else f'Feature_{i}'
                current_min = params['min']
                current_max = params['max']
                
                # ì¡°ì •ëœ ë²”ìœ„ ê³„ì‚° (React ì»´í¬ë„ŒíŠ¸ ë¡œì§ ì ìš©)
                adjusted_min, adjusted_max = calculate_adjusted_range(feature_name, current_min, current_max)
                
                adjusted_params[i] = {
                    'min': adjusted_min,
                    'max': adjusted_max,
                    'range': adjusted_max - adjusted_min,
                    'original_min': current_min,
                    'original_max': current_max,
                    'source': 'auto_calculated'  # ìë™ ê³„ì‚°ëœ íŒŒë¼ë¯¸í„° í‘œì‹œ
                }
            
            st.session_state.normalization_params = adjusted_params
            st.success("âœ… ì •ê·œí™” íŒŒë¼ë¯¸í„°ê°€ ìë™ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì •ê·œí™” íŒŒë¼ë¯¸í„° í‘œì‹œ ë° í¸ì§‘ ì¸í„°í˜ì´ìŠ¤
    if st.session_state.normalization_params is not None:
        st.subheader("ğŸ“Š Min-Max ì •ê·œí™” ë²”ìœ„ ê³„ì‚°ê¸°")
        st.markdown("ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœì†Œ/ìµœëŒ€ê°’ì„ ê³„ì‚°í•˜ê³  ì¡°ì •í•©ë‹ˆë‹¤.")
        
        norm_params = st.session_state.normalization_params.copy()
        
        # ì»¨íŠ¸ë¡¤ ë²„íŠ¼ë“¤
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ“ JSON ë‹¤ìš´ë¡œë“œ", key="download_json"):
                # JSON í˜•ì‹ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì¤€ë¹„
                json_data = {}
                for i, params in norm_params.items():
                    feature_name = feature_names[i] if i < len(feature_names) else f'Feature_{i}'
                    json_data[feature_name] = {
                        'min': params['min'],
                        'max': params['max']
                    }
                
                json_str = json.dumps(json_data, indent=2, ensure_ascii=False)
                st.download_button(
                    label="ğŸ’¾ JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=json_str,
                    file_name="minmax_ranges.json",
                    mime="application/json"
                )
        
        with col2:
            if st.button("ğŸ“Š CSV ë‹¤ìš´ë¡œë“œ", key="download_csv"):
                # CSV í˜•ì‹ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì¤€ë¹„
                csv_data = "feature_name,min_value,max_value\n"
                for i, params in norm_params.items():
                    feature_name = feature_names[i] if i < len(feature_names) else f'Feature_{i}'
                    csv_data += f"{feature_name},{params['min']},{params['max']}\n"
                
                st.download_button(
                    label="ğŸ’¾ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=csv_data,
                    file_name="minmax_ranges.csv",
                    mime="text/csv"
                )
        
        with col3:
            uploaded_param_file = st.file_uploader(
                "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ",
                type=['json', 'csv'],
                help="ì´ì „ì— ì €ì¥í•œ ì •ê·œí™” íŒŒë¼ë¯¸í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
                key="param_file_upload"
            )
        
        # íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ (íŠ¹ì§• ë§¤ì¹­ ê¸°ëŠ¥ í¬í•¨)
        if uploaded_param_file is not None:
            try:
                uploaded_params = {}
                uploaded_feature_names = []
                
                if uploaded_param_file.name.endswith('.json'):
                    param_data = json.load(uploaded_param_file)
                    uploaded_feature_names = list(param_data.keys())
                    
                elif uploaded_param_file.name.endswith('.csv'):
                    param_df = pd.read_csv(uploaded_param_file)
                    uploaded_feature_names = param_df['feature_name'].tolist()
                    param_data = {}
                    for _, row in param_df.iterrows():
                        param_data[row['feature_name']] = {
                            'min': row['min_value'],
                            'max': row['max_value']
                        }
                
                # íŠ¹ì§• ë§¤ì¹­ ë¶„ì„
                current_features = set(feature_names)
                uploaded_features = set(uploaded_feature_names)
                
                matched_features = current_features & uploaded_features
                current_only_features = current_features - uploaded_features
                uploaded_only_features = uploaded_features - current_features
                
                # ë§¤ì¹­ ê²°ê³¼ í‘œì‹œ
                st.markdown("---")
                st.subheader("ğŸ” íŠ¹ì§• ë§¤ì¹­ ë¶„ì„")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("âœ… ë§¤ì¹­ëœ íŠ¹ì§•", len(matched_features))
                    if matched_features:
                        with st.expander("ë§¤ì¹­ëœ íŠ¹ì§• ëª©ë¡"):
                            for feature in sorted(matched_features):
                                st.write(f"â€¢ {feature}")
                
                with col2:
                    st.metric("ğŸ†• í˜„ì¬ ë°ì´í„°ë§Œ ìˆëŠ” íŠ¹ì§•", len(current_only_features))
                    if current_only_features:
                        with st.expander("ìƒˆë¡œìš´ íŠ¹ì§• ëª©ë¡"):
                            for feature in sorted(current_only_features):
                                st.write(f"â€¢ {feature}")
                
                with col3:
                    st.metric("ğŸ—‘ï¸ íŒŒì¼ì—ë§Œ ìˆëŠ” íŠ¹ì§•", len(uploaded_only_features))
                    if uploaded_only_features:
                        with st.expander("ì œê±°ë  íŠ¹ì§• ëª©ë¡"):
                            for feature in sorted(uploaded_only_features):
                                st.write(f"â€¢ {feature}")
                
                # ë§¤ì¹­ ì²˜ë¦¬ ì˜µì…˜
                st.markdown("**ë§¤ì¹­ ì²˜ë¦¬ ë°©ë²•:**")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    merge_strategy = st.radio(
                        "ë§¤ì¹­ ì „ëµ ì„ íƒ",
                        [
                            "ìë™ ë³‘í•© (ë§¤ì¹­ëœ íŠ¹ì§•ë§Œ ì‚¬ìš©)",
                            "ì„ íƒì  ë³‘í•© (ì‚¬ìš©ìê°€ ì„ íƒ)",
                            "ì·¨ì†Œ (ì—…ë¡œë“œ ì·¨ì†Œ)"
                        ],
                        key="merge_strategy"
                    )
                
                with col2:
                    if merge_strategy != "ì·¨ì†Œ (ì—…ë¡œë“œ ì·¨ì†Œ)":
                        if st.button("ğŸ”„ íŠ¹ì§• ë§¤ì¹­ ì ìš©", key="apply_feature_matching"):
                            # ë§¤ì¹­ ì ìš© ë¡œì§
                            new_params = apply_feature_matching(
                                norm_params, param_data, feature_names, merge_strategy
                            )
                            
                            if new_params:
                                st.session_state.normalization_params = new_params
                                st.success("âœ… íŠ¹ì§• ë§¤ì¹­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.rerun()
                            else:
                                st.error("âŒ íŠ¹ì§• ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                
                # ì„ íƒì  ë³‘í•©ì¸ ê²½ìš° ì¶”ê°€ ì˜µì…˜
                if merge_strategy == "ì„ íƒì  ë³‘í•© (ì‚¬ìš©ìê°€ ì„ íƒ)":
                    st.markdown("**ë§¤ì¹­ëœ íŠ¹ì§•ë³„ ì‚¬ìš© ì—¬ë¶€ ì„ íƒ:**")
                    
                    if matched_features:
                        selected_features = st.multiselect(
                            "ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ì‚¬ìš©í•  íŠ¹ì§• ì„ íƒ",
                            sorted(matched_features),
                            default=sorted(matched_features),
                            key="selective_features"
                        )
                        
                        st.info(f"ì„ íƒëœ íŠ¹ì§•: {len(selected_features)}ê°œ, ìë™ ê³„ì‚° íŠ¹ì§•: {len(current_only_features)}ê°œ")
            
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # ì „ì²´ íŒŒë¼ë¯¸í„° í…Œì´ë¸” í‘œì‹œ
        st.subheader("ğŸ“‹ ì „ì²´ ì •ê·œí™” íŒŒë¼ë¯¸í„°")
        
        # DataFrameìœ¼ë¡œ í‘œì‹œ (ì¶œì²˜ ì •ë³´ í¬í•¨) - ì„ íƒ ê°€ëŠ¥í•œ í…Œì´ë¸”
        display_data = []
        for i, params in norm_params.items():
            feature_name = feature_names[i] if i < len(feature_names) else f'Feature_{i}'
            source = params.get('source', 'auto_calculated')
            source_icon = {
                'auto_calculated': 'ğŸ”¢',
                'uploaded': 'ğŸ”„',
                'manually_edited': 'âœï¸'
            }.get(source, 'â“')
            
            display_data.append({
                'index': i,  # ì¸ë±ìŠ¤ ì¶”ê°€
                'íŠ¹ì§•ëª…': f"{source_icon} {feature_name}",
                'í˜„ì¬ ìµœì†Œê°’': f"{params['original_min']:.4f}",
                'í˜„ì¬ ìµœëŒ€ê°’': f"{params['original_max']:.4f}",
                'ì¡°ì •ëœ ìµœì†Œê°’': f"{params['min']:.4f}",
                'ì¡°ì •ëœ ìµœëŒ€ê°’': f"{params['max']:.4f}",
                'ë²”ìœ„': f"{params['range']:.4f}",
                'ì¶œì²˜': {
                    'auto_calculated': 'ìë™ê³„ì‚°',
                    'uploaded': 'ì—…ë¡œë“œ',
                    'manually_edited': 'ìˆ˜ë™í¸ì§‘'
                }.get(source, 'ì•Œ ìˆ˜ ì—†ìŒ')
            })
        
        param_df = pd.DataFrame(display_data)
        
        # ìƒ‰ìƒì„ ì‚¬ìš©í•œ ê°•ì¡° í‘œì‹œë¥¼ ìœ„í•´ ìŠ¤íƒ€ì¼ë§ ì ìš©
        def highlight_by_source(row):
            styles = [''] * len(row)
            if 'ğŸ”„' in row['íŠ¹ì§•ëª…']:  # ì—…ë¡œë“œëœ íŠ¹ì§•
                styles = ['background-color: #e8f5e8; color: #2e7d32'] * len(row)
            elif 'âœï¸' in row['íŠ¹ì§•ëª…']:  # ìˆ˜ë™ í¸ì§‘ëœ íŠ¹ì§•
                styles = ['background-color: #fff3e0; color: #f57c00'] * len(row)
            
            # ì¡°ì •ëœ ê°’ë“¤ì„ ê°•ì¡°
            styles[4] = styles[4] + '; font-weight: bold'  # ì¡°ì •ëœ ìµœì†Œê°’ (index ê³ ë ¤)
            styles[5] = styles[5] + '; font-weight: bold'  # ì¡°ì •ëœ ìµœëŒ€ê°’ (index ê³ ë ¤)
            return styles
        
        # index ì»¬ëŸ¼ ìˆ¨ê¸°ê³  ì„ íƒ ê°€ëŠ¥í•œ í…Œì´ë¸”ë¡œ í‘œì‹œ
        styled_df = param_df.style.apply(highlight_by_source, axis=1)
        
        # ì„ íƒ ê°€ëŠ¥í•œ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
        event = st.dataframe(
            styled_df,
            use_container_width=True, 
            hide_index=True,
            on_select="rerun",
            selection_mode="single-row",
            key="param_table_selection"
        )
        
        # ë²”ë¡€
        st.markdown("""
        **ë²”ë¡€:** ğŸ”¢ ìë™ê³„ì‚° | ğŸ”„ íŒŒì¼ì—…ë¡œë“œ | âœï¸ ìˆ˜ë™í¸ì§‘  
        **ì‚¬ìš©ë²•:** í…Œì´ë¸”ì—ì„œ í–‰ì„ í´ë¦­í•˜ì—¬ í¸ì§‘í•  íŠ¹ì§•ì„ ì„ íƒí•˜ì„¸ìš”.
        """)
        
        # í…Œì´ë¸”ì—ì„œ ì„ íƒëœ í–‰ í™•ì¸
        selected_row_idx = None
        if event.selection.rows:
            selected_row_idx = event.selection.rows[0]
        
        # í…Œì´ë¸”ì—ì„œ ì„ íƒëœ í–‰ ê¸°ë°˜ í¸ì§‘ ì¸í„°í˜ì´ìŠ¤
        st.markdown("---")
        st.subheader("ğŸ”§ ì •ê·œí™” íŒŒë¼ë¯¸í„° í¸ì§‘")
        
        if selected_row_idx is not None:
            # ì„ íƒëœ í–‰ì˜ ì‹¤ì œ íŠ¹ì§• ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            edit_feature_idx = param_df.iloc[selected_row_idx]['index']
            selected_feature_name = param_df.iloc[selected_row_idx]['íŠ¹ì§•ëª…']
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown(f"**ì„ íƒëœ íŠ¹ì§•:** {selected_feature_name}")
                st.info("ğŸ’¡ í…Œì´ë¸”ì—ì„œ ë‹¤ë¥¸ í–‰ì„ í´ë¦­í•˜ì—¬ í¸ì§‘í•  íŠ¹ì§•ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            with col2:
                st.markdown("**ì„ íƒëœ íŠ¹ì§• ì •ë³´**")
                current_params = norm_params[edit_feature_idx]
                source = current_params.get('source', 'auto_calculated')
                source_text = {
                    'auto_calculated': 'ìë™ê³„ì‚°',
                    'uploaded': 'ì—…ë¡œë“œë¨',
                    'manually_edited': 'ìˆ˜ë™í¸ì§‘'
                }.get(source, 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                st.write(f"ì¶œì²˜: {source_text}")
                st.write(f"í˜„ì¬ ìµœì†Œê°’: {current_params['original_min']:.4f}")
                st.write(f"í˜„ì¬ ìµœëŒ€ê°’: {current_params['original_max']:.4f}")
            
            # í¸ì§‘ ì¸í„°í˜ì´ìŠ¤
            current_params = norm_params[edit_feature_idx]
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                new_min = st.number_input(
                    "ì¡°ì •ëœ ìµœì†Œê°’",
                    value=float(current_params['min']),
                    format="%.6f",
                    key=f"edit_min_value_{edit_feature_idx}"  # ê³ ìœ  í‚¤ ì‚¬ìš©
                )
            
            with col2:
                new_max = st.number_input(
                    "ì¡°ì •ëœ ìµœëŒ€ê°’",
                    value=float(current_params['max']),
                    format="%.6f",
                    key=f"edit_max_value_{edit_feature_idx}"  # ê³ ìœ  í‚¤ ì‚¬ìš©
                )
            
            with col3:
                st.markdown("**í¸ì§‘ ë™ì‘**")
                if st.button("âœ… ì—…ë°ì´íŠ¸", key=f"update_feature_params_{edit_feature_idx}"):
                    if new_max > new_min:
                        norm_params[edit_feature_idx]['min'] = new_min
                        norm_params[edit_feature_idx]['max'] = new_max
                        norm_params[edit_feature_idx]['range'] = new_max - new_min
                        norm_params[edit_feature_idx]['source'] = 'manually_edited'
                        st.session_state.normalization_params = norm_params
                        feature_name = feature_names[edit_feature_idx] if edit_feature_idx < len(feature_names) else f'Feature_{edit_feature_idx}'
                        st.success(f"âœ… {feature_name} íŠ¹ì§•ì˜ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()
                    else:
                        st.error("âŒ ìµœëŒ“ê°’ì€ ìµœì†Ÿê°’ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        
        else:
            st.info("ğŸ“ ìœ„ì˜ í…Œì´ë¸”ì—ì„œ í¸ì§‘í•  íŠ¹ì§•ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        # ê³„ì‚° ê·œì¹™ ì„¤ëª…
        with st.expander("ğŸ“ ì¡°ì •ëœ ë²”ìœ„ ê³„ì‚° ê·œì¹™ ë° íŠ¹ì§• ë§¤ì¹­"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **ìë™ ê³„ì‚° ê·œì¹™:**
                
                - **ìµœì†Œê°’ ì²˜ë¦¬:**
                    - í˜„ì¬ ìµœì†Œê°’ì´ 0ì´ë©´ ê·¸ëŒ€ë¡œ 0ìœ¼ë¡œ ìœ ì§€
                    - ì–‘ìˆ˜ ìµœì†Œê°’ì€ 0ìœ¼ë¡œ ì„¤ì •
                    - ìŒìˆ˜ ìµœì†Œê°’ì€ 20% í™•ì¥í•˜ì—¬ ë°˜ì˜¬ë¦¼
                
                - **ìµœëŒ€ê°’ ì²˜ë¦¬ (ë²”ìœ„ë³„):**
                    - 1 ì´í•˜: 20% í™•ì¥ í›„ ì†Œìˆ˜ì  1ìë¦¬ê¹Œì§€
                    - 10 ë¯¸ë§Œ: ì •ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼
                    - 100 ë¯¸ë§Œ: 10 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
                    - 1000 ë¯¸ë§Œ: 50 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼  
                    - 10000 ë¯¸ë§Œ: 100 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
                    - ê·¸ ì´ìƒ: ë” í° ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
                
                - **íŠ¹ë³„ ì²˜ë¦¬:**
                    - ì™¸ê¸°ì˜¨ë„(embient_temp): ìµœì†Œ -10, ìµœëŒ€ 35ë¡œ ê³ ì •
                """)
            
            with col2:
                st.markdown("""
                **íŠ¹ì§• ë§¤ì¹­ ê¸°ëŠ¥:**
                
                - **ë§¤ì¹­ëœ íŠ¹ì§•:** ğŸ”„
                    - í˜„ì¬ ë°ì´í„°ì™€ ì—…ë¡œë“œ íŒŒì¼ ëª¨ë‘ì— ì¡´ì¬
                    - ì—…ë¡œë“œëœ íŒŒë¼ë¯¸í„° ê°’ ì‚¬ìš©
                
                - **ìƒˆë¡œìš´ íŠ¹ì§•:** ğŸ”¢
                    - í˜„ì¬ ë°ì´í„°ì—ë§Œ ì¡´ì¬ (ì—…ë¡œë“œ íŒŒì¼ì— ì—†ìŒ)
                    - ìë™ ê³„ì‚°ëœ íŒŒë¼ë¯¸í„° ê°’ ì‚¬ìš©
                
                - **ì œê±°ëœ íŠ¹ì§•:**
                    - ì—…ë¡œë“œ íŒŒì¼ì—ë§Œ ì¡´ì¬ (í˜„ì¬ ë°ì´í„°ì— ì—†ìŒ)
                    - í…Œì´ë¸”ì—ì„œ ìë™ ì œê±°
                
                **í¸ì§‘ ê¸°ëŠ¥:**
                - ê° íŠ¹ì§•ë³„ë¡œ ê°œë³„ ì¡°ì • ê°€ëŠ¥
                - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë° ë¯¸ë¦¬ë³´ê¸°
                - íŒŒì¼ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ì§€ì›
                """)
        
        # ì •ê·œí™” ì ìš©
        st.markdown("---")
        if st.button("ğŸ¯ ì •ê·œí™” ì ìš©", key="apply_normalization", type="primary"):
            with st.spinner("ì •ê·œí™” ì ìš© ì¤‘..."):
                normalized_dataset = {}
                
                # ëª¨ë“  ë°ì´í„°ì— ì •ê·œí™” ì ìš©
                normalized_dataset['train_inputs'] = apply_normalization(
                    dataset['train_inputs'], norm_params
                )
                normalized_dataset['train_outputs'] = apply_normalization(
                    dataset['train_outputs'], norm_params
                )
                normalized_dataset['val_inputs'] = apply_normalization(
                    dataset['val_inputs'], norm_params
                )
                normalized_dataset['val_outputs'] = apply_normalization(
                    dataset['val_outputs'], norm_params
                )
                
                # ë©”íƒ€ë°ì´í„° ë³µì‚¬
                if 'metadata' in dataset:
                    normalized_dataset['metadata'] = dataset['metadata'].copy()
                    normalized_dataset['metadata']['normalization_applied'] = True
                    normalized_dataset['metadata']['normalization_params'] = norm_params
                
                st.session_state.normalized_dataset = normalized_dataset
                st.success("âœ… ì •ê·œí™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì •ê·œí™” ê²°ê³¼ í™•ì¸
        if st.session_state.normalized_dataset is not None:
            st.markdown("---")
            st.subheader("ğŸ“ˆ ì •ê·œí™” ê²°ê³¼ í™•ì¸")
            
            normalized_dataset = st.session_state.normalized_dataset
            
            # ì •ê·œí™” ì „í›„ ë¹„êµë¥¼ ìœ„í•œ ì„ íƒ ì˜µì…˜ë“¤
            col1, col2, col3 = st.columns(3)
            
            with col1:
                data_type = st.selectbox(
                    "ë°ì´í„° íƒ€ì… ì„ íƒ",
                    ["train_inputs", "train_outputs", "val_inputs", "val_outputs"],
                    key="norm_compare_data_type"
                )
            
            with col2:
                feature_to_compare = st.selectbox(
                    "ë¹„êµí•  íŠ¹ì§• ì„ íƒ",
                    range(len(feature_names)),
                    format_func=lambda x: f"{feature_names[x]}",
                    key="norm_compare_feature"
                )
            
            with col3:
                sample_count = st.number_input(
                    "ë¶„ì„í•  ìƒ˜í”Œ ìˆ˜",
                    min_value=10,
                    max_value=min(100000, len(normalized_dataset[data_type])),
                    value=min(1000, len(normalized_dataset[data_type])),
                    key="norm_sample_count"
                )
            
            # ë°ì´í„° íƒ€ì… í•œê¸€ í‘œì‹œ ë§¤í•‘
            data_type_korean = {
                "train_inputs": "í›ˆë ¨ìš© ì…ë ¥",
                "train_outputs": "í›ˆë ¨ìš© ë¼ë²¨",
                "val_inputs": "ê²€ì¦ìš© ì…ë ¥", 
                "val_outputs": "ê²€ì¦ìš© ë¼ë²¨"
            }
            
            st.markdown(f"**ì„ íƒëœ ë°ì´í„°**: {data_type_korean[data_type]} - {feature_names[feature_to_compare]}")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ì •ê·œí™” ì „**")
                original_data = dataset[data_type][:sample_count, :, feature_to_compare].flatten()
                
                fig_before = go.Figure()
                fig_before.add_trace(go.Histogram(
                    x=original_data,
                    nbinsx=50,
                    name="ì •ê·œí™” ì „",
                    marker_color='lightblue'
                ))
                fig_before.update_layout(
                    title=f"ì •ê·œí™” ì „ ë¶„í¬<br>{data_type_korean[data_type]}",
                    xaxis_title="ê°’",
                    yaxis_title="ë¹ˆë„",
                    height=350
                )
                st.plotly_chart(fig_before, use_container_width=True)
                
                # í†µê³„ ì •ë³´
                st.markdown("**í†µê³„ ì •ë³´:**")
                stats_before = pd.DataFrame({
                    'í†µê³„ëŸ‰': ['ìµœì†Ÿê°’', 'ìµœëŒ“ê°’', 'í‰ê· ', 'í‘œì¤€í¸ì°¨', 'ì¤‘ì•™ê°’'],
                    'ê°’': [
                        f"{np.min(original_data):.6f}",
                        f"{np.max(original_data):.6f}",
                        f"{np.mean(original_data):.6f}",
                        f"{np.std(original_data):.6f}",
                        f"{np.median(original_data):.6f}"
                    ]
                })
                st.dataframe(stats_before, use_container_width=True, hide_index=True)
            
            with col2:
                st.markdown("**ì •ê·œí™” í›„**")
                normalized_data = normalized_dataset[data_type][:sample_count, :, feature_to_compare].flatten()
                
                fig_after = go.Figure()
                fig_after.add_trace(go.Histogram(
                    x=normalized_data,
                    nbinsx=50,
                    name="ì •ê·œí™” í›„",
                    marker_color='lightgreen'
                ))
                fig_after.update_layout(
                    title=f"ì •ê·œí™” í›„ ë¶„í¬<br>{data_type_korean[data_type]}",
                    xaxis_title="ê°’ (0~1 ë²”ìœ„)",
                    yaxis_title="ë¹ˆë„",
                    height=350
                )
                st.plotly_chart(fig_after, use_container_width=True)
                
                # í†µê³„ ì •ë³´
                st.markdown("**í†µê³„ ì •ë³´:**")
                stats_after = pd.DataFrame({
                    'í†µê³„ëŸ‰': ['ìµœì†Ÿê°’', 'ìµœëŒ“ê°’', 'í‰ê· ', 'í‘œì¤€í¸ì°¨', 'ì¤‘ì•™ê°’'],
                    'ê°’': [
                        f"{np.min(normalized_data):.6f}",
                        f"{np.max(normalized_data):.6f}",
                        f"{np.mean(normalized_data):.6f}",
                        f"{np.std(normalized_data):.6f}",
                        f"{np.median(normalized_data):.6f}"
                    ]
                })
                st.dataframe(stats_after, use_container_width=True, hide_index=True)
            
            # ì •ê·œí™” í’ˆì§ˆ ê²€ì¦
            st.markdown("---")
            st.subheader("ğŸ” ì •ê·œí™” í’ˆì§ˆ ê²€ì¦")
            
            # ëª¨ë“  ë°ì´í„° íƒ€ì…ì— ëŒ€í•œ ì •ê·œí™” ë²”ìœ„ í™•ì¸
            validation_results = []
            for dt in ["train_inputs", "train_outputs", "val_inputs", "val_outputs"]:
                data = normalized_dataset[dt]
                min_val = np.min(data)
                max_val = np.max(data)
                
                # ë²”ìœ„ ë²—ì–´ë‚¨ ì²´í¬ (0~1 ë²”ìœ„)
                out_of_range = (min_val < -0.001) or (max_val > 1.001)  # ì‘ì€ ì˜¤ì°¨ í—ˆìš©
                
                validation_results.append({
                    'ë°ì´í„° íƒ€ì…': data_type_korean[dt],
                    'ìµœì†Ÿê°’': f"{min_val:.6f}",
                    'ìµœëŒ“ê°’': f"{max_val:.6f}",
                    'ì •ê·œí™” ìƒíƒœ': 'âœ… ì •ìƒ' if not out_of_range else 'âŒ ë²”ìœ„ ë²—ì–´ë‚¨'
                })
            
            validation_df = pd.DataFrame(validation_results)
            st.dataframe(validation_df, use_container_width=True, hide_index=True)


def apply_feature_matching(norm_params, uploaded_param_data, feature_names, merge_strategy):
    """íŠ¹ì§• ë§¤ì¹­ì„ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì •ê·œí™” íŒŒë¼ë¯¸í„° ìƒì„±"""
    try:
        new_params = {}
        
        # í˜„ì¬ íŠ¹ì§•ë“¤ê³¼ ì—…ë¡œë“œëœ íŠ¹ì§•ë“¤ ë¶„ì„
        current_features = set(feature_names)
        uploaded_features = set(uploaded_param_data.keys())
        matched_features = current_features & uploaded_features
        current_only_features = current_features - uploaded_features
        
        # ê° í˜„ì¬ íŠ¹ì§•ì— ëŒ€í•´ ì²˜ë¦¬
        for i, feature_name in enumerate(feature_names):
            if feature_name in matched_features:
                # ë§¤ì¹­ëœ íŠ¹ì§•: ì—…ë¡œë“œëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
                uploaded_values = uploaded_param_data[feature_name]
                new_params[i] = {
                    'min': uploaded_values['min'],
                    'max': uploaded_values['max'],
                    'range': uploaded_values['max'] - uploaded_values['min'],
                    'original_min': norm_params[i]['original_min'],
                    'original_max': norm_params[i]['original_max'],
                    'source': 'uploaded'
                }
            else:
                # í˜„ì¬ ë°ì´í„°ì—ë§Œ ìˆëŠ” íŠ¹ì§•: ê¸°ì¡´ ìë™ ê³„ì‚°ëœ íŒŒë¼ë¯¸í„° ìœ ì§€
                new_params[i] = norm_params[i].copy()
                new_params[i]['source'] = 'auto_calculated'
        
        return new_params
        
    except Exception as e:
        st.error(f"íŠ¹ì§• ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


def calculate_adjusted_range(feature_name, current_min, current_max):
    """React ì»´í¬ë„ŒíŠ¸ì˜ ê³„ì‚° ë¡œì§ì„ Pythonìœ¼ë¡œ ë³€í™˜"""
    
    # ì™¸ê¸°ì˜¨ë„ íŠ¹ë³„ ì²˜ë¦¬
    if 'embient_temp' in feature_name.lower() or 'ambient_temp' in feature_name.lower():
        return -10, 35
    
    # ìµœì†Œê°’ ì²˜ë¦¬: 0ì´ë©´ ê·¸ëŒ€ë¡œ 0, ì•„ë‹ˆë©´ ìŒìˆ˜ë¡œ í™•ì¥
    if current_min == 0:
        adjusted_min = 0
    elif current_min > 0:
        adjusted_min = 0  # ì–‘ìˆ˜ ìµœì†Œê°’ì€ 0ìœ¼ë¡œ ì„¤ì •
    else:
        # ìŒìˆ˜ì¸ ê²½ìš° ì ì ˆí•œ ë²”ìœ„ë¡œ í™•ì¥
        adjusted_min = np.floor(current_min * 1.2)
    
    # ìµœëŒ€ê°’ ì²˜ë¦¬
    if current_max <= 1:
        adjusted_max = np.ceil(current_max * 1.2 * 10) / 10  # ì†Œìˆ˜ì  1ìë¦¬ê¹Œì§€
    elif current_max < 10:
        adjusted_max = np.ceil(current_max)
    elif current_max < 100:
        adjusted_max = np.ceil(current_max / 10) * 10
    elif current_max < 1000:
        adjusted_max = np.ceil(current_max / 50) * 50
    elif current_max < 10000:
        adjusted_max = np.ceil(current_max / 100) * 100
    elif current_max < 100000:
        adjusted_max = np.ceil(current_max / 1000) * 1000
    else:
        adjusted_max = np.ceil(current_max / 10000) * 10000
    
    return adjusted_min, adjusted_max






def tab_feature_shift():
    """íƒ­3: íŠ¹ì§• ì‹œí”„íŠ¸"""
    st.header("â†”ï¸ íŠ¹ì§• ì‹œí”„íŠ¸")
    
    # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²°ì •
    if st.session_state.normalized_dataset is not None:
        current_dataset = st.session_state.normalized_dataset
        dataset_name = "ì •ê·œí™”ëœ ë°ì´í„°ì…‹"
    elif st.session_state.dataset is not None:
        current_dataset = st.session_state.dataset
        dataset_name = "ì›ë³¸ ë°ì´í„°ì…‹"
    else:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    feature_names = st.session_state.feature_names
    st.markdown(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°: **{dataset_name}**")
    st.markdown("íŠ¹ì • íŠ¹ì§•ì„ ì‹œê°„ ì¶•ì—ì„œ ì•ë‹¹ê¸°ê±°ë‚˜ ì§€ì—°ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì‹œí”„íŠ¸ ì„¤ì •
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        data_type = st.selectbox(
            "ë°ì´í„° íƒ€ì… ì„ íƒ",
            ["train_inputs", "train_outputs", "val_inputs", "val_outputs"],
            key="shift_data_type"
        )
    
    with col2:
        shift_feature = st.selectbox(
            "ì‹œí”„íŠ¸í•  íŠ¹ì§• ì„ íƒ",
            range(len(feature_names)),
            format_func=lambda x: f"{x}: {feature_names[x]}",
            key="shift_feature_select"
        )
    
    with col3:
        shift_amount = st.number_input(
            "ì‹œí”„íŠ¸ ì–‘ (í‹±)",
            min_value=-50,
            max_value=50,
            value=0,
            help="ì–‘ìˆ˜: ì˜¤ë¥¸ìª½ ì‹œí”„íŠ¸(ì§€ì—°), ìŒìˆ˜: ì™¼ìª½ ì‹œí”„íŠ¸(ì•ë‹¹ê¹€)",
            key="shift_amount"
        )
    
    with col4:
        st.markdown("**ì‹œí”„íŠ¸ ë°©í–¥:**")
        if shift_amount > 0:
            st.info("ğŸ”œ ì˜¤ë¥¸ìª½ ì‹œí”„íŠ¸ (ì§€ì—°)")
        elif shift_amount < 0:
            st.info("ğŸ”™ ì™¼ìª½ ì‹œí”„íŠ¸ (ì•ë‹¹ê¹€)")
        else:
            st.info("â¡ï¸ ì‹œí”„íŠ¸ ì—†ìŒ")
    
    # ì‹œí”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°
    if shift_amount != 0:
        with st.expander("ğŸ‘€ ì‹œí”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°"):
            sample_data = current_dataset[data_type][0, :, shift_feature]  # ì²« ë²ˆì§¸ ìƒ˜í”Œ
            shifted_sample = apply_feature_shift(
                current_dataset[data_type][:1], shift_feature, shift_amount
            )[0, :, shift_feature]
            
            fig_preview = go.Figure()
            fig_preview.add_trace(go.Scatter(
                y=sample_data,
                mode='lines+markers',
                name='ì›ë³¸',
                line=dict(color='blue', width=2)
            ))
            fig_preview.add_trace(go.Scatter(
                y=shifted_sample,
                mode='lines+markers',
                name=f'ì‹œí”„íŠ¸ ({shift_amount}í‹±)',
                line=dict(color='red', width=2, dash='dash')
            ))
            
            fig_preview.update_layout(
                title=f"{feature_names[shift_feature]} íŠ¹ì§• ì‹œí”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°",
                xaxis_title="Time Steps",
                yaxis_title="ê°’",
                height=400
            )
            st.plotly_chart(fig_preview, use_container_width=True)
    
    # ì‹œí”„íŠ¸ ì ìš©
    if st.button("ğŸ¯ ì‹œí”„íŠ¸ ì ìš©", key="apply_shift"):
        if shift_amount == 0:
            st.warning("âš ï¸ ì‹œí”„íŠ¸ ì–‘ì´ 0ì´ë¯€ë¡œ ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            with st.spinner("ì‹œí”„íŠ¸ ì ìš© ì¤‘..."):
                # í˜„ì¬ ë°ì´í„°ì…‹ ë³µì‚¬
                if st.session_state.shifted_dataset is None:
                    shifted_dataset = {}
                    for key in current_dataset.keys():
                        if isinstance(current_dataset[key], np.ndarray):
                            shifted_dataset[key] = current_dataset[key].copy()
                        else:
                            shifted_dataset[key] = current_dataset[key]
                else:
                    shifted_dataset = st.session_state.shifted_dataset
                
                # ì„ íƒëœ ë°ì´í„°ì— ì‹œí”„íŠ¸ ì ìš©
                shifted_dataset[data_type] = apply_feature_shift(
                    shifted_dataset[data_type], shift_feature, shift_amount
                )
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                if 'metadata' in shifted_dataset:
                    if 'shift_history' not in shifted_dataset['metadata']:
                        shifted_dataset['metadata']['shift_history'] = []
                    
                    shifted_dataset['metadata']['shift_history'].append({
                        'data_type': data_type,
                        'feature_index': shift_feature,
                        'feature_name': feature_names[shift_feature],
                        'shift_amount': shift_amount,
                        'timestamp': datetime.now().isoformat()
                    })
                
                st.session_state.shifted_dataset = shifted_dataset
                st.success(f"âœ… {feature_names[shift_feature]} íŠ¹ì§•ì— {shift_amount}í‹± ì‹œí”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬ í‘œì‹œ
    if (st.session_state.shifted_dataset is not None and 
        'metadata' in st.session_state.shifted_dataset and
        'shift_history' in st.session_state.shifted_dataset['metadata']):
        
        st.subheader("ğŸ“‹ ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬")
        
        shift_history = st.session_state.shifted_dataset['metadata']['shift_history']
        if shift_history:
            history_df = pd.DataFrame(shift_history)
            history_df = history_df[['data_type', 'feature_name', 'shift_amount', 'timestamp']]
            history_df.columns = ['ë°ì´í„° íƒ€ì…', 'íŠ¹ì§•ëª…', 'ì‹œí”„íŠ¸ ì–‘', 'ì ìš© ì‹œê°„']
            
            st.dataframe(history_df, use_container_width=True, hide_index=True)
            
            if st.button("ğŸ—‘ï¸ ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”", key="clear_shift_history"):
                st.session_state.shifted_dataset = None
                st.success("âœ… ì‹œí”„íŠ¸ íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
        else:
            st.info("ğŸ“ ì•„ì§ ì ìš©ëœ ì‹œí”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

def tab_sequence_reshape():
    """íƒ­4: ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •"""
    st.header("ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •")
    
    # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²°ì •
    if st.session_state.shifted_dataset is not None:
        current_dataset = st.session_state.shifted_dataset
        dataset_name = "ì‹œí”„íŠ¸ ì ìš©ëœ ë°ì´í„°ì…‹"
    elif st.session_state.normalized_dataset is not None:
        current_dataset = st.session_state.normalized_dataset
        dataset_name = "ì •ê·œí™”ëœ ë°ì´í„°ì…‹"
    elif st.session_state.dataset is not None:
        current_dataset = st.session_state.dataset
        dataset_name = "ì›ë³¸ ë°ì´í„°ì…‹"
    else:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    feature_names = st.session_state.feature_names
    st.markdown(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°: **{dataset_name}**")
    
    # í˜„ì¬ ë°ì´í„° í˜•íƒœ ì •ë³´ í‘œì‹œ
    st.subheader("ğŸ“Š í˜„ì¬ ë°ì´í„° í˜•íƒœ")
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ì…ë ¥ ë°ì´í„°:**")
        current_input_shape = current_dataset['train_inputs'].shape
        st.write(f"- í›ˆë ¨ ì…ë ¥: {current_input_shape}")
        st.write(f"- ê²€ì¦ ì…ë ¥: {current_dataset['val_inputs'].shape}")
        st.write(f"- í˜„ì¬ Lookback ê¸¸ì´: **{current_input_shape[1]}**")
    
    with col2:
        st.markdown("**ì¶œë ¥ ë°ì´í„°:**")
        current_output_shape = current_dataset['train_outputs'].shape
        st.write(f"- í›ˆë ¨ ì¶œë ¥: {current_output_shape}")
        st.write(f"- ê²€ì¦ ì¶œë ¥: {current_dataset['val_outputs'].shape}")
        st.write(f"- í˜„ì¬ Horizon ê¸¸ì´: **{current_output_shape[1]}**")
    
    st.markdown("---")
    
    # ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
    st.subheader("âš™ï¸ ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        new_lookback = st.number_input(
            "ìƒˆë¡œìš´ Lookback ê¸¸ì´",
            min_value=1,
            max_value=current_input_shape[1],
            value=min(current_input_shape[1], 50),
            help="ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ (ê³¼ê±°ë¥¼ ì–¼ë§ˆë‚˜ ë³¼ ê²ƒì¸ê°€)",
            key="new_lookback_length"
        )
    
    with col2:
        new_horizon = st.number_input(
            "ìƒˆë¡œìš´ Horizon ê¸¸ì´", 
            min_value=1,
            max_value=current_output_shape[1],
            value=min(current_output_shape[1], 10),
            help="ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ (ë¯¸ë˜ë¥¼ ì–¼ë§ˆë‚˜ ì˜ˆì¸¡í•  ê²ƒì¸ê°€)",
            key="new_horizon_length"
        )
    
    with col3:
        st.markdown("**ë°ì´í„° ì¶”ì¶œ ë°©ë²•:**")
        st.info("ğŸ“‹ Lookback: ë’·ë¶€ë¶„ì—ì„œ ì¶”ì¶œ\nğŸ“‹ Horizon: ì•ë¶€ë¶„ì—ì„œ ì¶”ì¶œ")
        st.markdown("*ì¶”ì¶œ ë°©ë²•ì´ ê³ ì •ë˜ì–´ ì‹œí€€ìŠ¤ ì—°ì†ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.*")
    
    # ë³€ê²½ ì‚¬í•­ ë¯¸ë¦¬ë³´ê¸°
    if new_lookback != current_input_shape[1] or new_horizon != current_output_shape[1]:
        st.markdown("### ğŸ”„ ë³€ê²½ ì‚¬í•­ ë¯¸ë¦¬ë³´ê¸°")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**ë³€ê²½ ì „ â†’ ë³€ê²½ í›„**")
            change_df = pd.DataFrame({
                'êµ¬ë¶„': ['Lookback ê¸¸ì´', 'Horizon ê¸¸ì´', 'ì´ ì‹œí€€ìŠ¤ ê¸¸ì´'],
                'ë³€ê²½ ì „': [
                    current_input_shape[1],
                    current_output_shape[1], 
                    current_input_shape[1] + current_output_shape[1]
                ],
                'ë³€ê²½ í›„': [
                    new_lookback,
                    new_horizon,
                    new_lookback + new_horizon
                ]
            })
            st.dataframe(change_df, use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("**ì˜ˆìƒ í˜•íƒœ ë³€í™”**")
            shape_df = pd.DataFrame({
                'ë°ì´í„° íƒ€ì…': ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs'],
                'í˜„ì¬ í˜•íƒœ': [
                    str(current_dataset['train_inputs'].shape),
                    str(current_dataset['train_outputs'].shape),
                    str(current_dataset['val_inputs'].shape),
                    str(current_dataset['val_outputs'].shape)
                ],
                'ë³€ê²½ í›„ í˜•íƒœ': [
                    f"({current_dataset['train_inputs'].shape[0]}, {new_lookback}, {current_dataset['train_inputs'].shape[2]})",
                    f"({current_dataset['train_outputs'].shape[0]}, {new_horizon}, {current_dataset['train_outputs'].shape[2]})",
                    f"({current_dataset['val_inputs'].shape[0]}, {new_lookback}, {current_dataset['val_inputs'].shape[2]})",
                    f"({current_dataset['val_outputs'].shape[0]}, {new_horizon}, {current_dataset['val_outputs'].shape[2]})"
                ]
            })
            st.dataframe(shape_df, use_container_width=True, hide_index=True)
        
        # ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ‘€ ë³€ê²½ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"):
            sample_idx = 0  # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ ë¯¸ë¦¬ë³´ê¸°
            
            # í˜„ì¬ ë°ì´í„° ì¶”ì¶œ
            current_input = current_dataset['train_inputs'][sample_idx]
            current_output = current_dataset['train_outputs'][sample_idx]
            
            # ìƒˆë¡œìš´ ê¸¸ì´ë¡œ ë³€í™˜ (ë¯¸ë¦¬ë³´ê¸°)
            # Lookback: ë’·ë¶€ë¶„ì—ì„œ ì¶”ì¶œ, Horizon: ì•ë¶€ë¶„ì—ì„œ ì¶”ì¶œ
            new_input_preview = current_input[-new_lookback:]  # ë’·ë¶€ë¶„ì—ì„œ
            new_output_preview = current_output[:new_horizon]  # ì•ë¶€ë¶„ì—ì„œ
            
            # ì²« ë²ˆì§¸ íŠ¹ì§•ë§Œ ì‹œê°í™”
            feature_idx = 0
            feature_name = feature_names[feature_idx] if feature_idx < len(feature_names) else f'Feature_{feature_idx}'
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig_input = go.Figure()
                fig_input.add_trace(go.Scatter(
                    y=current_input[:, feature_idx],
                    mode='lines+markers',
                    name=f'í˜„ì¬ ì…ë ¥ (ê¸¸ì´: {len(current_input)})',
                    line=dict(color='blue', width=2)
                ))
                fig_input.add_trace(go.Scatter(
                    y=new_input_preview[:, feature_idx],
                    mode='lines+markers',
                    name=f'ë³€ê²½ í›„ ì…ë ¥ (ê¸¸ì´: {len(new_input_preview)})',
                    line=dict(color='red', width=2, dash='dash')
                ))
                fig_input.update_layout(
                    title=f"ì…ë ¥ ì‹œí€€ìŠ¤ ë³€í™” ë¯¸ë¦¬ë³´ê¸°<br>{feature_name}",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_input, use_container_width=True)
            
            with col2:
                fig_output = go.Figure()
                fig_output.add_trace(go.Scatter(
                    y=current_output[:, feature_idx],
                    mode='lines+markers',
                    name=f'í˜„ì¬ ì¶œë ¥ (ê¸¸ì´: {len(current_output)})',
                    line=dict(color='green', width=2)
                ))
                fig_output.add_trace(go.Scatter(
                    y=new_output_preview[:, feature_idx],
                    mode='lines+markers',
                    name=f'ë³€ê²½ í›„ ì¶œë ¥ (ê¸¸ì´: {len(new_output_preview)})',
                    line=dict(color='orange', width=2, dash='dash')
                ))
                fig_output.update_layout(
                    title=f"ì¶œë ¥ ì‹œí€€ìŠ¤ ë³€í™” ë¯¸ë¦¬ë³´ê¸°<br>{feature_name}",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_output, use_container_width=True)
    
    # ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì ìš©
    if st.button("ğŸ¯ ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì ìš©", key="apply_sequence_reshape"):
        if new_lookback == current_input_shape[1] and new_horizon == current_output_shape[1]:
            st.warning("âš ï¸ ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            with st.spinner("ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì ìš© ì¤‘..."):
                try:
                    reshaped_dataset = {}
                    
                    # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ í¬ê¸° ì¡°ì •
                    # Lookback (ì…ë ¥): ë’·ë¶€ë¶„ì—ì„œ ì¶”ì¶œ
                    for data_type in ['train_inputs', 'val_inputs']:
                        current_data = current_dataset[data_type]
                        reshaped_dataset[data_type] = current_data[:, -new_lookback:, :]
                    
                    # Horizon (ì¶œë ¥): ì•ë¶€ë¶„ì—ì„œ ì¶”ì¶œ
                    for data_type in ['train_outputs', 'val_outputs']:
                        current_data = current_dataset[data_type]
                        reshaped_dataset[data_type] = current_data[:, :new_horizon, :]
                    
                    # ë©”íƒ€ë°ì´í„° ë³µì‚¬ ë° ì—…ë°ì´íŠ¸
                    if 'metadata' in current_dataset:
                        reshaped_dataset['metadata'] = current_dataset['metadata'].copy()
                        reshaped_dataset['metadata']['sequence_reshaped'] = True
                        reshaped_dataset['metadata']['reshape_info'] = {
                            'original_lookback': current_input_shape[1],
                            'original_horizon': current_output_shape[1],
                            'new_lookback': new_lookback,
                            'new_horizon': new_horizon,
                            'extraction_method': 'lookback_from_end_horizon_from_start',
                            'reshape_timestamp': datetime.now().isoformat()
                        }
                    
                    # ê¸°íƒ€ ì •ë³´ ë³µì‚¬
                    for key in current_dataset.keys():
                        if key not in reshaped_dataset and key not in ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs']:
                            reshaped_dataset[key] = current_dataset[key]
                    
                    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.reshaped_dataset = reshaped_dataset
                    
                    st.success("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                except Exception as e:
                    st.error(f"âŒ ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ë³€ê²½ ê²°ê³¼ í‘œì‹œ
    if hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None:
        st.markdown("---")
        st.subheader("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€ê²½ ì™„ë£Œ")
        
        reshaped_dataset = st.session_state.reshaped_dataset
        
        # ë³€ê²½ í›„ í˜•íƒœ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“ í›ˆë ¨ ìƒ˜í”Œ", f"{len(reshaped_dataset['train_inputs']):,}")
        with col2:
            st.metric("ğŸ”¬ ê²€ì¦ ìƒ˜í”Œ", f"{len(reshaped_dataset['val_inputs']):,}")
        with col3:
            st.metric("ğŸ“ˆ ìƒˆ Lookback", reshaped_dataset['train_inputs'].shape[1])
        with col4:
            st.metric("ğŸ“Š ìƒˆ Horizon", reshaped_dataset['train_outputs'].shape[1])
        
        # ìƒì„¸ ì •ë³´
        with st.expander("ğŸ“‹ ë³€ê²½ëœ ë°ì´í„° ìƒì„¸ ì •ë³´"):
            info_df = pd.DataFrame({
                'ë°ì´í„° íƒ€ì…': ['train_inputs', 'train_outputs', 'val_inputs', 'val_outputs'],
                'ë³€ê²½ í›„ í˜•íƒœ': [
                    str(reshaped_dataset['train_inputs'].shape),
                    str(reshaped_dataset['train_outputs'].shape),
                    str(reshaped_dataset['val_inputs'].shape),
                    str(reshaped_dataset['val_outputs'].shape)
                ]
            })
            st.dataframe(info_df, use_container_width=True, hide_index=True)
            
            if 'reshape_info' in reshaped_dataset.get('metadata', {}):
                reshape_info = reshaped_dataset['metadata']['reshape_info']
                st.json(reshape_info)

def tab_feature_selection():
    """íƒ­5: ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •"""
    st.header("ğŸ¯ ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •")
    
    # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²°ì • (ìš°ì„ ìˆœìœ„: reshaped > shifted > normalized > original)
    if hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None:
        current_dataset = st.session_state.reshaped_dataset
        dataset_name = "ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •ëœ ë°ì´í„°ì…‹"
    elif st.session_state.shifted_dataset is not None:
        current_dataset = st.session_state.shifted_dataset
        dataset_name = "ì‹œí”„íŠ¸ ì ìš©ëœ ë°ì´í„°ì…‹"
    elif st.session_state.normalized_dataset is not None:
        current_dataset = st.session_state.normalized_dataset
        dataset_name = "ì •ê·œí™”ëœ ë°ì´í„°ì…‹"
    elif st.session_state.dataset is not None:
        current_dataset = st.session_state.dataset
        dataset_name = "ì›ë³¸ ë°ì´í„°ì…‹"
    else:
        st.warning("âš ï¸ ë¨¼ì € 'ë°ì´í„° ì…ë ¥' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    feature_names = st.session_state.feature_names
    st.markdown(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë°ì´í„°: **{dataset_name}**")
    st.markdown("ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ì…ë ¥ íŠ¹ì§•ê³¼ ì¶œë ¥ íŠ¹ì§•ì„ ì„ íƒí•˜ì„¸ìš”.")
    
    # íŠ¹ì§• ì„ íƒ ì¸í„°í˜ì´ìŠ¤
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“¥ ì…ë ¥ íŠ¹ì§• ì„ íƒ")
        input_features = st.multiselect(
            "ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  íŠ¹ì§•ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
            range(len(feature_names)),
            default=list(range(min(5, len(feature_names)))),  # ê¸°ë³¸ê°’: ì²˜ìŒ 5ê°œ íŠ¹ì§•
            format_func=lambda x: f"{x}: {feature_names[x]}",
            key="input_features_select"
        )
        
        if input_features:
            st.write(f"**ì„ íƒëœ ì…ë ¥ íŠ¹ì§• ìˆ˜:** {len(input_features)}")
            input_feature_names = [feature_names[i] for i in input_features]
            for i, name in enumerate(input_feature_names):
                st.write(f"  {i+1}. {name}")
    
    with col2:
        st.subheader("ğŸ“¤ ì¶œë ¥ íŠ¹ì§• ì„ íƒ")
        output_features = st.multiselect(
            "ëª¨ë¸ ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•  íŠ¹ì§•ë“¤ì„ ì„ íƒí•˜ì„¸ìš”",
            range(len(feature_names)),
            default=list(range(min(3, len(feature_names)))),  # ê¸°ë³¸ê°’: ì²˜ìŒ 3ê°œ íŠ¹ì§•
            format_func=lambda x: f"{x}: {feature_names[x]}",
            key="output_features_select"
        )
        
        if output_features:
            st.write(f"**ì„ íƒëœ ì¶œë ¥ íŠ¹ì§• ìˆ˜:** {len(output_features)}")
            output_feature_names = [feature_names[i] for i in output_features]
            for i, name in enumerate(output_feature_names):
                st.write(f"  {i+1}. {name}")
    
    # íŠ¹ì§• ì„ íƒ ìœ íš¨ì„± ê²€ì‚¬
    if not input_features:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒì˜ ì…ë ¥ íŠ¹ì§•ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        return
    
    if not output_features:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒì˜ ì¶œë ¥ íŠ¹ì§•ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        return
    
    # ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¬êµ¬ì„±
    if st.button("ğŸ¯ ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¬êµ¬ì„±", key="reconstruct_dataset"):
        with st.spinner("ë°ì´í„°ì…‹ ì¬êµ¬ì„± ì¤‘..."):
            try:
                selected_dataset = select_features_from_dataset(
                    current_dataset, input_features, output_features
                )
                
                # ì„ íƒëœ íŠ¹ì§• ì´ë¦„ë“¤ ì €ì¥
                selected_input_names = [feature_names[i] for i in input_features]
                selected_output_names = [feature_names[i] for i in output_features]
                
                selected_dataset['selected_input_names'] = selected_input_names
                selected_dataset['selected_output_names'] = selected_output_names
                
                st.session_state.selected_features = selected_dataset
                st.success("âœ… ì„ íƒëœ íŠ¹ì§•ìœ¼ë¡œ ë°ì´í„°ì…‹ì´ ì¬êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
            except Exception as e:
                st.error(f"âŒ ë°ì´í„°ì…‹ ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ì¬êµ¬ì„±ëœ ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
    if st.session_state.selected_features is not None:
        st.markdown("---")
        st.subheader("âœ… ì¬êµ¬ì„±ëœ ë°ì´í„°ì…‹ ì •ë³´")
        
        selected_dataset = st.session_state.selected_features
        
        # ê¸°ë³¸ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“ í›ˆë ¨ ìƒ˜í”Œ", f"{len(selected_dataset['train_inputs']):,}")
        with col2:
            st.metric("ğŸ”¬ ê²€ì¦ ìƒ˜í”Œ", f"{len(selected_dataset['val_inputs']):,}")
        with col3:
            st.metric("ğŸ“¥ ì…ë ¥ íŠ¹ì§•", len(input_features))
        with col4:
            st.metric("ğŸ“¤ ì¶œë ¥ íŠ¹ì§•", len(output_features))
        
        # í˜•íƒœ ì •ë³´
        with st.expander("ğŸ“‹ ì¬êµ¬ì„±ëœ ë°ì´í„° í˜•íƒœ"):
            col1, col2 = st.columns(2)
            with col1:
                st.write("**í›ˆë ¨ ë°ì´í„°:**")
                st.write(f"- ì…ë ¥ í˜•íƒœ: {selected_dataset['train_inputs'].shape}")
                st.write(f"- ì¶œë ¥ í˜•íƒœ: {selected_dataset['train_outputs'].shape}")
            with col2:
                st.write("**ê²€ì¦ ë°ì´í„°:**")
                st.write(f"- ì…ë ¥ í˜•íƒœ: {selected_dataset['val_inputs'].shape}")
                st.write(f"- ì¶œë ¥ í˜•íƒœ: {selected_dataset['val_outputs'].shape}")
        
        # ì„ íƒëœ íŠ¹ì§• ìš”ì•½
        with st.expander("ğŸ·ï¸ ì„ íƒëœ íŠ¹ì§• ìš”ì•½"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ì…ë ¥ íŠ¹ì§•:**")
                input_df = pd.DataFrame({
                    'ì¸ë±ìŠ¤': input_features,
                    'íŠ¹ì§•ëª…': selected_dataset['selected_input_names']
                })
                st.dataframe(input_df, use_container_width=True, hide_index=True)
            
            with col2:
                st.markdown("**ì¶œë ¥ íŠ¹ì§•:**")
                output_df = pd.DataFrame({
                    'ì¸ë±ìŠ¤': output_features,
                    'íŠ¹ì§•ëª…': selected_dataset['selected_output_names']
                })
                st.dataframe(output_df, use_container_width=True, hide_index=True)
        
        # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        with st.expander("ğŸ‘€ ì¬êµ¬ì„±ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
            sample_idx = st.selectbox(
                "ë¯¸ë¦¬ë³¼ ìƒ˜í”Œ ì„ íƒ",
                range(min(5, len(selected_dataset['train_inputs']))),
                key="final_sample_preview"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ì…ë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)**")
                fig_input = go.Figure()
                input_sample = selected_dataset['train_inputs'][sample_idx]
                
                for i, feature_name in enumerate(selected_dataset['selected_input_names']):
                    fig_input.add_trace(go.Scatter(
                        y=input_sample[:, i],
                        mode='lines+markers',
                        name=feature_name,
                        line=dict(width=2)
                    ))
                
                fig_input.update_layout(
                    title="ì…ë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_input, use_container_width=True)
            
            with col2:
                st.markdown("**ì¶œë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)**")
                fig_output = go.Figure()
                output_sample = selected_dataset['train_outputs'][sample_idx]
                
                for i, feature_name in enumerate(selected_dataset['selected_output_names']):
                    fig_output.add_trace(go.Scatter(
                        y=output_sample[:, i],
                        mode='lines+markers',
                        name=feature_name,
                        line=dict(width=2)
                    ))
                
                fig_output.update_layout(
                    title="ì¶œë ¥ ì‹œí€€ìŠ¤ (ì„ íƒëœ íŠ¹ì§•)",
                    xaxis_title="Time Steps",
                    yaxis_title="ê°’",
                    height=300
                )
                st.plotly_chart(fig_output, use_container_width=True)
        
        # ë°ì´í„°ì…‹ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ
        st.subheader("ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥")
        
        dataset_filename = st.text_input(
            "ì €ì¥í•  íŒŒì¼ëª…",
            value="processed_dataset",
            key="final_dataset_filename"
        )
        
        if st.button("ğŸ“¦ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±", key="generate_final_dataset"):
            try:
                with st.spinner("ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì¤‘..."):
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    final_metadata = selected_dataset.get('metadata', {}).copy()
                    final_metadata.update({
                        'processing_complete': True,
                        'final_input_features': input_features,
                        'final_output_features': output_features,
                        'final_input_feature_names': selected_dataset['selected_input_names'],
                        'final_output_feature_names': selected_dataset['selected_output_names'],
                        'processing_timestamp': datetime.now().isoformat(),
                        'final_shapes': {
                            'train_inputs': selected_dataset['train_inputs'].shape,
                            'train_outputs': selected_dataset['train_outputs'].shape,
                            'val_inputs': selected_dataset['val_inputs'].shape,
                            'val_outputs': selected_dataset['val_outputs'].shape
                        }
                    })
                    
                    # ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
                    final_dataset = {
                        'train_inputs': selected_dataset['train_inputs'],
                        'train_outputs': selected_dataset['train_outputs'],
                        'val_inputs': selected_dataset['val_inputs'],
                        'val_outputs': selected_dataset['val_outputs'],
                        'metadata': final_metadata,
                        'input_feature_names': selected_dataset['selected_input_names'],
                        'output_feature_names': selected_dataset['selected_output_names']
                    }
                    
                    # NPY í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì¤€ë¹„
                    import io
                    buffer = io.BytesIO()
                    np.save(buffer, final_dataset)
                    dataset_data = buffer.getvalue()
                
                st.download_button(
                    label="ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ",
                    data=dataset_data,
                    file_name=f"{dataset_filename}.npy",
                    mime="application/octet-stream",
                    help="ì²˜ë¦¬ëœ ìµœì¢… ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
                )
                
                st.success("âœ… ìµœì¢… ë°ì´í„°ì…‹ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                
                # ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ
                with st.expander("ğŸ Python ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ"):
                    st.code(f"""
import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader

# ìµœì¢… ë°ì´í„°ì…‹ ë¡œë“œ
dataset = np.load('{dataset_filename}.npy', allow_pickle=True).item()

# ë°ì´í„° ì ‘ê·¼
train_inputs = dataset['train_inputs']    # Shape: {selected_dataset['train_inputs'].shape}
train_outputs = dataset['train_outputs']  # Shape: {selected_dataset['train_outputs'].shape}
val_inputs = dataset['val_inputs']        # Shape: {selected_dataset['val_inputs'].shape}
val_outputs = dataset['val_outputs']      # Shape: {selected_dataset['val_outputs'].shape}

# íŠ¹ì§• ì´ë¦„ í™•ì¸
input_features = dataset['input_feature_names']   # {selected_dataset['selected_input_names']}
output_features = dataset['output_feature_names'] # {selected_dataset['selected_output_names']}

# ë©”íƒ€ë°ì´í„° í™•ì¸
metadata = dataset['metadata']
print("ì²˜ë¦¬ ì™„ë£Œ:", metadata['processing_complete'])
print("ì„ íƒëœ ì…ë ¥ íŠ¹ì§•:", metadata['final_input_feature_names'])
print("ì„ íƒëœ ì¶œë ¥ íŠ¹ì§•:", metadata['final_output_feature_names'])

# PyTorch ë°ì´í„°ë¡œë” ìƒì„± ì˜ˆì‹œ
train_dataset = TensorDataset(
    torch.FloatTensor(train_inputs),
    torch.FloatTensor(train_outputs)
)
val_dataset = TensorDataset(
    torch.FloatTensor(val_inputs),
    torch.FloatTensor(val_outputs)
)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print(f"í›ˆë ¨ ë°ì´í„°: {{len(train_dataset):,}}ê°œ ìƒ˜í”Œ")
print(f"ê²€ì¦ ë°ì´í„°: {{len(val_dataset):,}}ê°œ ìƒ˜í”Œ")
print(f"ì…ë ¥ ì°¨ì›: {{train_inputs.shape[1:]}}")
print(f"ì¶œë ¥ ì°¨ì›: {{train_outputs.shape[1:]}}")
                    """, language="python")
                
            except Exception as e:
                st.error(f"âŒ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

# =================================================================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# =================================================================================
def main():
    # í˜ì´ì§€ ì„¤ì • ë° ì´ˆê¸°í™”
    st.set_page_config(
        page_title="ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ì•±", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    setup_korean_font()
    initialize_session_state()
    
    # ë©”ì¸ íƒ€ì´í‹€
    st.title("ğŸ”¬ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ë„êµ¬")
    st.markdown("ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì „ì²˜ë¦¬ë¶€í„° ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ê¹Œì§€")
    
    # íƒ­ ìƒì„±
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“ ë°ì´í„° ì…ë ¥", 
        "ğŸ“ ì •ê·œí™”", 
        "â†”ï¸ íŠ¹ì§• ì‹œí”„íŠ¸", 
        "ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •",
        "ğŸ¯ ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •"
    ])
    
    # ê° íƒ­ ì‹¤í–‰
    with tab1:
        tab_data_input()
    
    with tab2:
        tab_normalization()
    
    with tab3:
        tab_feature_shift()
    
    with tab4:
        tab_sequence_reshape()
    
    with tab5:
        tab_feature_selection()
    
    # ì‚¬ì´ë“œë°” ì •ë³´
    with st.sidebar:
        st.header("ğŸ“Š í˜„ì¬ ìƒíƒœ")
        
        # ë°ì´í„° ë¡œë“œ ìƒíƒœ
        if st.session_state.dataset is not None:
            st.success("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        else:
            st.error("âŒ ë°ì´í„° ë¯¸ë¡œë“œ")
        
        # ì •ê·œí™” ìƒíƒœ
        if st.session_state.normalized_dataset is not None:
            st.success("âœ… ì •ê·œí™” ì™„ë£Œ")
        else:
            st.warning("âš ï¸ ì •ê·œí™” ë¯¸ì™„ë£Œ")
        
        # ì‹œí”„íŠ¸ ìƒíƒœ
        if st.session_state.shifted_dataset is not None:
            st.success("âœ… ì‹œí”„íŠ¸ ì ìš©ë¨")
        else:
            st.info("â„¹ï¸ ì‹œí”„íŠ¸ ë¯¸ì ìš©")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • ìƒíƒœ
        if hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None:
            st.success("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • ì™„ë£Œ")
        else:
            st.info("â„¹ï¸ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì • ë¯¸ì ìš©")
        
        # íŠ¹ì§• ì„ íƒ ìƒíƒœ
        if st.session_state.selected_features is not None:
            st.success("âœ… íŠ¹ì§• ì„ íƒ ì™„ë£Œ")
        else:
            st.warning("âš ï¸ íŠ¹ì§• ì„ íƒ ë¯¸ì™„ë£Œ")
        
        st.markdown("---")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_steps = [
            ("ë°ì´í„° ë¡œë“œ", st.session_state.dataset is not None),
            ("ì •ê·œí™”", st.session_state.normalized_dataset is not None),
            ("ì‹œí”„íŠ¸ ì ìš©", st.session_state.shifted_dataset is not None),
            ("ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •", hasattr(st.session_state, 'reshaped_dataset') and st.session_state.reshaped_dataset is not None),
            ("íŠ¹ì§• ì„ íƒ", st.session_state.selected_features is not None)
        ]
        
        completed_steps = sum(1 for _, completed in progress_steps if completed)
        progress = completed_steps / len(progress_steps)
        
        st.subheader("ğŸ“ˆ ì§„í–‰ë¥ ")
        st.progress(progress)
        st.write(f"{completed_steps}/{len(progress_steps)} ë‹¨ê³„ ì™„ë£Œ ({progress:.0%})")
        
        # ê° ë‹¨ê³„ë³„ ìƒíƒœ
        for step_name, completed in progress_steps:
            if completed:
                st.write(f"âœ… {step_name}")
            else:
                st.write(f"â­• {step_name}")
        
        # ì‚¬ìš© ê°€ì´ë“œ
        st.markdown("---")
        with st.expander("ğŸ“– ì‚¬ìš© ê°€ì´ë“œ"):
            st.markdown("""
            **1ë‹¨ê³„: ë°ì´í„° ì…ë ¥**
            - NPY í˜•ì‹ì˜ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì—…ë¡œë“œ
            - ë°ì´í„° êµ¬ì¡° ë° íŠ¹ì§• í™•ì¸
            
            **2ë‹¨ê³„: ì •ê·œí™”** 
            - Min-Max ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚°
            - í•„ìš”ì‹œ íŒŒë¼ë¯¸í„° ìˆ˜ë™ ì¡°ì •
            - ëª¨ë“  ë°ì´í„°ì— ì •ê·œí™” ì ìš©
            
            **3ë‹¨ê³„: íŠ¹ì§• ì‹œí”„íŠ¸**
            - íŠ¹ì • íŠ¹ì§•ì„ ì‹œê°„ì¶•ì—ì„œ ì´ë™
            - ì–‘ìˆ˜: ì§€ì—°, ìŒìˆ˜: ì•ë‹¹ê¹€
            - ì—¬ëŸ¬ íŠ¹ì§•ì— ìˆœì°¨ì  ì ìš© ê°€ëŠ¥
            
            **4ë‹¨ê³„: ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •**
            - Lookbackê³¼ Horizon ê¸¸ì´ ì‚¬ìš©ì ì •ì˜
            - ë°ì´í„° ì¶”ì¶œ ë°©ë²• ì„ íƒ ê°€ëŠ¥
            - ì‹œí€€ìŠ¤ ë³€í™” ë¯¸ë¦¬ë³´ê¸° ì œê³µ
            
            **5ë‹¨ê³„: ì…ì¶œë ¥ íŠ¹ì§• ì„ ì •**
            - ëª¨ë¸ ì…ë ¥/ì¶œë ¥ íŠ¹ì§• ì„ íƒ
            - ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ
            - PyTorch ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ ì œê³µ
            """)
        
        # í’‹ë…¸íŠ¸
        st.markdown(
            """
            <style>
            .bottom-info {
                position: fixed;
                bottom: 0;
                left: 0;
                width: 21rem;
                max-width: 21rem;
                background-color: var(--background-color);
                padding: 1rem;
                border-top: 1px solid var(--border-color);
                z-index: 999;
                box-sizing: border-box;
            }
            .bottom-info hr {
                margin: 0.2rem 0;
                border-color: var(--text-color-light);
                width: 100%;
            }
            </style>
            <div class="bottom-info">
                <hr>
                ğŸ§  <strong>íšŒì‚¬ëª…:</strong> ãˆœíŒŒì‹œë””ì—˜<br>
                ğŸ« <strong>ì—°êµ¬ì‹¤:</strong> visLAB@PNU<br>
                ğŸ‘¨â€ğŸ’» <strong>ì œì‘ì:</strong> (C)Dong2<br>
                ğŸ› ï¸ <strong>ë²„ì „:</strong> V.2.0 (06-12-2025)<br>
                <hr>
            </div>
            """, 
            unsafe_allow_html=True
        )

if __name__ == "__main__":
    main()

